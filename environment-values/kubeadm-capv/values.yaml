---

units:
  capm3:
    enabled: yes

  cabpr:  # RKE2
    enabled: yes

  metal3:
    enabled: yes

  cluster:
    kustomization_spec:
      path: ./kustomize-units/cluster-manifests/kubeadm-capv/base
      postBuild:
        substitute:
          SSH_KEY: '{{ .Values.cluster.capv.ssh_key }}'
          VS_USERNAME: '{{ .Values.cluster.capv.username }}'
    kustomization_substitute_secrets:
      VS_PASSWORD: '{{ .Values.cluster.capv.password }}'

cluster:
  name: management-cluster
  k8s_version: v1.24.10
  control_plane_replicas: 1
  worker_replicas: 3

  # image reference depends provider
  image: "ubuntu-2004-kube-v1.24.10"

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capv   # capv
    bootstrap_provider: cabpk  # RKE2 or kubeadm

  capv:
    # -- Datacenter to use
    dataCenter: # replace me
    # -- VSphere network for VMs and CSI
    network: # replace me
    # -- VSphere server dns name
    server: # replace me
    # -- VSphere datastore name
    dataStore: # replace me
    # -- VSphere https TLS thumbprint
    tlsThumbprint: # replace me
    # -- SSH public key for VM access
    ssh_key: # replace me
    # -- VSphere folder
    folder: # replace me
    # -- VSphere resoucepool
    resourcePool: # replace me
    # -- VSphere storage policy name
    storagePolicyName: # replace me


  cluster_external_ip: # replace me

  metal3:
    # -- cluster_external_ip may be reused to expose ironic too
    ironic_ip: # replace me

proxies:
  http_proxy: # replace me
  https_proxy: # replace me
  no_proxy: # replace me
