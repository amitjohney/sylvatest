# This example environment values files will let you build
# a Sylva CAPO mgmt cluster
# plus a CAPM3 (Bare-Metal) workload cluster consisting of 1 control plane node and 1 worker node (all these numbers can be changed)

---
proxies:
  # put your own proxy settings here if you need
  # In NIS context theses proxies values are mandatory
  http_proxy: "http://172.20.73.196:3128"
  https_proxy: "http://172.20.73.196:3128"
  no_proxy: "127.0.0.0/8,localhost,10.0.0.0/8,172.16.0.0/12,172.30.0.0/24,192.168.0.0/16,.sylva,.repos.tech.orange"

ntp:
  enabled: yes
  servers:
  - 172.20.4.108                        # CHANGE ME, ip address of NTP servers reachable from your tenant
  - 172.20.1.104                        # CHANGE ME, ip address of NTP servers reachable from your tenant

cluster:
  image: "ubuntu-rke2-base"             # CHANGE ME, the name of the OpenStack image for VMs turned into K8s nodes
  # worker_machine_image: "ubuntu-rke2-base-worker"       # CHANGE ME if needed, the name of the OpenStack image for VMs turned into K8s worker nodes; defaults to the one used by control plane nodes
  capo:
    ssh_key_name: my_key                # CHANGE ME, the name of the OpenStack key name
    network_id: NET_ID                  # CHANGE ME, needs to be a pre-existing OpenStack tenant network, or a provider network (see pre-requisites)
    # external_network_id: EXT_NET_ID   # CHANGE ME, this needs to be an OpenStack external network, it's optional and if set, a Floating IP will be defined on this network, pointing to the IP of the mgmt cluster (the network specified by network_id must have a router attached to this external network)
    ## If cinder CSI is used, provide a name for the storage class and the storage type from OpenStack
    rootVolume:
      diskSize: 100
      volumeType: ceph_ssd              # CHANGE ME
    storageClass:
      type: "ceph_ssd"                  # CHANGE ME
    worker_az: "WORKER_AZ"              # CHANGE ME, name of the availability zone for the workers
    flavor_name: "FLAVOR"               # CHANGE ME, this is the management cluster master node flavor
    worker_flavor_name: "FLAVOR"        # CHANGE ME, this is the management cluster worker node flavor
    # resources_tag: my-deployment-MVP2   # CHANGE ME if needed, tag set for OpenStack resources in management and test workload cluster; by default set to sylva-<$OS_USERNAME>
  control_plane_replicas: 3             # Management cluster control plane replicas
  worker_replicas: 0                    # Management cluster workers replicas
  os_images:
    ubuntu-22.04-plain.qcow2:
      uri: oci://docker-proxy-asis-gitlab.repos.tech.orange/sylva-projects/sylva-elements/diskimage-builder/diskimage-builder-plain:0.0.7
      filename: ubuntu-22.04-plain.qcow2
      checksum: 56a6363f25a5491aa955e4dda6fae17faa10c5e0850d01fe2a10c996c7830971

units:
  cluster:
    kustomization_spec:
      interval: 240h
    helmrelease_spec:
      values:
        cis_profile: ""
  capm3:
    enabled: true
  metal3:
    enabled: true

    # NOTE: If you want to be able to log into ironic python agent to troubleshoot node provisionning, uncomment following line
    #helmrelease_spec:
    #  postRenderers:
    #    - kustomize:
    #        patches:
    #        - target:
    #            kind: ConfigMap
    #            name: baremetal-operator-ironic
    #          patch: |
    #            kind: _unused_
    #            metadata:
    #              name: _unused_
    #            data:
    #              IRONIC_KERNEL_PARAMS: console=ttyS0 -rootpwd="$$1$$DAhhpomu$$YqNcZc3ce5H7uswK4skCZ." # "secret"

  workload-capo-cluster-resources:
    enabled: false
  workload-cluster:
    enabled: true
    helmrelease_spec:
      values:
        cluster:
          capi_providers:
            infra_provider: capm3
            bootstrap_provider: cabpr
          control_plane_replicas: 1
          cluster_external_ip: 10.188.36.149  # CHANGE ME, Control Plane endpoint of the workload k8s cluster, last ip address of the public pool

          # uncomment following lines to provision SR-IOV VFs on the supported NICs of cluster nodes
          #sriov:
          #  node_policies:
          #    mypolicy1:
          #      # nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
          #      resourceName: sriov_policy
          #      numVfs: 12
          #      nicSelector:
          #        # Supported device IDs and vendors can be found at: https://github.com/k8snetworkplumbingwg/sriov-network-operator/blob/master/doc/supported-hardware.md
          #        #deviceID: "158b"       # for Intel XXV710
          #        #vendor: "8086"         # Intel
          #        #vfNames: []
          #        rootDevices:
          #          - "0000:37:00.0"
          #          - "0000:37:00.1"

        units:

          # uncomment following lines to enable multus & sriov components on workload cluster
          #multus:
          #  enabled: true
          #sriov:
          #  enabled: true
          #sriov-resources:
          #  enabled: true

          longhorn:
            enabled: true
          cluster:
            helmrelease_spec:
              values:
                cis_profile: ""
                air_gapped: false
                metal3:
                  machine_image_url: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2
                  machine_image_format: qcow2
                  machine_image_checksum: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2.sha256sum
                  machine_image_checksum_type: sha256
                  worker_machine_image_url: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2
                  worker_machine_image_format: qcow2
                  worker_machine_image_checksum: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2.sha256sum
                  worker_machine_image_checksum_type: sha256
                  public_pool_name: "public-pool"
                  public_pool_network: 10.188.36.128            # CHANGE ME, needs to be a pre-existing public network (see pre-requisites)
                  public_pool_gateway: 10.188.36.129            # CHANGE ME
                  public_pool_start: 10.188.36.148              # CHANGE ME, starting address in the public network range
                  public_pool_end: 10.188.36.149                # CHANGE ME, end address in the public network range
                  public_pool_prefix: "26"                      # CHANGE ME, Public pool IP address collection
                  provisioning_pool_name: "provisioning-pool"
                  provisioning_pool_network: 10.199.39.192      # CHANGE ME, needs to be a pre-existing provisioning network (see pre-requisites)
                  provisioning_pool_gateway: 10.199.39.193      # CHANGE ME
                  provisioning_pool_start: 10.199.39.219        # CHANGE ME, starting address in the provisioning address range
                  provisioning_pool_end: 10.199.39.220          # CHANGE ME, end address in the provisioning address range
                  provisioning_pool_prefix: "27"                # CHANGE ME, Provisiong pool IP address collection
                  dns_server: 1.2.3.4                           # CHANGE ME

                control_plane:   # tweak network configuration as needed ...
                  infra:
                    provisioning_pool_interface: bond0
                    public_pool_interface: bond0.13
                    network_interfaces:
                      # for CAPM3 folowing are used and mapped to Metal3Data.spec.template.spec.networkData.links
                      bond0:
                        # bond_mode can be one of balance-rr, active-backup, balance-xor, broadcast, balance-tlb, balance-alb, 802.3ad
                        # https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/api/v1alpha5/metal3datatemplate_types.go#L201-L202
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy

                  # uncomment following lines to configure specific cpu & topology managers policies in control-plane nodes
                  #kubelet_extra_args:
                  #  feature-gates: "TopologyManager=true,CPUManager=true"
                  #  topology-manager-policy: "best-effort"
                  #  cpu-manager-policy: "static"
                  #  system-reserved: "cpu=100m"
                  #  kube-reserved: "cpu=100m"

                  rke2:
                    additionalUserData:  # sylva-capi-cluster pushes this under the .config key (we have to fix that)

                      # uncomment following lines to provide specific kernel parameters to enable iommu and hugepages in control-plane nodes
                      #bootcmd:
                      #  - if [ ! -f "/var/lib/grub-init" ]; then
                      #  - touch "/var/lib/grub-init"
                      #  - current_grub=$(grep '^GRUB_CMDLINE_LINUX_DEFAULT=' /etc/default/grub | tail -1 | sed -e "s/^[^=]*=[\"']\\?//" -e "s/['\"]$//")
                      #  - next_grub="$current_grub iommu=pt intel_iommu=on hugepagesz=2M hugepages=0 hugepagesz=1G hugepages=32 default_hugepagesz=1G"
                      #  # No need for a timeout. Nobody attends
                      #  - sed -i -e '/GRUB_TIMEOUT=5/ d' /etc/default/grub
                      #  # Delete the old command-line
                      #  - sed -i -e '/^GRUB_CMDLINE_LINUX_DEFAULT=/ d' /etc/default/grub
                      #  # No wait on reboot (first considered as failed)
                      #  - echo "GRUB_RECORDFAIL_TIMEOUT=1" >> /etc/default/grub
                      #  - echo "GRUB_CMDLINE_LINUX_DEFAULT=\"$next_grub\"" >> /etc/default/grub
                      #  - update-grub
                      #  - cloud-init clean --reboot
                      #  - fi

                      users:
                        - name: ubuntu
                          groups: users
                          sudo: ALL=(ALL) NOPASSWD:ALL
                          shell: /bin/bash
                          lock_passwd: false
                          passwd: "put your password hash here" # (copy pasted from /etc/shadow or created with "mkpasswd" --method=SHA-512 --stdin")
                          ssh_authorized_keys:
                            - ssh-rsa AAAA...... YOUR KEY HERE ....UqnQ==

                machine_deployment_default: # tweak as needed ...
                  provisioning_pool_interface: bond0
                  public_pool_interface: bond0.13

                machine_deployments:
                  md0:
                    infra_provider: capm3
                    replicas: 1
                    capm3:
                      hostSelector:
                        matchLabels:
                          cluster-role: worker # tweak as needed must match cluster-role defined in baremetal_hosts
                    failure_domain: region0
                    network_interfaces: # tweak network configuration as needed ...
                      bond0:
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy

                    # uncomment following lines to provide specific cpu & topology managers policies to worker nodes
                    #kubelet_extra_args:
                    #  feature-gates: "TopologyManager=true,CPUManager=true"
                    #  topology-manager-policy: "best-effort"
                    #  cpu-manager-policy: "static"
                    #  system-reserved: "cpu=100m"
                    #  kube-reserved: "cpu=100m"

                    rke2:
                      additionalUserData:
                        # uncomment following lines to provide specific kernel parameters to enables iommu and hugepages 
                        #bootcmd:
                        #  - if [ ! -f "/var/lib/grub-init" ]; then
                        #  - touch "/var/lib/grub-init"
                        #  - current_grub=$(grep '^GRUB_CMDLINE_LINUX_DEFAULT=' /etc/default/grub | tail -1 | sed -e "s/^[^=]*=[\"']\\?//" -e "s/['\"]$//")
                        #  - next_grub="$current_grub iommu=pt intel_iommu=on hugepagesz=2M hugepages=0 hugepagesz=1G hugepages=32 default_hugepagesz=1G"
                        #  # No need for a timeout. Nobody attends
                        #  - sed -i -e '/GRUB_TIMEOUT=5/ d' /etc/default/grub
                        #  # Delete the old command-line
                        #  - sed -i -e '/^GRUB_CMDLINE_LINUX_DEFAULT=/ d' /etc/default/grub
                        #  # No wait on reboot (first considered as failed)
                        #  - echo "GRUB_RECORDFAIL_TIMEOUT=1" >> /etc/default/grub
                        #  - echo "GRUB_CMDLINE_LINUX_DEFAULT=\"$next_grub\"" >> /etc/default/grub
                        #  - update-grub
                        #  - cloud-init clean --reboot
                        #  - fi
                        users:
                          - name: ubuntu
                            groups: users
                            sudo: ALL=(ALL) NOPASSWD:ALL
                            shell: /bin/bash
                            ssh_authorized_keys:
                              - ssh-rsa AAAA...... YOUR KEY HERE ....UqnQ==

                baremetal_hosts:
                  # this example is based on what worked on an HP Proliant DL360 Gen10
                  my-server:  # corresponding credentials need to be set in secrets.yaml
                    bmh_metadata:
                      labels:
                        cluster-role: control-plane
                        longhorn: "true"
                      annotations:
                        disk: /dev/sdb # this disk will be mounted on the K8S node in a dedicated directory and used by Longhorn
                    bmh_spec:
                      description: Downstream dev clusters
                      online: true
                      externallyProvisioned: false
                      bmc:
                        address: redfish-virtualmedia://<ipmi-address>/redfish/v1/Systems/1    # CHANGE ME, <impi-address> is the physical device host IP
                        disableCertificateVerification: true
                      automatedCleaningMode: metadata
                      bootMACAddress: ba:ad:00:c0:ff:ee    # CHANGE ME, this is the MAC address of baremetal server
                      bootMode: legacy
                      rootDeviceHints:
                        hctl: 2:1:0:0   # CHANGE ME, A SCSI bus address of the OS Disk. Parameters like deviceName, model could also be used instead of hctl

                  my-server2:  # corresponding credentials need to be set in secrets.yaml
                    bmh_metadata:
                      labels:
                        cluster-role: worker
                        longhorn: "true"
                      annotations:
                        disk: /dev/sdb # this disk will be mounted on the K8S node in a dedicated directory and used by Longhorn
                    bmh_spec:
                      description: Downstream dev clusters
                      online: true
                      externallyProvisioned: false
                      bmc:
                        address: redfish-virtualmedia://<ipmi-address>/redfish/v1/Systems/1    # CHANGE ME, <impi-address> is the physical device host IP
                        disableCertificateVerification: true
                      automatedCleaningMode: metadata
                      bootMACAddress: ba:ad:00:c0:ff:ef    # CHANGE ME, this is the MAC address of baremetal server
                      bootMode: legacy
                      rootDeviceHints:
                        hctl: 2:1:0:0   # CHANGE ME, A SCSI bus address of the OS Disk. Parameters like deviceName, model could also be used instead of hctl
                        # deviceName: /dev/sda
