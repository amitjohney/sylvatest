# This example environment values files will let you build
# a Sylva CAPO mgmt cluster
# plus a CAPM3 (Bare-Metal) workload cluster consisting of 1 control plane node and 1 worker node (all these numbers can be changed)

---
proxies:
  # put your own proxy settings here if you need
  # In NIS context theses proxies values are mandatory
  http_proxy: "http://172.20.73.196:3128"
  https_proxy: "http://172.20.73.196:3128"
  no_proxy: ".repos.tech.orange,localhost,127.0.0.1,192.168.0.0/16,172.16.0.0/12,10.0.0.0/8,docker-proxy-asis-gitlab.repos.tech.orange"

registry_mirrors:
  default_settings:
    skip_verify: true
    override_path: true
  hosts_config:
    docker.io:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_docker.io
    quay.io:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_quay.io
    registry.gitlab.com:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_registry.gitlab.com
    k8s.gcr.io:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_k8s.gcr.io
    ghcr.io:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_ghcr.io
    registry.k8s.io:
      - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_registry.k8s.io

ntp:
  enabled: yes
  servers:
  - 172.20.1.104                        # CHANGE ME, ip address of NTP servers reachable from your tenant
  - 172.20.1.105                        # CHANGE ME, ip address of NTP servers reachable from your tenant

cluster:
  image: "ubuntu-rke2-base"             # CHANGE ME, the name of the OpenStack image for VMs turned into K8s nodes
  # worker_machine_image: "ubuntu-rke2-base-worker"       # CHANGE ME if needed, the name of the OpenStack image for VMs turned into K8s worker nodes; defaults to the one used by control plane nodes
  capo:
    ssh_key_name: my_key                # CHANGE ME, the name of the OpenStack key name
    network_id: c314d52c-80fe-42b6-9092-55be383d1951                  # CHANGE ME, needs to be a pre-existing OpenStack tenant network, or a provider network (see pre-requisites)
    # external_network_id: EXT_NET_ID   # CHANGE ME, this needs to be an OpenStack external network, it's optional and if set, a Floating IP will be defined on this network, pointing to the IP of the mgmt cluster (the network specified by network_id must have a router attached to this external network)
    ## If cinder CSI is used, provide a name for the storage class and the storage type from OpenStack
    rootVolume:
      diskSize: 100
      volumeType: ceph_sas              # CHANGE ME
    storageClass:
      type: "ceph_sas"                  # CHANGE ME
    worker_az: 'dev-az'              # CHANGE ME, name of the availability zone for the workers
    flavor_name: "m1.xlarge"               # CHANGE ME, this is the management cluster master node flavor
    worker_flavor_name: "m1.xlarge"        # CHANGE ME, this is the management cluster worker node flavor
    # resources_tag: my-deployment-MVP2   # CHANGE ME if needed, tag set for OpenStack resources in management and test workload cluster; by default set to sylva-<$OS_USERNAME>
  control_plane_replicas: 1             # Management cluster control plane replicas
  worker_replicas: 0                    # Management cluster workers replicas
  os_images:
    ubuntu-2204-plain-qcow2:
      uri: oci://docker-proxy-asis-gitlab.repos.tech.orange/sylva-projects/sylva-elements/diskimage-builder/diskimage-builder-plain:0.0.7
      filename: ubuntu-22.04-plain.qcow2
      checksum: 56a6363f25a5491aa955e4dda6fae17faa10c5e0850d01fe2a10c996c7830971

units:
  cluster:
    kustomization_spec:
      interval: 240h
    helmrelease_spec:
      values:
        cis_profile: ""
  capm3:
    enabled: true
  metal3:
    enabled: true

    # NOTE: If you want to be able to log into ironic python agent to troubleshoot node provisionning, uncomment following line
    #helmrelease_spec:
    #  postRenderers:
    #    - kustomize:
    #        patches:
    #        - target:
    #            kind: ConfigMap
    #            name: baremetal-operator-ironic
    #          patch: |
    #            kind: _unused_
    #            metadata:
    #              name: _unused_
    #            data:
    #              IRONIC_KERNEL_PARAMS: console=ttyS0 -rootpwd="$$1$$DAhhpomu$$YqNcZc3ce5H7uswK4skCZ." # "secret"

  workload-capo-cluster-resources:
    enabled: false
  workload-cluster:
    enabled: true
    helmrelease_spec:
      values:
        cluster:
          capi_providers:
            infra_provider: capm3
            bootstrap_provider: cabpr
          control_plane_replicas: 1
          cluster_external_ip: 172.20.36.173  # CHANGE ME, Control Plane endpoint of the workload k8s cluster, last ip address of the public pool

          # uncomment following lines to provision SR-IOV VFs on the supported NICs of cluster nodes
          #sriov:
          #  node_policies:
          #    mypolicy1:
          #      # nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
          #      resourceName: sriov_policy
          #      numVfs: 12
          #      nicSelector:
          #        # Supported device IDs and vendors can be found at: https://github.com/k8snetworkplumbingwg/sriov-network-operator/blob/master/doc/supported-hardware.md
          #        #deviceID: "158b"       # for Intel XXV710
          #        #vendor: "8086"         # Intel
          #        #vfNames: []
          #        rootDevices:
          #          - "0000:37:00.0"
          #          - "0000:37:00.1"

        units:

          # uncomment following lines to enable multus & sriov components on workload cluster
          #multus:
          #  enabled: true
          #sriov:
          #  enabled: true
          #sriov-resources:
          #  enabled: true

          longhorn:
            enabled: true
          cluster:
            helmrelease_spec:
              values:
                cis_profile: ""
                air_gapped: false
                metal3:
                  machine_image_url: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2
                  machine_image_format: qcow2
                  machine_image_checksum: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2.sha256sum
                  machine_image_checksum_type: sha256
                  worker_machine_image_url: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2
                  worker_machine_image_format: qcow2
                  worker_machine_image_checksum: http://{{ .Values.cluster.display_external_ip }}/ubuntu-22.04-plain.qcow2.sha256sum
                  worker_machine_image_checksum_type: sha256
                  public_pool_name: "public-pool"
                  public_pool_network: 172.20.36.128            # CHANGE ME, needs to be a pre-existing public network (see pre-requisites)
                  public_pool_gateway: 172.20.36.129            # CHANGE ME
                  public_pool_start: 172.20.36.171              # CHANGE ME, starting address in the public network range
                  public_pool_end: 172.20.36.175                # CHANGE ME, end address in the public network range
                  public_pool_prefix: "26"                      # CHANGE ME, Public pool IP address collection
                  provisioning_pool_name: "provisioning-pool"
                  provisioning_pool_network: 172.20.39.192      # CHANGE ME, needs to be a pre-existing provisioning network (see pre-requisites)
                  provisioning_pool_gateway: 172.20.39.193      # CHANGE ME
                  provisioning_pool_start: 172.20.39.195        # CHANGE ME, starting address in the provisioning address range
                  provisioning_pool_end: 172.20.39.197          # CHANGE ME, end address in the provisioning address range
                  provisioning_pool_prefix: "27"                # CHANGE ME, Provisiong pool IP address collection
                  dns_server: 10.193.21.160                           # CHANGE ME

                control_plane:   # tweak network configuration as needed ...
                  infra:
                    provisioning_pool_interface: bond0
                    public_pool_interface: bond0.13
                    network_interfaces:
                      # for CAPM3 folowing are used and mapped to Metal3Data.spec.template.spec.networkData.links
                      bond0:
                        # bond_mode can be one of balance-rr, active-backup, balance-xor, broadcast, balance-tlb, balance-alb, 802.3ad
                        # https://github.com/metal3-io/cluster-api-provider-metal3/blob/main/api/v1alpha5/metal3datatemplate_types.go#L201-L202
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy

                  # uncomment following lines to configure specific cpu & topology managers policies in control-plane nodes
                  #kubelet_extra_args:
                  #  feature-gates: "TopologyManager=true,CPUManager=true"
                  #  topology-manager-policy: "best-effort"
                  #  cpu-manager-policy: "static"
                  #  system-reserved: "cpu=100m"
                  #  kube-reserved: "cpu=100m"

                  rke2:
                    additionalUserData:  # sylva-capi-cluster pushes this under the .config key (we have to fix that)

                      # uncomment following lines to provide specific kernel parameters to enable iommu and hugepages in control-plane nodes
                      #bootcmd:
                      #  - if [ ! -f "/var/lib/grub-init" ]; then
                      #  - touch "/var/lib/grub-init"
                      #  - current_grub=$(grep '^GRUB_CMDLINE_LINUX_DEFAULT=' /etc/default/grub | tail -1 | sed -e "s/^[^=]*=[\"']\\?//" -e "s/['\"]$//")
                      #  - next_grub="$current_grub iommu=pt intel_iommu=on hugepagesz=2M hugepages=0 hugepagesz=1G hugepages=32 default_hugepagesz=1G"
                      #  # No need for a timeout. Nobody attends
                      #  - sed -i -e '/GRUB_TIMEOUT=5/ d' /etc/default/grub
                      #  # Delete the old command-line
                      #  - sed -i -e '/^GRUB_CMDLINE_LINUX_DEFAULT=/ d' /etc/default/grub
                      #  # No wait on reboot (first considered as failed)
                      #  - echo "GRUB_RECORDFAIL_TIMEOUT=1" >> /etc/default/grub
                      #  - echo "GRUB_CMDLINE_LINUX_DEFAULT=\"$next_grub\"" >> /etc/default/grub
                      #  - update-grub
                      #  - cloud-init clean --reboot
                      #  - fi

                      users:
                        - name: ubuntu
                          groups: users
                          sudo: ALL=(ALL) NOPASSWD:ALL
                          shell: /bin/bash
                          lock_passwd: false
                          passwd: $6$gwLxr/rEczCWRnTM$S/6h7fR.Mqu3DeTqiRAtk/eHHCauTykzkHnuuwNxJhpTAYfnXtiO9QNXUR4.KsVaEdhdJzeBRIZViAHFv91.v1 # (copy pasted from /etc/shadow or created with "mkpasswd" --method=SHA-512 --stdin")
                          ssh_authorized_keys:
                            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid
                        - name: akki
                          groups: users
                          sudo: ALL=(ALL) NOPASSWD:ALL
                          shell: /bin/bash
                          lock_passwd: false
                          passwd: Akki@12345678
                          ssh_authorized_keys:
                            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid

                machine_deployment_default: # tweak as needed ...
                  provisioning_pool_interface: bond0
                  public_pool_interface: bond0.13

                machine_deployments:
                  md0:
                    infra_provider: capm3
                    replicas: 1
                    capm3:
                      hostSelector:
                        matchLabels:
                          cluster-role: worker # tweak as needed must match cluster-role defined in baremetal_hosts
                    failure_domain: region0
                    network_interfaces: # tweak network configuration as needed ...
                      bond0:
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy

                    # uncomment following lines to provide specific cpu & topology managers policies to worker nodes
                    #kubelet_extra_args:
                    #  feature-gates: "TopologyManager=true,CPUManager=true"
                    #  topology-manager-policy: "best-effort"
                    #  cpu-manager-policy: "static"
                    #  system-reserved: "cpu=100m"
                    #  kube-reserved: "cpu=100m"

                    rke2:
                      additionalUserData:
                        # uncomment following lines to provide specific kernel parameters to enables iommu and hugepages 
                        #bootcmd:
                        #  - if [ ! -f "/var/lib/grub-init" ]; then
                        #  - touch "/var/lib/grub-init"
                        #  - current_grub=$(grep '^GRUB_CMDLINE_LINUX_DEFAULT=' /etc/default/grub | tail -1 | sed -e "s/^[^=]*=[\"']\\?//" -e "s/['\"]$//")
                        #  - next_grub="$current_grub iommu=pt intel_iommu=on hugepagesz=2M hugepages=0 hugepagesz=1G hugepages=32 default_hugepagesz=1G"
                        #  # No need for a timeout. Nobody attends
                        #  - sed -i -e '/GRUB_TIMEOUT=5/ d' /etc/default/grub
                        #  # Delete the old command-line
                        #  - sed -i -e '/^GRUB_CMDLINE_LINUX_DEFAULT=/ d' /etc/default/grub
                        #  # No wait on reboot (first considered as failed)
                        #  - echo "GRUB_RECORDFAIL_TIMEOUT=1" >> /etc/default/grub
                        #  - echo "GRUB_CMDLINE_LINUX_DEFAULT=\"$next_grub\"" >> /etc/default/grub
                        #  - update-grub
                        #  - cloud-init clean --reboot
                        #  - fi
                        users:
                          - name: ubuntu
                            groups: users
                            sudo: ALL=(ALL) NOPASSWD:ALL
                            shell: /bin/bash
                            ssh_authorized_keys:
                              - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid
                          - name: akki
                            groups: users
                            sudo: ALL=(ALL) NOPASSWD:ALL
                            shell: /bin/bash
                            lock_passwd: false
                            passwd: Akki@12345678

                baremetal_hosts:
                  # this example is based on what worked on an HP Proliant DL360 Gen10
                  dl360-31:  # corresponding credentials need to be set in secrets.yaml
                    bmh_metadata:
                      labels:
                        cluster-role: control-plane
                        longhorn: "true"
                      annotations:
                        disk: /dev/sdb # this disk will be mounted on the K8S node in a dedicated directory and used by Longhorn
                    bmh_spec:
                      description: Downstream dev clusters
                      online: true
                      externallyProvisioned: false
                      bmc:
                        address: redfish-virtualmedia://172.20.188.239/redfish/v1/Systems/1    # CHANGE ME, <impi-address> is the physical device host IP
                        disableCertificateVerification: true
                      automatedCleaningMode: metadata
                      bootMACAddress: 48:df:37:e7:68:c8    # CHANGE ME, this is the MAC address of baremetal server
                      bootMode: legacy
                      rootDeviceHints:
                        hctl: 2:1:0:0   # CHANGE ME, A SCSI bus address of the OS Disk. Parameters like deviceName, model could also be used instead of hctl

                  dl360-33:  # corresponding credentials need to be set in secrets.yaml
                    bmh_metadata:
                      labels:
                        cluster-role: worker
                        longhorn: "true"
                      annotations:
                        disk: /dev/sdb # this disk will be mounted on the K8S node in a dedicated directory and used by Longhorn
                    bmh_spec:
                      description: Downstream dev clusters
                      online: true
                      externallyProvisioned: false
                      bmc:
                        address: redfish-virtualmedia://172.20.188.241/redfish/v1/Systems/1    # CHANGE ME, <impi-address> is the physical device host IP
                        disableCertificateVerification: true
                      automatedCleaningMode: metadata
                      bootMACAddress: 48:df:37:e6:0d:64    # CHANGE ME, this is the MAC address of baremetal server
                      bootMode: legacy
                      rootDeviceHints:
                        hctl: 2:1:0:0   # CHANGE ME, A SCSI bus address of the OS Disk. Parameters like deviceName, model could also be used instead of hctl
                        # deviceName: /dev/sda

  workload-cluster2:
    depends_on:
      namespace-defs: true
      #workload-capo-cluster-resources2: '{{ tuple . "workload-capo-cluster-resources2" | include "unit-enabled" }}'   # produces the capo-cluster-resources used below
      #workload-cluster-cloud-config2: '{{ tuple . "workload-cluster-cloud-config2" | include "unit-enabled" }}'
      #workload-capo-cluster-resources: ""
      #workload-cluster-cloud-config: ""
    enabled: true
    helm_secret_values:
      cluster:
        capo:
          clouds_yaml:
            clouds:
              capo_cloud:
                auth:
                  auth_url: https://172.20.129.60:13002/v3
                  password: Akshay.Akshay.otccaas
                  project_domain_name: default
                  project_name: Akshay.Akshay.otccaas
                  user_domain_name: default
                  username: Akshay.Akshay.otccaas
                region_name: RegionOne
                verify: false
        capv:
          password: ""
          username: ""
    helmrelease_spec:
      chart:
        spec:
          chart: charts/sylva-units
          reconcileStrategy: Revision
          sourceRef:
            kind: HelmRepository
            name: sylva-core
            namespace: default
          valuesFiles:
            - charts/sylva-units/values.yaml
            - charts/sylva-units/workload-cluster.values.yaml
            - use-oci-artifacts.values.yaml
      values:
        cluster:
          air_gapped: true
          capi_providers:
            bootstrap_provider: cabpr
            infra_provider: capm3
          capo:
            flavor_name: m1.xlarge
            network_id: c314d52c-80fe-42b6-9092-55be383d1951
            resources_tag: sylva-Akshay.Akshay.otccaas
            rootVolume:
              diskSize: 100
              volumeType: ceph_sas
            ssh_key_name: my_key
            storageClass:
              name: cinder-csi
              type: ceph_sas
            worker_az: dev-az
            worker_flavor_name: m1.xlarge
          capv:
            control_plane_profile:
              diskGiB: 25
              memoryMiB: 8192
              numCPUs: 2
            dataCenter: ""
            dataStore: ""
            folder: ""
            network: ""
            resourcePool: ""
            server: ""
            ssh_key: ""
            storageClass:
              fsType: ext4
              storagePolicyName: ""
            storagePolicyName: ""
            tlsThumbprint: ""
          cis_profile: cis-1.6
          cluster_external_domain: sylva
          cluster_external_ip: 172.20.36.178
          control_plane_replicas: 1
          image: ubuntu-rke2-base
          k8s_version: v1.24.12+rke2r1
          name: second-workload-cluster
            #name: first-workload-cluster
          worker_machine_image: ""
        metallb_helm_oci_url: oci://docker-proxy-asis-gitlab.repos.tech.orange/sylva-projects/sylva-core/metallb
        ntp:
          enabled: true
          servers:
            - 172.20.1.104
            - 172.20.1.105
        proxies:
          http_proxy: http://172.20.73.196:3128
          https_proxy: http://172.20.73.196:3128
          no_proxy: .repos.tech.orange,localhost,127.0.0.1,192.168.0.0/16,172.16.0.0/12,10.0.0.0/8,docker-proxy-asis-gitlab.repos.tech.orange
        registry_mirrors:
          default_settings:
            capabilities:
              - pull
              - resolve
            override_path: true
            skip_verify: true
          hosts_config:
            docker.io:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_docker.io
            gcr.io:
              - mirror_url: https://docker-proxy-asis-google.repos.tech.orange
            ghcr.io:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_ghcr.io
            k8s.gcr.io:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_k8s.gcr.io
            quay.io:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_quay.io
            registry.gitlab.com:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_registry.gitlab.com
            registry.k8s.io:
              - mirror_url: https://172.20.129.169/v2/proxy_cache_diod_registry.k8s.io
        source_templates:
          sylva-core:
            existing_source:
              kind: OCIRepository
              name: sylva-core
              namespace: default
        units:
          cluster:
            helmrelease_spec:
              values:
                air_gapped: false
                baremetal_hosts:
                  dl360-37:
                    bmh_metadata:
                      annotations:
                        disk: /dev/sdb
                      labels:
                        cluster-role: control-plane
                        longhorn: "true"
                    bmh_spec:
                      automatedCleaningMode: metadata
                      bmc:
                        address: redfish-virtualmedia://172.20.188.245/redfish/v1/Systems/1
                        disableCertificateVerification: true
                      bootMACAddress: 48:df:37:e6:0b:a8
                      bootMode: legacy
                      description: Downstream dev clusters
                      externallyProvisioned: false
                      online: true
                      rootDeviceHints:
                        hctl: "2:1:0:0"
                        #   dl360-33:
                        # bmh_metadata:
                        #   annotations:
                        #   disk: /dev/sdb
                        #  labels:
                        # cluster-role: worker
                        # longhorn: "true"
                        #   bmh_spec:
                        # automatedCleaningMode: metadata
                        #   bmc:
                        #  address: redfish-virtualmedia://172.20.188.241/redfish/v1/Systems/1
                        #   disableCertificateVerification: true
                        # bootMACAddress: 48:df:37:e6:0d:64
                        # bootMode: legacy
                        #  description: Downstream dev clusters
                        #  externallyProvisioned: false
                        #  online: true
                        #  rootDeviceHints:
                        # hctl: "2:1:0:0"
                        #  credentials:
                        #  password: 7XF79WDC
                        #  username: Administrator
                capi_rancher_import: true
                capo:
                  control_plane_security_group_name: capo-first-workload-cluster-security-group-ctrl-plane-sylva-Akshay.Akshay.otccaas
                  worker_security_group_name: capo-first-workload-cluster-security-group-workers-sylva-Akshay.Akshay.otccaas
                cis_profile: ""
                control_plane:
                  infra:
                    network_interfaces:
                      bond0:
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy
                    provisioning_pool_interface: bond0
                    public_pool_interface: bond0.13
                  rke2:
                    additionalUserData:
                      users:
                        - groups: users
                          lock_passwd: false
                          name: ubuntu
                          passwd: $6$gwLxr/rEczCWRnTM$S/6h7fR.Mqu3DeTqiRAtk/eHHCauTykzkHnuuwNxJhpTAYfnXtiO9QNXUR4.KsVaEdhdJzeBRIZViAHFv91.v1
                          shell: /bin/bash
                          ssh_authorized_keys:
                            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid
                          sudo: ALL=(ALL) NOPASSWD:ALL
                        - groups: users
                          lock_passwd: false
                          name: akki
                          passwd: Akki@12345678
                          shell: /bin/bash
                          ssh_authorized_keys:
                            - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid
                          sudo: ALL=(ALL) NOPASSWD:ALL
                control_plane_replicas: 1
                enable_longhorn: true
                machine_deployment_default:
                  provisioning_pool_interface: bond0
                  public_pool_interface: bond0.13
                machine_deployments:
                  md0:
                    capm3:
                      hostSelector:
                        matchLabels:
                          cluster-role: worker
                    failure_domain: region0
                    infra_provider: capm3
                    network_interfaces:
                      bond0:
                        bond_mode: 802.3ad
                        interfaces:
                          - ens1f0
                          - ens1f1
                        vlans:
                          - id: 13
                      ens1f0:
                        type: phy
                      ens1f1:
                        type: phy
                    replicas: 1
                    rke2:
                      additionalUserData:
                        users:
                          - groups: users
                            name: ubuntu
                            shell: /bin/bash
                            ssh_authorized_keys:
                              - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCtyRQNGiGWixzfPAvqWD7qWrpopLCaukuSK4dgru9ijfU8MNSYC6D26PtydjEe8dsOIDlZf/+3xaetko9rVL5fv+fBUwtnMZviuUZrP96QOPe6f7tOSH922XvJNp6S34B/9BmY/QoeEAI7F9drJd0xXI7fRHBZsjMcNG0TJCph3AatHgN3Od64b54EDSznfSb7Stp1nDNKf3dmsBEZnjUbIT562WtHL3/DjSDvau4jC5boKu/HHsNIaUwD7Wz0Hb5S80izwoXMh2QCyJy3QorqrSMojpQMoiUvW8TPEv7OrCXZArubL5MbLosJsRJ4BWSUC7tt7r7cIlvaJh5VrLZXE5oF3rQJXB1Zl39tIRH72j1nirTIMqvbImEiFV81/yDpoUL8+zf2fS26kqvhLQpsNUKZI0Ll5CJtK/1CNEegjdR2swA5WPyg6CLj5vlYNOlxZIFPExSDCvxnKQP6sb9CEuDHTKvHePU3NO9GRqVFqI3wXMWFksqUrVq61NRUVq0= ubuntu@akki-otccaas-b-vm-hybrid
                            sudo: ALL=(ALL) NOPASSWD:ALL
                          - groups: users
                            lock_passwd: false
                            name: akki
                            passwd: Akki@12345678
                            shell: /bin/bash
                            sudo: ALL=(ALL) NOPASSWD:ALL
                metal3:
                  dns_server: 10.193.21.160
                  machine_image_checksum: http://172.20.129.252/ubuntu-22.04-plain.qcow2.sha256sum
                  machine_image_checksum_type: sha256
                  machine_image_format: qcow2
                  machine_image_url: http://172.20.129.252/ubuntu-22.04-plain.qcow2
                  provisioning_pool_end: 172.20.39.199
                  provisioning_pool_gateway: 172.20.39.193
                  provisioning_pool_name: provisioning-pool
                  provisioning_pool_network: 172.20.39.192
                  provisioning_pool_prefix: "27"
                  provisioning_pool_start: 172.20.39.198
                  public_pool_end: 172.20.36.180
                  public_pool_gateway: 172.20.36.129
                  public_pool_name: public-pool
                  public_pool_network: 172.20.36.128
                  public_pool_prefix: "26"
                  public_pool_start: 172.20.36.176
                  worker_machine_image_checksum: http://172.20.129.252/ubuntu-22.04-plain.qcow2.sha256sum
                  worker_machine_image_checksum_type: sha256
                  worker_machine_image_format: qcow2
                  worker_machine_image_url: http://172.20.129.252/ubuntu-22.04-plain.qcow2
                mgmt_cluster_external_domain: sylva
                mgmt_cluster_external_ip: 172.20.129.252
          cluster-import:
            enabled: ""
          longhorn:
            enabled: true
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          optional: true
          targetPath: cluster.cluster_external_ip
          valuesKey: allocated_ip
        - kind: ConfigMap
          name: capo-cluster-resources
          optional: true
          targetPath: cluster.capo.floating_ip
          valuesKey: allocated_fip
        - kind: ConfigMap
          name: capo-cluster-resources
          optional: true
          targetPath: cluster.capo.control_plane_servergroup_id
          valuesKey: control_plane_servergroup_id
        - kind: ConfigMap
          name: capo-cluster-resources
          optional: true
          targetPath: cluster.capo.worker_servergroup_id
          valuesKey: worker_servergroup_id
    kustomization_spec:
      healthChecks:
        - apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
          kind: Kustomization
          name: sylva-units-status
          namespace: cattle-system
      targetNamespace: cattle-system
    unit_template: sylva-units
