# Bootstrap values for telco-cloud-init.
# 
# These value are meant to be used as overrides to the default values from values.yaml
# when the chart is instantiated in the bootstrap cluster

# set 'phase: bootstrap' so that
# * components with 'enabled: management-only' are ignored
# * components with 'enabled: boostrap-only' are enabled
phase: bootstrap

component_kustomization_spec_default:
  interval: 100000m   # for bootstrap, we don't care about having flux update things on the longterm
  retryInterval: 30s  # but we want it to retry quickly so that the system converges faster (quicker reaction when a dependency is met)

component_helmrelease_spec_default:
  interval: 100000m

components:

  cert-manager:
    enabled: yes

  capi:
    enabled: yes

  management-cluster-configs:
    enabled: yes
    depends_on:
      - name: cluster
    repo: capi-bootstrap
    kustomization_spec:
      # FIXME: This is very hacky, we should use an ad-hoc kustomisation instead of this job to re-create configmap and secrets on maangement cluster
      path: ./kustomize-components/kube-job
      wait: true
      force: true
      patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: copy-configs-job
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash

              set -e

              # FIXME; Flux wait/healthcheck on management cluster should be handling that...
              # maybe cluster CRD is not compatible with kstatus? (see https://fluxcd.io/flux/components/kustomize/kustomization/#health-assessment)
              echo "-- Wait for management cluster kubeconfig"
              attempts=0
              max_attempts=60
              until kubectl get secret management-cluster-kubeconfig; do
                sleep 10
                ((attempts++)) && ((attempts==max_attempts)) && exit -1
              done

              echo "-- Retrieve target cluster kubeconfig"
              kubectl get secret management-cluster-kubeconfig -o jsonpath='{.data.value}' | base64 -d > management-cluster-kubeconfig

              echo "-- Wait for management cluster to be ready"
              attempts=0
              until kubectl --kubeconfig management-cluster-kubeconfig wait --for condition=Ready --timeout 2s --all nodes; do
                sleep 8
                ((attempts++)) && ((attempts==max_attempts)) && exit -1
              done

              echo "-- Copy secrets and configmaps from bootstrap to management cluster"
              kubectl get configmaps,secrets,gitrepository \
                -l copy-from-bootstrap-to-management= \
                -o json \
                  | jq '.items[] | del(.metadata.labels."helm.toolkit.fluxcd.io/name") | del(.metadata.labels."copy-from-bootstrap-to-management") | del(.metadata.labels."helm.toolkit.fluxcd.io/namespace") | del(.metadata.resourceVersion) | del(.metadata.uid) | del(.metadata.creationTimestamp)' \
                  | kubectl --kubeconfig management-cluster-kubeconfig apply -f -

              echo "-- All done"

  management-cluster-flux:
    enabled: yes
    depends_on:
      - name: management-cluster-configs
    repo: capi-bootstrap
    kustomization_spec:
      path: ./kustomize-components/flux-system
      kubeConfig:
        secretRef:
          name: management-cluster-kubeconfig
      targetNamespace: flux-system
      wait: true
      postBuild:
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
    labels:
      suspend-on-pivot: "yes"  # this component must be suspended before pivot

  # instantiate 'telco-cloud-init' chart again in the management cluster
  management-telco-cloud-init:
    enabled: yes
    depends_on:
      - name: cluster
      - name: management-cluster-flux
    repo: capi-bootstrap
    kustomization_spec:
      path: ./kustomize-components/telco-cloud-init/base
      kubeConfig:
        secretRef:
          name: management-cluster-kubeconfig
      wait: true
      patches:
        # for now we keep the 'cluster' component disabled, it produces CAPI resources
        # defining the management cluster, which we can't define before having done the pivot
        # (this value override is reverted after pivot by the pivot job below)
        - target:
            kind: HelmRelease 
            name: telco-cloud-init
          patch: |
            kind: HelmRelease
            metadata:
              name: ignored
            spec:
              values:
                components:
                  cluster:
                    enabled: false
        # we don't need to define the GitRepository because the one from bootstrap cluster is copied into
        # the management cluster (thanks to copy-from-bootstrap-to-management label)
        - target:  
            kind: GitRepository 
            name: telco-cloud-init
          patch: |
            kind: GitRepository
            metadata:
              name: unwanted
            $patch: delete
    labels:
      suspend-on-pivot: "yes"  # this component must be suspended before pivot

  pivot:
    enabled: yes
    depends_on:
      - name: management-telco-cloud-init
    repo: capi-bootstrap
    kustomization_spec:
      path: ./kustomize-components/kube-job
      wait: true
      force: true
      patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: pivot-job
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash

              set -e

              echo "-- Retrieve target cluster kubeconfig"
              kubectl get secret management-cluster-kubeconfig -o jsonpath='{.data.value}' | base64 -d > management-cluster-kubeconfig

              echo "-- Wait for cluster and machines to be ready as it is a required condition to move"
              kubectl wait --for condition=ControlPlaneReady --timeout 600s --all cluster
              kubectl wait --for condition=NodeHealthy --timeout 600s --all machine

              echo "-- Wait for capi CRD to be installed on management cluster"
              attempts=0
              max_attempts=60
              until kubectl --kubeconfig management-cluster-kubeconfig get customresourcedefinitions clusters.cluster.x-k8s.io; do
                sleep 5
                ((attempts++)) && ((attempts==max_attempts)) && exit -1
              done
              # FIXME: use "until clusterctl move --dry-run" instead of checking conditions one by one above?

              echo "-- Freeze reconciliation on Kustomisations in source cluster that related to the management cluster"
              kubectl annotate kustomizations -l suspend-on-pivot=yes kustomize.toolkit.fluxcd.io/reconcile=disabled --overwrite

              echo "-- Move cluster definitions from source to target cluster"
              clusterctl move --kubeconfig $KUBECONFIG --to-kubeconfig management-cluster-kubeconfig -v 3

              echo "-- Patch the telco-cloud-init HelmRelease to allow the Flux cluster redefinition to happen in the management cluster"
              kubectl --kubeconfig management-cluster-kubeconfig \
                  patch helmrelease telco-cloud-init --type=json --patch='[{"op":"remove","path":"/spec/values/components/cluster/enabled"}]'

              # NOTE: here we could add a reference to another valueFile that would deploy components specific to the management cluster
              # '[{"op":"add","path":"/spec/chart/spec/valuesFiles/-","value":"charts/telco-cloud-init/management.values.yaml"}]'
              # and/or create a kustomisation on management-cluster to manage telco-cloud-init helmRelease from git

              echo "-- Freeze reconciliation of current job Kustomisation in source cluster as we're done"
              kubectl annotate kustomizations pivot kustomize.toolkit.fluxcd.io/reconcile=disabled --overwrite

              echo "-- Accelerate reconciliation of the telco-cloud-init HelmRelease"
              kubectl --kubeconfig management-cluster-kubeconfig \
                  annotate --overwrite helmrelease/telco-cloud-init reconcile.fluxcd.io/requestedAt="$(date +%s)"

              echo "-- All done"

