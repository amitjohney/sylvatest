# Default values for telco-cloud-init.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret:
  # when using gitlab, git creds can be used as registry creds
  registry.gitlab.com: '{{ .Values.git_auth_default | include "preserve-type" }}'

# repository secrets
# (when this chart is instantiated with a Flux HelmRelease following the definition in `kustomize-components/telco-cloud-init/base/helm-release.yaml` `git_auth_default` will be copied from the Secret used for the corresponding Flux GitRepository, but it can be overwritten)
git_auth_default:
  username: your_user_name
  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s
  gitImplementation: libgit2

git_repo_templates: # template to generate Flux GitRepository resources
  # <repo-name>:
  #   # auth: # can be used to override git_auth_default
  #   #   username: plop
  #   #   password: flop
  #   spec:  # partial spec for a Flux GitRepository
  #     url: https://gitlab.com/t6306/components/capi-bootstrap.git
  #     #secretRef: # is autogenerated based on 'auth' or 'git_auth_default'
  #     ref: # can be overridden per-component, with 'ref_override'
  #       branch: main
  #   existing_gitrepository: optional, when this value is set the specified GitRepository will be used instead of creating one based on 'spec'

  capi-bootstrap:
    spec:
      url: https://gitlab.com/t6306/components/capi-bootstrap.git
      ref:
        # tag/commit will take precedence, see https://fluxcd.io/flux/components/source/api/#source.toolkit.fluxcd.io/v1beta2.GitRepositoryRef
        branch: develop
    auth: '{{ .Values.git_auth_default | include "preserve-type" }}'

  capi-rancher-import:
    spec:
      url: https://gitlab.com/t6306/components/capi-rancher-import.git
      ref:
        branch: main
    auth: '{{ .Values.git_auth_default | include "preserve-type" }}'

  weave-gitops:
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.10.2

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'components'
component_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  wait: false
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a component with a "helmrelease_spec" field
component_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: 10
      remediateLastFailure: true

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated a component with a "helmrelease_spec" field
component_helmrelease_kustomization_spec_default:
  path: ./kustomize-components/helmrelease-generic
  sourceRef:
    kind: GitRepository
    name: capi-bootstrap  # GitRepository created by templates/gitrepository-self.yaml

  wait: true


# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository (and Secret, TODO: the Secret don't need to be generated for each component)
components:
  # <component-name>:
  #   enabled: yes/no/management-only
  #   repo: <name of the repo under 'git_repo_templates', defaults to 'default'>
  #   labels: (optional) dict holding labels to add to the resources for this component
  #   ref_override: optional, if defined, this dict will be used for the GitRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: list of '{name: component}' maps defining the dependencies of this component (this is injected in the component Kustomization as dependsOn field)
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from component_helmrelease_spec_default
  #                      # and from other fields in the component definition
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from component_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-component-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <component>.spec.path  (if <git repo template> has spec.url defined)
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret

  flux-system:
    # note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
    # this components contains Flux definitions *to manage the Flux system itself via gitops*

    repo: capi-bootstrap
    kustomization_spec:
      path: ./kustomize-components/flux-system
      targetNamespace: flux-system
      wait: true
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  kubed:
    enabled: '{{ eq .Values.cluster.flavor.infra_provider "capd" }}'
    # FIXME: maybe is isn't required any more with this helm chart, and we could create secrets in components namespaces?
    # We need kubed to copy secrets from namespace to namespace for the 'capd' provider
    # Unfortunately, it doesn't seems to be possible to pass registry secrets from default namespace to others without the use of an additionnal tool like this one:
    # - We can't use kustomize's secretGenerator to copy secret across namespaces, as secret value would be base64-encoded, thus flux PostBuild wouldn't be able to substitute it.
    # - Additionnaly, flux substituteFrom doesn't supports keys starting by a dot, so we can't susbstitute directly from docker-registry secret as the key is .dockerconfigjson
    # - And finally, if we manage to place this secret in another variable, substitution will fail as secret well be passed as a string, whereas .dockerconfigjson is supposed to contain a json...
    helm_repo_url: https://charts.appscode.com/stable
    helmrelease_spec:
      chart:
        spec:
          chart: kubed
          version: v0.13.2
      targetNamespace: kubed
      install:
        createNamespace: true
      values:
        installCRDs: true

  cert-manager:
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.8.2
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true

  cis-operator-crd:
    enabled: '{{ and (.Values.cluster.flavor.bootstrap_provider | eq "cabpr") (.Values.phase | eq "management") }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: v3.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    enabled: '{{ and (.Values.cluster.flavor.bootstrap_provider | eq "cabpr") (.Values.phase | eq "management") }}'
    depends_on:
      - name: cis-operator-crd
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: v3.0.0
      targetNamespace: cis-operator-system

  # this allows for running a CIS scan for management cluster
  # generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
  cis-operator-scan:
    enabled: '{{ and (.Values.cluster.flavor.bootstrap_provider | eq "cabpr") (.Values.phase | eq "management") }}'
    depends_on:
      - name: cis-operator
    repo: capi-bootstrap
    kustomization_spec:
      path: ./kustomize-components/cis-operator-scan
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cluster.cis_benchmark_scan_profile }}'


  capi:
    # ref_override:  # test
    #   branch: i-really-need-dev-branch-foo
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/capi
      wait: true

  capd:
    enabled: '{{ eq .Values.cluster.flavor.infra_provider "capd" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/capd
      postBuild:
        substitute:
          DOCKER_HOST: '{{ .Values.capd.docker_host }}'
      wait: true

  capo:
    enabled: '{{ eq .Values.cluster.flavor.infra_provider "capo" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/capo
      wait: true

  capm3:
    enabled: '{{ eq .Values.cluster.flavor.infra_provider "capm3" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/capm3
      wait: true

  capv:
    enabled: '{{ eq .Values.cluster.flavor.infra_provider "capv" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/capv
      wait: true

  cabpk:  # kubeadm
    enabled: '{{ eq .Values.cluster.flavor.bootstrap_provider "cabpk" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/cabpk
      wait: true

  cabpr:  # RKE2
    enabled: '{{ eq .Values.cluster.flavor.bootstrap_provider "cabpr" }}'
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
    kustomization_spec:
      path: ./kustomize-components/cabpr
      wait: true

  cluster:
    enabled: yes
    repo: capi-bootstrap
    depends_on:
      - name: capi
      - name: '{{ .Values.cluster.flavor.infra_provider }}'
      - name: '{{ .Values.cluster.flavor.bootstrap_provider }}'
    labels:
      suspend-on-pivot: "yes"  # this component must be suspended before pivot
    kustomization_spec:
      # see note below under .cluster
      # the choice made here, for now, requires setting other values consistently under .cluster.xxx
      path: ./kustomize-components/cluster-manifests/_override_me_/base
      postBuild:
        substituteFrom:
        - kind: ConfigMap
          name: cluster-vars
      healthChecks:
        - apiVersion: cluster.x-k8s.io/v1beta1
          kind: Cluster
          name: management-cluster
          namespace: default

  calico:
    enabled: '{{ eq .Values.cluster.flavor.bootstrap_provider "cabpk" }}'
    depends_on:
      - name: cluster
    helm_repo_url: https://projectcalico.docs.tigera.io/charts
    helmrelease_spec:
      targetNamespace: tigera-operator
      install:
        createNamespace: true
      chart:
        spec:
          chart: tigera-operator
          version: v3.24.5

  cinder-csi:
    enabled: '{{ and (.Values.cluster.flavor.infra_provider | eq "capo") (.Values.phase | eq "management") }}'
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.3.0
      targetNamespace: cinder-csi
      install:
        createNamespace: true
      values:
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.cluster.capo.storageClass.name }}"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.cluster.capo.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            [Global]
            auth-url = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url }}"
            tenant-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name }}"
            domain-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name }}"
            username = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}"
            password = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password }}"
            region = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.region }}"
            tls-insecure = "{{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}"
            [BlockStorage]
            ignore-volume-az = true

  rancher:
    enabled: no
    repo: capi-bootstrap
    depends_on:
      - name: cert-manager
      - name: k8s-gateway
    helm_repo_url: https://releases.rancher.com/server-charts/stable
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: "2.6.9"
          sourceRef:
            namespace: default
      targetNamespace: cattle-system
      interval: 10m0s
      install:
        createNamespace: true
        remediation:
          retries: 3
          remediateLastFailure: false
      upgrade:
        remediation:
          retries: 3
      values:
        bootstrapPassword: "{{ .Values.cluster.admin_password }}"
        useBundledSystemChart: true
        hostname: '{{ .Values.cluster.rancher.external_hostname }}'
        ingress:
          enabled: true
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ get .Values.proxies "no_proxy" }}'
      postRenderers:
        - kustomize:
            patches:
              # this is to avoid that the too-short default liveness probe
              # prevents the Rancher installation from finishing before the pod is killed
              - patch: |-
                  apiVersion: apps/v1
                  kind: Deployment
                  metadata:
                    name: rancher
                  spec:
                    template:
                      spec:
                        containers:
                          - name: rancher
                            livenessProbe:
                              initialDelaySeconds: 120
                              periodSeconds: 30
                              failureThreshold: 20
                target:
                  kind: Deployment
                  name: rancher
                  namespace: cattle-system

  rancher-webhook-service:
    enabled: '{{ include "is-component-enabled" (tuple . "rancher") | eq "true" }}'
    repo: capi-bootstrap
    depends_on:
      - name: rancher
    kustomization_spec:
      path: ./kustomize-components/rancher-webhook

  k8s-gateway:
    enabled: no
    repo: capi-bootstrap
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: "2.0.0"
          sourceRef:
            namespace: default
      interval: 1m0s
      values:
        domain: "sylva"
        replicaCount: 3
        service:
          loadBalancerIP: "{{ .Values.cluster.capo.kube_vip_ip }}"
          annotations:
            metallb.universe.tf/allow-shared-ip: "share-vip-{{ .Values.cluster.capo.kube_vip_ip }}"

  capi-rancher-import:
    enabled: '{{ include "is-component-enabled" (tuple . "rancher") | eq "true" }}'
    repo: capi-rancher-import
    depends_on:
      - name: rancher
      - name: k8s-gateway
    helmrelease_spec:
      chart:
        spec:
          sourceRef:
            namespace: default
          chart: charts/capi-rancher-import
          reconcileStrategy: Revision
          valuesFiles:
            - charts/capi-rancher-import/values.yaml
      interval: 1m0s
      values:
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.cluster.rancher.external_hostname }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: component-capi-rancher-import
            namespace: default
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    enabled: no
    repo: capi-bootstrap
    helm_repo_url: https://kubernetes.github.io/ingress-nginx
    helmrelease_spec:
      chart:
        spec:
          chart: ingress-nginx
          version: "4.4.0"
          sourceRef:
            namespace: default
      interval: 1m0s
      values:
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            externalIPs:
            - "{{ .Values.cluster.capo.kube_vip_ip }}"

  first-login-rancher:
    enabled: '{{ include "is-component-enabled" (tuple . "rancher") | eq "true" }}'
    repo: capi-bootstrap
    depends_on:
      - name: rancher
    kustomization_spec:
      path: ./kustomize-components/kube-job
      wait: true
      force: true
      patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: first-login-rancher-job
          - op: replace
            path: /spec/backoffLimit
            value: 10
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/first-login-rancher.sh" | indent 4 }}
      - target:
          kind: Job
          name: kube-job
        patch: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ignored
          spec:
            template:
              spec:
                containers:
                - name: run-script
                  env:
                  - name: RANCHER_EXTERNAL_URL
                    value: https://{{ .Values.cluster.rancher.external_hostname }}/

  flux-webui:
    depends_on:
      - name: flux-system
    repo: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        installCRDs: true
        rbac:
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "wacth" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: flux.sylva
              paths:
                - path: /   # setting this to another value like '/flux' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
    helm_secret_values:
      adminUser:
        create: true
        username: admin
        passwordHash: '{{ htpasswd "" .Values.cluster.flux_webui.admin_password | trimPrefix ":" }}' # we don't want the "<user>:" part generated by htpasswd

  # this is an example, still incomplete
  # it would be used along with a definition of a 'workload-cluster' component
  # and it would need to have proper customization of ${CLUSTER_NAME} and ${ENABLE_MONITORING}
  cluster-import:
    enabled: no
    repo: capi-bootstrap
    depends_on:
      - name: first-login-rancher
    kustomization_spec:
      path: ./kustomize-components/cluster-import
      postBuild:
        substitute:
          CLUSTER_FLAVOR: "{{ upper .Values.cluster.flavor.bootstrap_provider }} {{ upper .Values.cluster.flavor.infra_provider }}"

  monitoring:
    enabled: no
    repo: capi-bootstrap
    wait: true
    depends_on:
      - name: rancher
    kustomization_spec:
      path: ./kustomize-components/monitoring
      postBuild:
        substituteFrom:
        - kind: Secret
          name: management-cluster-secrets

## stuff related to the 'cluster' component

cluster:
  name: management-cluster
  # TODO: derive kubeconfig secret name from this ^

  # Admin password that will be configured by default on various components
  admin_password: '{{ randAscii 64 }}'

  # image reference depends provider
  image: registry.gitlab.com/t6306/components/docker-images/rke2-in-docker:v1-24-4-rke2r1

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' component
  # e.g. you can use ./management-cluster-def/rke2-capd
  flavor:
    infra_provider: please-file-me     # capd or capo
    bootstrap_provider: please-file-me # cabpr (RKE2) or cabpk (kubeadm)

  # cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened

  cis_benchmark_scan_profile: '{{ eq .Values.cluster.flavor.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

  capo:
    ssh_key_name:
    image: capo-ubuntu-2004-kube-v1.23.6-calico-3.23.1
    network_id:  # openstack network to use as external network
    clouds_yaml: # (this is a dict, not a YAML string)
      clouds:
        capo_cloud:
          auth:
            auth_url: # replace me
            user_domain_name: # replace me
            project_domain_name: # replace me
            project_name: # replace me
            username: # replace me
            password: # replace me
            region: # optional
          verify: # e.g. false
    #cacert: # cert used to validate CA of OpenStack APIs
    kube_vip_ip: 55.55.55.55
    storageClass:
      name: foo  # name of the storageClass to be created
      type: xxx  # this is a cinder volume type e.g. 'ceph_sas' (must exist in openstack)

  flux_webui:
    admin_user: admin
    # admin password is to a random value by default, just to avoid setting a default that everyone would know
    # /!\ but the password set here will be generated by Helm and then hashed (see passwordHash in flux-webui
    # component definition) so this random password will be unknown & unusable
    admin_password: '{{ .Values.cluster.admin_password }}'

  rancher:
    external_hostname: rancher.sylva

capd:
  docker_host: unix:///var/run/docker.sock

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""

# 'phase' determines whether we are instantiating this chart
# on the bootstrap cluster or on the management cluster
#
# this is used to determine which component manifests to generate
# for a given item under 'components':
# if 'enabled' is set to 'management-only' then the component is enabled only if 'phase: management'
phase: management

component_default_enable: management-only  # this simply ensures that adding a component in this file under components does not implicitly enable it for "bootstrap"
