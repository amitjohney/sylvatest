# vSphere cluster helm chart

This helm chart is used to install a Kubernetes cluster in a vCenter. It installs the VM instances and installs a K8s cluster on them.

## Prerequisites

It is required to have a vCenter accessible with credentials (e.g. username and password). These credentials are needed to access the vCenter and create the cluster (see below).
It is required to have a VM template for VM instances, in order to deploy them with the helm repo installation command. To create the template, it is useful to deploy an Open Virtual Appliance (OVA) in vCenter, deploy a VM from it and mark it as immutable template. Find out here [how to create an OVA and deploy it in vCenter](https://cloudmaniac.net/ova-ovf-deployment-using-govc-cli/) and [how to create a VM template](https://github.com/kubernetes-sigs/cluster-api-provider-vsphere/blob/main/docs/getting_started.md#uploading-the-machine-images).
In order to deploy the K8s cluster, it is required to have a K8s management cluster with ClusterAPI CAPV provider installed. The helm chart will be installed in the management cluster, so that CAPV provider can process the yaml files and deploy the cluster on vCenter.

## Helm repo installation command

```shell
$ helm upgrade --install --namespace cluster-test --create-namespace \
    --set cluster.controlPlaneEndpoint.host="test" \
    --set machines.controlPlane.template="template" \
    --set machines.workers.worker-md-0.template="template" \
    --set vsphere.network="network" \
    --set vsphere.username="test" \
    --set vsphere.password="test" \
    cluster-test chart/cluster-api-vsphere
```

This command upgrades or installs (if not installed) this helm repository. if --install is set, --create-namespace creates the release namespace if not present.

### --set values

| Key | Example value | Description |
|-----|---------------|-------------|
| cluster.controlPlaneEndpoint.host | 163.162.114.137 | The hostname on which the API server is serving (can be IP address or hostname) |
| machines.controlPlane.template | ubuntu-2004-kube-v1.22.8 | Template's name used for VM instances of k8s cluster control plane nodes |
| machines.workers.worker-md-0.template | ubuntu-2004-kube-v1.22.8 | Template's name used for VM istances of k8s cluster worker nodes |
| vsphere.network | infra-training-1079 | vSphere's network VM istances belong to |
| vsphere.username | vsphereuser.local | Username used to access vCenter |
| vsphere.password | mysuperstrongpassword | Password used to access vCenter |

Replace the values in --set fields with your specific values. Replace "cluster-test chart/cluster-api-vsphere" with the name of the release and the name of the helm repository.

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| commonLabels | object | `{}` | Labels to add to all objects generated by this chart |
| commonAnnotations | object | `{}` | Annotations to add to all objects generated by this chart |
| wait.cluster | bool | `false` | Deploy a job waiting for machines to rollout after deployment |
| wait.apiServer | bool | `false` | Deploy a job waiting for apiServer to be available |
| wait.timeout | int | `900` | max time waiting for resources to rollout |
| cluster.name | string | `""` | Cluster name. If unset, the release name will be used |
| cluster.controlPlaneEndpoint.host | string | `""` | IP or DNS name of the kubernetes endpoint |
| cluster.controlPlaneEndpoint.port | int | `6443` | Kubernetes endpoint port |
| cluster.controlPlaneEndpoint.interface | string | `""` | Linux interface on the node |
| cluster.podCidrBlocks | list | `["19.0.0.0/16"]` | Network CIDR for pods |
| cluster.servicesCidrBlocks | list | `["56.0.0.0/16"]` | Network CIDR for services |
| cluster.additionalClusterResourceSet | list | `[]` | Additionnal ClusterResource to add to ClusterResourceSet |
| kubernetes.version | string | `"v1.24.7"` | Version of kubernetes |
| kubernetes.additionnalClientCaFile | string | `""` | Pem encoded certificate to authenticate clients over x509 |
| kubernetes.kubeadm.commonKubeletExtraArgs | object | `{}` | Additional kubelet command line arguments for init and join configurations |
| kubernetes.kubeadm.controlPlane.clusterConfiguration | object | `{}` | Kubeadm cluster configuration, more info : https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/ |
| kubernetes.kubeadm.controlPlane.additionalFiles | list | `[]` | Additional files to create on the kubeadm control plane instances |
| kubernetes.kubeadm.controlPlane.preKubeadmAdditionalCommands | list | `[]` | Additional commands to execute on control plane before kubeadm join/init |
| kubernetes.kubeadm.controlPlane.postKubeadmAdditionalCommands | list | `[]` | Additional commands to execute on control plane after kubeadm join/init |
| kubernetes.kubeadm.controlPlane.patchesDirectory | string | `""` | Directory containing kubeadm patches on target vms |
| kubernetes.kubeadm.workers.files | list | `[]` | files to create on the workers instances |
| kubernetes.kubeadm.workers.preKubeadmAdditionalCommands | list | `[]` | Additional commands to execute on workers before kubeadm join/init |
| kubernetes.kubeadm.workers.postKubeadmAdditionalCommands | list | `[]` | Additional commands to execute on workers after kubeadm join/init |
| machines.dhcp4 | bool | `true` | Use dhcp for ipv4 configuration |
| machines.nameServers | list | `[]` | Nameservers for VMs DNS resolution |
| machines.searchDomains | list | `[]` | Search domains suffixes to configure on VMs |
| machines.domain | string | `""` | VM network domain |
| machines.gateway | string | `""` | IPv4 gateway |
| machines.users | list | `[]` | users to create on machines see https://github.com/kubernetes-sigs/cluster-api/blob/main/bootstrap/kubeadm/api/v1beta1/kubeadmconfig_types.go#L257 for documentation about user config object |
| machines.controlPlane.annotations | object | `{}` | Control plane VsphereMachineTemplate annotations |
| machines.controlPlane.replicas | int | `3` | Number of control plane VMs instances |
| machines.controlPlane.kubeVipVersion | string | `"v0.5.5"` | kube-vip version |
| machines.controlPlane.ipAddrs | list | `[]` | ipaddrs passed to capi if necessary |
| machines.controlPlane.criSocket | string | `"/var/run/containerd/containerd.sock"` | Path of the CRI socket to use |
| machines.controlPlane.resourcePool | string | `""` | Control plane VSphere resource pool |
| machines.controlPlane.template | string | `""` | Control plane VSphere machine template to clone, must contain kubeadm at the same version as specified in kubernetes.version |
| machines.controlPlane.folder | string | `""` | Control plane VSphere folder to store VM |
| machines.controlPlane.storagePolicy | string | `""` | Control plane VSphere storage policy name to use for disks |
| machines.controlPlane.dataStore | string | `""` | Control plane VSphere datastore to create/locate machine |
| machines.controlPlane.cpuCount | int | `2` | Control plane Number of vCPUs to allocate to controlPlane instances |
| machines.controlPlane.diskSizeGiB | int | `40` | Control plane Disk size of VM in GiB |
| machines.controlPlane.memorySizeMiB | int | `4096` | Control plane Memory to allocate to controlPlane VMs |
| machines.controlPlane.kubeletExtraArgs | object | `{}` | Additional kubelet command line arguments for join configurations |
| machines.controlPlane.nodeDrainTimeout | string | `""` | Node drain timeout is the total amount of time that the controller will spend on draining a node |
| machines.controlPlane.machineHealthCheck.enabled | bool | `false` | Deploys a machineHealthCheck object for the controlPlane |
| machines.controlPlane.machineHealthCheck.maxUnhealthy | string | `"100%"` | Any further remediation is only allowed if at most "MaxUnhealthy" machines are not healthy |
| machines.controlPlane.machineHealthCheck.nodeStartupTimeout | string | `""` | Machines older than this duration without a node will be considered to have failed and will be remediated |
| machines.controlPlane.machineHealthCheck.unhealthyConditions | list | `[{"status":"Unknown","timeout":"300s","type":"Ready"},{"status":"False","timeout":"300s","type":"Ready"}]` | list of the conditions that determine whether a node is considered unhealthy. if any of the conditions is met, the node is unhealthy. |
| machines.controlPlane.machineHealthCheck.unhealthyRange | string | `""` | remediation is only allowed if the number of machines selected by "selector" as not healthy is within the range of "UnhealthyRange" |
| machines.workers | object | `{}` | Worker pools, more details on its configuration in [Worker pools configuration](#worker-pools-configuration) |
| vsphere.dataCenter | string | `""` | Datacenter to use |
| vsphere.network | string | `""` | VSphere network for VMs and CSI |
| vsphere.username | string | `""` | Vsphere username |
| vsphere.password | string | `""` | VSphere password |
| vsphere.server | string | `""` | VSphere server dns name |
| vsphere.tlsThumbprint | string | `""` | VSphere https TLS thumbprint |
| vsphereCsi.enabled | bool | `true` | Installs vsphere-csi on the cluster |
| vsphereCsi.cloudControllerManager.resources | object | `{"requests":{"cpu":"200m"}}` | resources of vsphere-cloud-controller-manager |
| vsphereCsi.controller.csiAttacher.resources | object | `{}` | resources of the container csi-attacher in vsphere-csi-controller |
| vsphereCsi.controller.csiResizer.resources | object | `{}` | resources of the container csi-resizer in vsphere-csi-controller |
| vsphereCsi.controller.vsphereCsiController.resources | object | `{}` | resources of the container vsphere-csi-controller in vsphere-csi-controller |
| vsphereCsi.controller.vsphereSyncer.resources | object | `{}` | resources of the container vsphere-syncer in vsphere-csi-controller |
| vsphereCsi.controller.csiProvisioner.resources | object | `{}` | resources of the container csi-provisioner in vsphere-csi-controller |
| vsphereCsi.controller.csiSnapshotter.resources | object | `{}` | resources of the container csi-snapshotter in vsphere-csi-controller |
| vsphereCsi.controller.livenessProbe.resources | object | `{}` | resources of the container liveness-probe in vsphere-csi-controller |
| vsphereCsi.node.nodeDriverRegistrar.resources | object | `{}` | resources of the container node-driver-registrar in vsphere-csi-node |
| vsphereCsi.node.vsphereCsiNode.resources | object | `{}` | resources of the container vsphere-csi-node in vsphere-csi-node |
| vsphereCsi.node.livenessProbe.resources | object | `{}` | resources of the container liveness-probe in vsphere-csi-node |
| storageClass.enabled | bool | `false` | Create storage class |
| storageClass.name | string | `"default"` | Storage class name |
| storageClass.default | bool | `true` | Define storage class as default class |
| storageClass.reclaimPolicy | string | `"Delete"` | Storage class reclaimPolicy |
| storageClass.fsType | string | `"ext4"` | Storage class fsType |
| storageClass.storagePolicy | string | `""` | VSphere Storage policy |
| cni.calico.enabled | bool | `false` | Installs cni calico on the cluster |

### Worker pools configuration

You can configure as many worker pools as desired, the value `machines.workers` takes a map in which each key is the worker pool name :

__exemple__:

```yaml
# -- Name of the worker pool, you can define as many others pools as desired
worker-md-0:
  # -- Number of worker VMs instances
  replicas: 1
  # -- MachineDeployment annotations
  machineDeploymentAnnotations: {}
  # -- Labels to add to the machines created by the machineDeployment
  machinesLabels: {}
  # -- Labels to add to the machineDeployment selector to match the machines
  machinesSelectors: {}
  # -- workers VsphereMachineTemplate annotations
  annotations: {}
  # -- ipaddrs passed to capi if necessary
  ipAddrs: []
  # -- Path of the CRI socket to use
  criSocket: "/var/run/containerd/containerd.sock"
  # -- workers VSphere resource pool
  resourcePool: ""
  # -- workers VSphere machine template to clone, must contain kubeadm at the same version as specified in kubernetes.version
  template: ""
  # -- workers VSphere folder to store VM
  folder: ""
  # -- workers VSphere storage policy name to use for disks
  storagePolicy: ""
  # -- workers VSphere datastore to create/locate machine
  dataStore: ""
  # -- Number of vCPUs to allocate to worker instances
  cpuCount: 4
  # -- disk size of workers VM in GiB
  diskSizeGiB: 100
  # -- Memory to allocate to worker VMs
  memorySizeMiB: 8192
  # -- files to create on the workers instances of this pool
  files: []
  # -- Additional commands to execute on workers before kubeadm join/init on this pool
  preKubeadmAdditionalCommands: []
  # -- Additional commands to execute on workers after kubeadm join/init on this pool
  postKubeadmAdditionalCommands: []
  # -- Additional kubelet command line arguments for join configurations
  kubeletExtraArgs: {}
  # -- Node drain timeout is the total amount of time that the controller will spend on draining a node
  nodeDrainTimeout: ""
  machineHealthCheck:
    # -- Deploys a machineHealthCheck object for the workerPool
    enabled: false
    # -- Any further remediation is only allowed if at most "MaxUnhealthy" machines are not healthy
    maxUnhealthy: ""
    # -- Machines older than this duration without a node will be considered to have failed and will be remediated
    nodeStartupTimeout: ""
    # -- list of the conditions that determine whether a node is considered unhealthy. if any of the conditions is met, the node is unhealthy.
    unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: Ready
      status: "False"
      timeout: 300s
    # -- remediation is only allowed if the number of machines selected by "selector" as not healthy is within the range of "UnhealthyRange"
    unhealthyRange: ""
```

### Client x509 authentication

To configure client x509 authentication, you need to specify `kubernetes.additionnalClientCaFile` to the client ca as a PEM encoded certificate :

```yaml
kubernetes:
  additionnalClientCaFile: |
    -----BEGIN CERTIFICATE-----
    [...]
    -----END CERTIFICATE-----
```

### Additional resources on the new cluster

You can leverage the ClusterAPI `clusterResourceSet` capability through this chart to deploy additionnals resources on the newly created cluster.
To do so, create either a Secret or a ConfigMap in the namespace which will host the cluster resources.

Then, in the values.yaml, specify which resources should be deployed on the cluster:

```yaml
cluster:
  additionalClusterResourceSet:
    - kind: ConfigMap
      name: your_configmap
```
