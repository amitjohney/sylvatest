# ############################################################################
#
# This Helm values files contains override used to create a workload cluster
# with sylva-units chart and deploy a selected subset of units into it.
#
# ############################################################################

# This value override file is meant to deploy sylva-units _in the mgmt cluster_
# _in a dedicated namespace_, it will produce Flux Kustomization and HelmReleases
# _in the mgmt cluster_ and those will ultimately deploy Kubernetes resources
# in the workload cluster itself.
#
# There are a few exceptions:
# - the 'cluster' unit produces CAPI resource for the cluster _in the mgmt cluster_

# when this values override file is used, it is assumed that
# - `cluster.capi_providers.(infra|bootstrap)_provider` are set to relevant values
# - `openstack` for Openstack settings needed by cinder-csi
# - etc.
# any settings at the root of values, or under 'cluster', which is
# needed for any unit listed below, will have to be specified

cluster:
  name: '{{ .Release.Namespace }}'

# ensure that our unit Kustomizations are deployed
# in the workload cluster
# (some units are deployed on mgmt cluster and will use
#  "kustomization-deployed-on-mgmt-cluster" or "helmrelease-deployed-on-mgmt-cluster"
#  unit templates)
unit_kustomization_spec_default:
  kubeConfig:
    secretRef:
      name: '{{ .Values.cluster.name }}-kubeconfig'
  targetNamespace: sylva-system  # (in workload cluster)

# for units that rely on Helm, the Kustomization will
# produce a HelmRelease which needs to be in the workload cluster ns
# in the mgmt cluster, so we "reset" the targetNamespace and kubeconfig
# set right above and .... (more below ...)
unit_helmrelease_kustomization_spec_default:
  kubeConfig: null
  targetNamespace: '{{ .Release.Namespace }}'  # (in mgmt cluster)

# ... and it's at the level of the HelmRelease that we ensure
# that the Helm release is deployed in the workload cluster
# (in sylva-system ns)
unit_helmrelease_spec_default:
  kubeConfig:
    secretRef:
      name: '{{ .Values.cluster.name }}-kubeconfig'
  targetNamespace: sylva-system  # (in workload cluster)
  storageNamespace: sylva-system  # (in workload cluster)

# Enable delete hook for workload cluster
delete_hook:
  enabled: true
  job_timeout: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" | ternary 3600 1200 | include "preserve-type" }}'


unit_definition_defaults:
  depends_on:
    # all units depend on mgmt-cluster-ready, except root-dependency (everything depends on it, so it can't depend on anything without creating a dependency loop)
    # and mgmt-cluster-ready itself
    mgmt-cluster-ready: '{{ not (list "mgmt-cluster-ready" "root-dependency" | has .Values._unit_name_) }}'

unit_templates:

  base-deps:
    # we put here the list of base units that most units will need to depend on
    # they correspond to "the workload cluster is ready for stuff to be deployed on it"
    depends_on:
      cluster-reachable: true
      namespace-defs: true
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'

  # used for units which are deployed on mgmt cluster itself
  kustomization-deployed-on-mgmt-cluster:
    depends_on:
      cluster-machines-ready: false
    # the resources produced by this Kustomization will live in the mgmt cluster
    # so we override what is defined in unit_kustomization_spec_default
    kustomization_spec:
      kubeConfig: null
      targetNamespace: null

  helmrelease-deployed-on-mgmt-cluster:
    depends_on:
      cluster-machines-ready: false
    helmrelease_spec:
      # the resources produced by this HelmRelease will live in the mgmt cluster:
      # so we override what is defined in unit_helmrelease_spec_default
      kubeConfig: null
      targetNamespace: '{{ .Release.Namespace }}'  # (in mgmt cluster)
      storageNamespace: '{{ .Release.Namespace }}'  # (in mgmt cluster)


# here we select which units we want to enable for a workload cluster
units:
  cabpe:  # EKS
    enabled: false
  capa:
    enabled: false

  root-dependency:
    enabled: on
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster

  mgmt-cluster-ready:
    # all units (except 'root-dependency') depend on this unit
    # (see unit_definition_defaults.depends_on.mgmt-cluster-ready)
    enabled: on
    info:
      description: (workload cluster) this unit reflects the readiness of the mgmt cluster
      details: this unit acts as simple dependency lock to prevent deploying a workload cluster before the mgmt cluster is ready
    unit_templates:
    - dummy
    - kustomization-deployed-on-mgmt-cluster
    kustomization_spec:
      healthChecks:
        - apiVersion: v1
          kind: ConfigMap
          name: sylva-units-status
          namespace: sylva-system

  capo-cloud-config:
    enabled: on
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster

  capo-cluster-resources:
    enabled: on
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster
    depends_on:
      # the heat-operator that we depend on is not in the per-cluster namespace
      heat-operator: false
    kustomization_spec:
      # heatstack parameters specific to workload clusters:
      _patches:
        - target:
            kind: HeatStack
          patch: |
            - op: replace
              path: /spec/heatStack/template/parameters/common_sg_rules
              value:
                type: json
                description: "Common security group rules associated with the control plane and worker VMs"
                default:
                  direction: [ingress, ingress, ingress, egress]
                  port: ['80', '443', '22', '53']
                  protocol: [tcp, tcp, tcp, udp]
                  remote_ip_prefix: [0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0]

  sandbox-privileged-namespace:
    kustomization_spec:
      targetNamespace: sandbox # because this is set for sylva-system in workload-clusters

  os-images-info:
    enabled: true
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster

  get-openstack-images:
    enabled: true
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster

  cluster-bmh:
    enabled: true
    unit_templates:
    - helmrelease-deployed-on-mgmt-cluster

  cluster:
    enabled: true
    unit_templates:
    - helmrelease-deployed-on-mgmt-cluster

  cluster-reachable:
    enabled: true
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster
    - dummy

  cluster-ready:
    enabled: true
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster
    - dummy
    kustomization_spec:
      targetNamespace: '{{ .Release.Namespace }}'

  cluster-machines-ready:
    enabled: true
    unit_templates:
    - kustomization-deployed-on-mgmt-cluster

  namespace-defs:
    enabled: true
    depends_on:
      # some dependencies of base-deps are inherited, but don't apply here:
      namespace-defs: false
      calico: false
    kustomization_spec:
      targetNamespace: null  # if we don't do this, there are errors because namespaces are a non-namespaced resource

  calico-crd:
    enabled: true
    unit_templates:
    - base-deps
    # some dependencies of base-deps are inherited, but don't apply here:
    depends_on:
      calico: false

  tigera-clusterrole:
    enabled: true
    unit_templates:
    - base-deps
    # some dependencies of base-deps are inherited, but don't apply here:
    depends_on:
      calico: false

  calico:
    enabled: true
    unit_templates:
    - base-deps
    # some dependencies of base-deps are inherited, but don't apply here:
    depends_on:
      namespace-defs: false
      calico: false

  harbor-init:
    # this unit isn't relevant in workload clusters
    # postgress passwords are not handled with Vault/ExternalSecret
    enabled_conditions: [false]

  harbor-postgres:
    enabled: true
    helmrelease_spec:
      values:
        auth:
          existingSecret: ""  # cancel the setting coming from values.yaml
    helm_secret_values:
      # in a workload cluster, postgress passwords are not handled with Vault/ExternalSecret
      # but generated with Helm by sylva-units
      auth:
        username: harbor
        password: '{{ .Values._internal.harbor_postgres_user_password }}'
        database: harbor
        postgresPassword: '{{ .Values._internal.harbor_postgres_admin_password }}'
        replicationUsername: repl_user
        replicationPassword: '{{ .Values._internal.harbor_postgres_replication_password }}'

  harbor:
    # can be enabled at runtime if desired
    enabled: false
    helmrelease_spec:
      values:
        database:
          external:
            existingSecret: ""  # cancel the setting coming from values.yaml
    helm_secret_values:
      # in a workload cluster, postgress passwords are not handled with Vault/ExternalSecret
      # but generated with Helm by sylva-units
      database:
        external:
          username: harbor
          password: '{{ .Values._internal.harbor_postgres_user_password }}'

  metallb:
    enabled: true

  ingress-nginx:
    enabled: true
    depends_on:
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    helmrelease_spec:
      values:
        controller:
          service:
            externalIPs: '{{ tuple (list .Values.cluster_virtual_ip) (not (.Values.cluster.capi_providers.infra_provider | eq "capo")) | include "set-only-if" }}'
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: controller.service.externalIPs[0]
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'

  monitoring-crd:
    enabled: true

  monitoring:
    enabled: true
    depends_on:
      cluster-import: '{{ tuple . "cluster-import" | include "unit-enabled" }}'
    helmrelease_spec:
      valuesFrom:
        - kind: ConfigMap
          name: '{{ .Values.cluster.name }}-capi-monitoring-info'  # this configmap is produced by a kyverno policy defined by the "rancher-monitoring-clusterid-inject" mgmt unit
          optional: false

  kubevirt:
    # can be enabled at runtime if desired
    enabled: false

  kubevirt-test-vms:
    enabled: false

  longhorn-crd:
    enabled: true

  longhorn:
    # can be enabled at runtime if desired
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'

  multus:
    # can be enabled at runtime if desired
    enabled: false

  multus-ready:
    enabled: true

  sriov:
    # can be enabled at runtime if desired
    enabled: false

  sriov-crd:
    enabled: yes

  sriov-resources:
    # can be enabled at runtime if desired
    enabled: false

  cinder-csi:
    enabled: true

  ceph-csi-cephfs:
    enabled: false

  kyverno:
    enabled: true
    # in mgmt cluster, kyverno is part of base-deps, so cannot depend on itself, but here in workload-cluster we need it
    unit_templates:
    - base-deps

  cluster-import:
    enabled: false  # overriden depending on whether Rancher is enabled or not (eg. via shared-workload-clusters-settings)
    info:
      description: imports workload cluster into Rancher
      internal: true
    unit_templates:
    - base-deps
    - kustomization-deployed-on-mgmt-cluster
    repo: sylva-core
    kustomization_spec:
      # this Kustomization lives in the mgmt cluster
      # **and it needs a specific kubeconfig to make a clean Rancher import**
      kubeConfig:
        secretRef:
          name: cluster-creator-kubeconfig
          key: kubeconfig
      targetNamespace: '{{ .Release.Namespace }}'  # (in mgmt cluster)
      wait: false
      path: ./kustomize-units/cluster-import
      postBuild:
        substitute:
          CLUSTER_FLAVOR: '{{ upper .Values.cluster.capi_providers.bootstrap_provider }} {{ upper .Values.cluster.capi_providers.infra_provider }}'
          CLUSTER_NAME: '{{ .Values.cluster.name }}'
        substituteFrom:
          - kind: Secret
            name: cluster-creator-kubeconfig  # this secret is created by a Kyverno policy defined by cluster-creator-login/cluster-creator-policy mgmt units
      healthChecks:
        # this resource is created by capi-rancher-import based on the existence of
        # the Cluster.provisioning.cattle.io produced by the kustomization
        - apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
          kind: Kustomization
          name: "cattle-agent-{{ .Values.cluster.name }}"
          namespace: '{{ .Release.Namespace }}'
        # this resource would be checked by "wait: true" but we have
        # to check it explicitly because we use `healthChecks` which implies `wait: false`
        - apiVersion: provisioning.cattle.io/v1
          kind: Cluster
          name: "{{ .Values.cluster.name }}-capi"
          namespace: '{{ .Release.Namespace }}'

  vsphere-cpi:
    enabled: true
    # setting the same dependencies of the capv cluster unit in order to run in parallel

  vsphere-csi-driver:
    enabled: true

  logging-crd:
    enabled: true

  logging:
    enabled: false

  logging-config:
    enabled: true
    kustomization_spec:
      targetNamespace: "cattle-logging-system"

  sylva-prometheus-rules:
    enabled: true

_internal:
  # for capm3 the os-image-info configmap is copied by kyverno from main sylva-units release, corresponding to what os-image-server will serve
  # for capo the os-image-info configmap is re-generated by os-image-info unit
  os_images_info_configmap:
    '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" | ternary "kyverno-cloned-os-images-info-capm3" .Values._internal.default_os_images_info_configmap }}'

  harbor_postgres_admin_password: '{{ include "internalPersistentRandomPasswd" (tuple . "harbor_postgres_admin_password") }}'
  harbor_postgres_user_password: '{{ include "internalPersistentRandomPasswd" (tuple . "harbor_postgres_user_password") }}'
  harbor_postgres_replication_password: '{{ include "internalPersistentRandomPasswd" (tuple . "harbor_postgres_replication_password") }}'

  checks:
    # checks that the selected infra/bootstrap providers are supported
    # by the management cluster
    capi_infra_provider: >-
      {{
      .Values._internal.sylva_mgmt_enabled_units
         | dig .Values.cluster.capi_providers.infra_provider ""
         | required (printf "CAPI infra provider '%s' isn't enabled in Sylva management cluster" .Values.cluster.capi_providers.infra_provider)
      }}
    capi_bootstrap_provider: >-
      {{
      .Values._internal.sylva_mgmt_enabled_units
         | dig .Values.cluster.capi_providers.bootstrap_provider ""
         | required (printf "CAPI bootstrap provider '%s' isn't enabled in Sylva management cluster" .Values.cluster.capi_providers.bootstrap_provider)
      }}
