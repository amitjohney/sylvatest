# Default values for sylva-units.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret: {}
  # when using gitlab, git creds can be used as registry creds
  #registry.gitlab.com:
  #  username: your_user_name
  #  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s

oci_repo_spec_default:
  interval: 60m0s

source_templates: # template to generate Flux GitRepository/OCIRepository resources
  # <repo-name>:
  #   kind:  GitRepository/OCIRepository
  #   spec:  # partial spec for a Flux resource
  #     url: https://gitlab.com/sylva-projects/sylva-core.git
  #     #secretRef: # is autogenerated based on 'auth'
  #     ref: # can be overridden per-unit, with 'ref_override'
  #       branch: main
  #   auth: # optional 'username'/'password' dict containing git authentication information
  #   existing_source: # optional, when this value is set the specified GitRepository or OCIRepository will be used instead of creating one based on 'spec'
  #     name: sylva-core
  #     kind: GitRepository or OCIRepository

  sylva-core:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-core.git
      ref:
        # this is provided only for reference
        # in practice the sylva-core framework will always override this ref so that the
        # currently checked out commit of sylva-core is used by sylva-units
        # (you can grep the code for "CURRENT_COMMIT" to find out why)
        branch: main

  capi-rancher-import:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capi-rancher-import.git
      ref:
        tag: 0.1.2

  weave-gitops:
    kind: GitRepository
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.22.0

  metal3:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3.git
      ref:
        tag: 0.1.0

  cluster-vsphere:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/cluster-vsphere.git
      ref:
        tag: 0.1.0

  local-path-provisioner:
    kind: GitRepository
    spec:
      url: https://github.com/rancher/local-path-provisioner.git
      ref:
        tag: v0.0.24

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'units'
unit_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: -1
      remediateLastFailure: false

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_kustomization_spec_default:
  path: ./kustomize-units/helmrelease-generic
  sourceRef: >-
    {{- $existing_source := get (index .Values.source_templates "sylva-core") "existing_source" -}}
    {{- if $existing_source -}}
      {{- $existing_source | include "preserve-type" -}}
    {{- else -}}
      {{- dict "kind" (index .Values.source_templates "sylva-core" | dig "kind" "GitRepository") "name" "sylva-core" | include "preserve-type" -}}
    {{- end -}}
  wait: true

# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository or OCIRepository (and Secret, TODO: the Secret don't need to be generated for each unit)
units:
  # <unit-name>:
  #   enabled: boolean or GoTPL
  #   repo: <name of the repo under 'source_templates'> (for use with kustomization_spec)
  #   helm_repo_url: URL of the Helm repo to use (for use with helmrelease_spec, but not mandatory, 'repo' can be used as well to use a git repo)
  #   labels: (optional) dict holding labels to add to the resources for this unit
  #   ref_override: optional, if defined, this dict will be used for the GitRepository or OCIRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: dict defining the dependencies of this unit, keys are unit names, values are booleans
  #               (these dependencies are injected in the unit Kustomization via 'spec.dependsOn')
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from unit_helmrelease_spec_default
  #                      # and from other fields in the unit definition
  #     _postRenderers: # this field can be used in this file, it will be merged into user-provided 'postRenderers'
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from unit_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-unit-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <unit>.spec.path  (if <git repo template> has spec.url defined)
  #     _patches: # this field can be used in this file, it will be merged into user-provided 'patches'
  #     _components: # this field can be used in this file, it will be merged into user-provided 'components'
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret

  flux-system:
    # note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
    # this units contains Flux definitions *to manage the Flux system itself via gitops*
    enabled: yes
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/flux-system/base
      targetNamespace: flux-system
      wait: true
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  kubed:
    enabled: no
    # Kubed may be used to copy secrets from a namespace to another. It can be usefull for example, if some unit need a registry secrets to pull images.
    # This chart could be used to generate registry secret, but it would have to create the unit namespace too in that case, which is not so clean
    # Using kubed, you may just add a label "add-registry-secret: sylva" to the target namespace, and kubed will copy secret in it
    helm_repo_url: https://charts.appscode.com/stable
    helmrelease_spec:
      chart:
        spec:
          chart: kubed
          version: v0.13.2
      targetNamespace: kubed
      install:
        createNamespace: true
      values:
        installCRDs: true

  cert-manager:
    enabled: yes
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.11.1
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true
        securityContext:
          seccompProfile: null
        webhook:
          securityContext:
            seccompProfile: null
        cainjector:
          securityContext:
            seccompProfile: null
        startupapicheck:
          securityContext:
            seccompProfile: null

  trivy-operator:
    enabled: no
    helm_repo_url: https://aquasecurity.github.io/helm-charts/
    helmrelease_spec:
      chart:
        spec:
          chart: trivy-operator
          version: 0.11.1
      targetNamespace: trivy-system
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 10000
          runAsUser: 10000
        serviceAccount:
          annotations: {}
          create: true
          name: trivy-operator
        trivy:
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" . }}'
          severity: UNKNOWN,HIGH,CRITICAL
        trivyOperator:
          scanJobPodTemplatePodSecurityContext:
            runAsGroup: 10000
            runAsUser: 10000


# sylva-ca provides a Certificate Authority for Components of the management cluster
  sylva-ca:
    enabled: yes
    depends_on:
      cert-manager: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca
      targetNamespace: cert-manager
      wait: true

# namespace-defs creates the namespaces to be used by various units
  namespace-defs:
    enabled: yes
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/namespace-defs
      wait: true
      prune: false

# Postgresql deployment for keycloak
  postgres:
    enabled: '{{ .Values.units.keycloak.enabled }}'
    depends_on:
      namespace-defs: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql
          version: 12.3.1
      targetNamespace: keycloak
      values:
        global:
          postgresql:
            auth:
              database: keycloak
          auth:
            username: postgres
        nameOverride: postgres

  vault-operator:
    enabled: true
    helm_repo_url: https://kubernetes-charts.banzaicloud.com
    helmrelease_spec:
      chart:
        spec:
          chart: vault-operator
          version: 1.19.0
      targetNamespace: vault
      install:
        createNamespace: true

# Vault assumes that the certificates vault-tls have been issued
  vault:
    enabled: yes
    depends_on:
      sylva-ca-certs: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      vault-operator: true
    annotations:
      sylvactl/readyMessage: "Vault UI can be reached at https://{{ .Values.cluster.vault.external_hostname }} ({{ .Values.cluster.vault.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/vault
      wait: true
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'

  cis-operator-crd:
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: 4.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator-crd: true
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: 4.0.0
      targetNamespace: cis-operator-system
      values:
        global:
          cattle:
            psp:
              enabled: true

  # this allows for running a CIS scan for management cluster
  # generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
  cis-operator-scan:
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cis-operator-scan
      wait: false
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cluster.cis_benchmark_scan_profile }}'

  keycloak:
    enabled: yes
    depends_on:
      sylva-ca: true
      sylva-ca-certs: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
      postgres: true
    annotations:
      sylvactl/readyMessage: "Keycloak UI can be reached at https://{{ .Values.cluster.keycloak.external_hostname }} ({{ .Values.cluster.keycloak.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_ADMIN_PASSWORD: '{{ .Values.cluster.admin_password }}'
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'
      healthChecks:  # cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        # the Keycloak StatefulSet set is produced, by the combination of Keycloak operator
        # and a Keycloak custom resource, it relies on the postgres DB also deployed by this unit
        # hence, checking for the health of this component can be done by checking this StatefulSet
        - apiVersion: apps/v1
          kind: StatefulSet
          name: keycloak
          namespace: keycloak

  keycloak-legacy-operator:
    enabled: '{{ .Values.units.keycloak.enabled }}'
    depends_on:
      keycloak: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-legacy-operator
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_ADMIN_PASSWORD: '{{ .Values.cluster.admin_password }}'
      wait: true
      _patches:
        - patch: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: keycloak-realm-operator
            spec:
              template:
                spec:
                  securityContext:
                    runAsUser: 10000
          target:
            kind: Deployment
            name: keycloak-realm-operator

  keycloak-resources:
    enabled: '{{ .Values.units.keycloak.enabled }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
    repo: sylva-core
    kustomization_substitute_secrets:
      RANCHER_CLIENT_SECRET: '{{ .Values.cluster.admin_password }}'
      FLUX_CLIENT_SECRET: '{{ .Values.cluster.admin_password }}' # We need to randomly generate this instead
    kustomization_spec:
      path: ./kustomize-units/keycloak-resources
      targetNamespace: keycloak
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.cluster.rancher.external_hostname }}'
          FLUX_DNS: '{{ .Values.cluster.flux.external_hostname }}'
      healthChecks:  #  cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakRealm
          name: sylva
          namespace: keycloak
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakClient
          name: rancher-client
          namespace: keycloak

  capi:
    enabled: yes
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capi
      wait: true

  capd:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: '{{ ternary "./kustomize-units/capd/base" "./kustomize-units/capd/rke2" (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") }}'
      postBuild:
        substitute:
          CAPD_DOCKER_HOST: '{{ .Values.cluster.capd.docker_host }}'
      wait: true

  capo:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capo
      wait: true

  capm3:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capm3
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  capv:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capv" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capv
      wait: true

  cabpk:  # kubeadm
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpk
      wait: true

  cabpr:  # RKE2
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpr
      wait: true

  metal3:
    enabled: no
    depends_on:
      cert-manager: true
    repo: metal3
    helmrelease_spec:
      chart:
        spec:
          chart: .
      install:
        createNamespace: true
      targetNamespace: metal3-system
      values:  # see https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3/-/blob/main/values.yaml
        global:
          storageClass: local-path
        # ironicIPADownloaderBaseURI:
        httpProxy: '{{ .Values.proxies.https_proxy }}'
        httpsProxy: '{{ .Values.proxies.https_proxy }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        persistence:
          ironic:
            accessMode: "ReadWriteOnce"
        services:
          ironic:
            # Specify the IP address used by Ironic service
            ironicIP: '{{ .Values.cluster.cluster_external_ip }}'
            # We allow shared IPs because we are exposing (by design) a single cluster external IP
            annotations:
              metallb.universe.tf/allow-shared-ip: '{{ .Values.cluster.cluster_external_ip }}'
        mariadb:
          auth:
            # if rootPassword is left empty a random one is generated at each installation but mariadb won't be able to reuse the same PVCs
            rootPassword: "changeme"
            # if replicationPassword is left empty a random one is generated at each installation but mariadb won't be able to reuse the same PVCs
            replicationPassword: "changeme"
          persistence:
            storageClass: local-path

  local-path-provisioner:
    enabled: yes
    repo: local-path-provisioner
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/chart/local-path-provisioner
      targetNamespace: kube-system
      install:
        createNamespace: true
      values:
        storageClass:
          defaultClass: '{{ not (tuple . (index .Values.units "cinder-csi" "enabled") | include "interpret-for-test") | include "as-bool" }}'
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: local-path-provisioner
                spec:
                  template:
                    spec:
                      containers:
                      - name: local-path-provisioner
                        securityContext:
                          runAsUser: 1000

  cluster:
    enabled: yes
    repo: sylva-core
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    labels:
      suspend-on-pivot: "yes"  # this unit must be suspended before pivot
    kustomization_spec:
      # see note below under .cluster
      # the choice made here, for now, requires setting other values consistently under .cluster.xxx
      path: ./kustomize-units/cluster-manifests/kubeadm-capd/base
      postBuild:
        substitute:
          CAPO_TAG: '{{ tuple .Values.cluster.capo.resources_tag (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          CONTROL_PLANE_AZ_LIST: '{{ tuple (.Values.cluster.capo.control_plane_az | toJson) (eq .Values.cluster.capi_providers.infra_provider "capo") | include "set-only-if" }}'
        substituteFrom:
        - kind: ConfigMap
          name: cluster-vars
        - kind: ConfigMap
          name: proxy-env-vars
      healthChecks:
        - apiVersion: cluster.x-k8s.io/v1beta1
          kind: Cluster
          name: '{{ .Values.cluster.name }}'
          namespace: default
      _components:
        - '{{ tuple "../../components/proxies/kubeadm"
                    (and .Values.proxies.http_proxy (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk")) | include "set-only-if" }}'
        - '{{ tuple "../../components/proxies/rke2"
                    (and .Values.proxies.http_proxy (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-root-volume"
                    (and .Values.cluster.capo.rootVolume (eq .Values.cluster.capi_providers.infra_provider "capo"))  | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-control-plane-az"
                    (and .Values.cluster.capo.control_plane_az (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
        - '{{ tuple "../../components/rke2-cis-profile"
                    (and .Values.cluster.cis_profile (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
      _patches:
        - '{{ tuple (.Files.Get "elements/controlplane-registry-mirrors.tpl" | fromYaml) (.Values.registry_mirrors | dig "hosts_config" "") | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/configtemplate-registry-mirrors.tpl" | fromYaml) (.Values.registry_mirrors | dig "hosts_config" "") | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/controlplane-ntp-servers.tpl" | fromYaml) (not (eq .Values.cluster.capi_providers.infra_provider "capd")) | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/configtemplate-ntp-servers.tpl" | fromYaml) (not (eq .Values.cluster.capi_providers.infra_provider "capd")) | include "set-only-if" }}'

  heat-operator:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/heat-operator
      wait: true

  capo-cluster-resources:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      healthChecks:
        # until the heat operator is improved to conform to kstatus specs
        # see https://gitlab.com/sylva-projects/sylva-core/-/issues/104
        - apiVersion: v1
          kind: ConfigMap
          namespace: default
          name: capo-cluster-resources
      postBuild:
        substitute:
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.cluster.capo.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.cluster.capo.worker_affinity_policy }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-cluster-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-cluster-security-group-worker-{{ .Values.cluster.capo.resources_tag }}'
          CAPO_EXTERNAL_NETWORK_ID: '{{ tuple .Values.cluster.capo.external_network_id
            (and .Values.cluster.capo.external_network_id (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
      _components:
        - '{{ tuple "../cluster-manifests/components/capo-vip-floating-ip"
            (and .Values.cluster.capo.external_network_id (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'


  calico:
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    helm_repo_url: https://projectcalico.docs.tigera.io/charts
    helmrelease_spec:
      targetNamespace: tigera-operator
      install:
        createNamespace: true
      chart:
        spec:
          chart: tigera-operator
          version: v3.24.5

  cinder-csi-psp:
    enabled: '{{ and (.Values.cluster.cis_profile | eq "cis-1.6") (tuple . (index .Values.units "cinder-csi" "enabled") | include "interpret-for-test") }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cinder-csi-psp
      wait: true

  cinder-csi:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    depends_on:
      cinder-csi-psp: '{{ .Values.cluster.cis_profile | eq "cis-1.6" }}'
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.27.1
      targetNamespace: cinder-csi
      install:
        createNamespace: false
      values:
        clusterID: '{{ .Values.cluster.capo.resources_tag }}'
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.cluster.capo.storageClass.name }}"
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.cluster.capo.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            [Global]
            auth-url = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url }}"
            tenant-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name }}"
            domain-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name }}"
            username = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}"
            password = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password }}"
            region = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.region_name }}"
            tls-insecure = "{{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}"
            [BlockStorage]
            ignore-volume-az = true

  imageswap-webhook:
    enabled: no
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/imageswap-webhook
      wait: true

  sylva-ca-certs:
    enabled: yes
    depends_on:
      namespace-defs: true
      cert-manager: true
      sylva-ca: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca-certs
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.cluster.rancher.external_hostname }}'
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'
          FLUX_DNS: '{{ .Values.cluster.flux.external_hostname }}'
      wait: true

  rancher:
    enabled: yes
    depends_on:
      cert-manager: true
      k8s-gateway: true
      sylva-ca-certs: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    annotations:
      sylvactl/readyMessage: "Rancher UI can be reached at https://{{ .Values.cluster.rancher.external_hostname }} ({{ .Values.cluster.rancher.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    helm_repo_url: https://releases.rancher.com/server-charts/latest
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: 2.7.3
      targetNamespace: cattle-system
      interval: 10m0s
      upgrade:
        remediation:
          retries: 3
      values:
        privateCA: true
        useBundledSystemChart: true
        hostname: '{{ .Values.cluster.rancher.external_hostname }}'
        ingress:
          enabled: true
          ingressClassName: nginx
          tls:
            source: secret
            secretName: rancher-tls
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        postDelete:
          namespaceList:
            - cattle-fleet-system
            - rancher-operator-system
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: rancher
                spec:
                  template:
                    spec:
                      volumes:
                        - name: tls-ca-volume
                          secret:
                            defaultMode: 256
                            secretName: rancher-tls
                            items:
                              - key: ca.crt
                                path: cacerts.pem
                      # this is to avoid that the too-short default liveness probe
                      # prevents the Rancher installation from finishing before the pod is killed:
                      containers:
                        - name: rancher
                          livenessProbe:
                            initialDelaySeconds: 120
                            periodSeconds: 30
                            failureThreshold: 20
    helm_secret_values:
      bootstrapPassword: '{{ .Values.cluster.admin_password }}'

    kustomization_spec:
      # these healthchecks are added so that this unit does not become ready before
      # a few things that Rancher sets up behind the scene is ready
      healthChecks:
        - apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          name: clusters.provisioning.cattle.io  # this is because capi-rancher-import needs this
        - apiVersion: apps/v1
          kind: Deployment
          name: rancher-webhook
          namespace: cattle-system
        - apiVersion: v1
          kind: Service
          name: rancher-webhook
          namespace: cattle-system

  rancher-webhook-service:
    enabled: '{{ .Values.units.rancher.enabled }}'
    repo: sylva-core
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/rancher-webhook
      wait: true

  rancher-add-keycloak-provider:
    enabled: '{{ .Values._internal.rancher_and_keycloak }}'
    repo: sylva-core
    depends_on:
      rancher: true
      keycloak: true
      keycloak-resources: true
    kustomization_substitute_secrets:
      RANCHER_CLIENT_SECRET: '{{ .Values.cluster.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak-auth-provider
      postBuild:
        substitute:
          KEYCLOAK_EXTERNAL_URL: '{{ .Values.cluster.keycloak.external_hostname }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.cluster.rancher.external_hostname }}'
      wait: true

  k8s-gateway:
    enabled: yes
    depends_on:
      metallb-config: '{{ .Values.units.metallb.enabled }}'
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: 2.0.3
      interval: 1m0s
      values:
        domain: '{{ .Values.cluster.cluster_external_domain }}'
        replicaCount: 3
        service:
          loadBalancerIP: '{{ .Values.cluster.cluster_external_ip }}'
          annotations:
            metallb.universe.tf/allow-shared-ip: '{{ tuple (.Values.cluster.cluster_external_ip) (or (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.capi_providers.infra_provider | eq "capd")) | include "set-only-if" }}'

  metallb:
    enabled: '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/metallb
      wait: true

  metallb-config:
    enabled: '{{ .Values.units.metallb.enabled }}'
    repo: sylva-core
    depends_on:
      metallb: true
    kustomization_spec:
      path: ./kustomize-units/metallb-config
      wait: true
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster.cluster_external_ip }}'

  capi-rancher-import:
    enabled: '{{ .Values.units.rancher.enabled }}'
    repo: capi-rancher-import
    depends_on:
      rancher: true
      k8s-gateway: true
    helmrelease_spec:
      chart:
        spec:
          chart: charts/capi-rancher-import
      interval: 1m0s
      values:
        securityContext:
          runAsUser: 1
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.cluster.rancher.external_hostname }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: unit-capi-rancher-import
            namespace: default
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    depends_on:
      metallb-config: '{{ .Values.units.metallb.enabled }}'
    helm_repo_url: https://kubernetes.github.io/ingress-nginx
    helmrelease_spec:
      chart:
        spec:
          chart: ingress-nginx
          version: 4.5.2
      interval: 1m0s
      values:
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            externalIPs:
            - '{{ .Values.cluster.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: '{{ tuple (.Values.cluster.cluster_external_ip) (eq .Values.cluster.capi_providers.infra_provider "capd") | include "set-only-if" }}'

  first-login-rancher:
    enabled: '{{ .Values.units.rancher.enabled }}'
    repo: sylva-core
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/first-login-rancher
      postBuild:
        substitute:
          RANCHER_EXTERNAL_URL: '{{ .Values.cluster.rancher.external_hostname }}'
          CURRENT_TIME: '{{ now | date "2006-01-02T15:04:05.999Z" }}'
      wait: true

  update-rancher-admin:
    enabled: '{{ .Values._internal.rancher_and_keycloak }}'
    repo: sylva-core
    depends_on:
      rancher: true
      rancher-add-keycloak-provider: true
      keycloak-resources: true
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: update-rancher-admin-job
          - op: replace
            path: /spec/backoffLimit
            value: 10
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/update-rancher-user.sh" | indent 4 }}

  flux-webui:
    enabled: yes
    depends_on:
      flux-system: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    annotations:
      sylvactl/readyMessage: "Fluw Webui can be reached at https://{{ .Values.cluster.flux.external_hostname }} ({{ .Values.cluster.flux.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        - name: SSL_CERT_FILE
          value: /etc/ssl/certs/ca.crt
        installCRDs: true
        rbac:
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "watch" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: '{{ .Values.cluster.flux.external_hostname }}'
              paths:
                - path: /   # setting this to another value like '/flux' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
          tls:
            - secretName: weave-tls
              hosts:
                - '{{ .Values.cluster.flux.external_hostname }}'
        extraVolumes:
          - name: custom-ca-cert
            secret:
              secretName: weave-tls
              items:
                - key: ca.crt
                  path: ca.crt
        extraVolumeMounts:
          - name: custom-ca-cert
            mountPath: /etc/ssl/certs
            readOnly: true
        oidcSecret:
          create: true
          clientID: flux
          clientSecret: '{{ .Values.cluster.admin_password }}'
          issuerURL: https://{{ .Values.cluster.keycloak.external_hostname }}/realms/sylva
          redirectURL: https://{{ .Values.cluster.flux.external_hostname }}/oauth2/callback
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: flux-webui-weave-gitops
                spec:
                  template:
                    spec:
                      containers:
                      - name: weave-gitops
                        securityContext:
                          seccompProfile:
                            $patch: delete
                            type: RuntimeDefault          
    helm_secret_values:
      adminUser:
        create: true
        username: admin
        passwordHash: '{{ htpasswd "" .Values.cluster.flux_webui.admin_password | trimPrefix ":" }}' # we don't want the "<user>:" part generated by htpasswd

  monitoring-crd:
    enabled: '{{ .Values.units.monitoring.enabled }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 101.0.0+up19.0.3

  monitoring:
    enabled: yes
    depends_on:
      monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      chart:
        spec:
          chart: rancher-monitoring
          version: 101.0.0+up19.0.3

  # this is a Job attempt to check successful import of a CAPI workload clusters in Rancher, still incomplete
  # it would be used along with a definition of a 'workload-cluster' unit
  # and is conditioned by a boolean flag .Values.env_type_ci
  check-rancher-clusters:
    enabled: '{{ index .Values.units "workload-cluster" "enabled" }}'
    repo: sylva-core
    depends_on:
      workload-cluster-import: true
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: check-rancher-clusters-job
          - op: replace
            path: /spec/backoffLimit
            value: 10
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/check-rancher-clusters.sh" | indent 4 }}
      - target:
          kind: Job
          name: kube-job
        patch: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ignored
          spec:
            template:
              spec:
                containers:
                - name: run-script
                  env:
                  - name: CLUSTER_NAME
                    value: '{{ .Values.cluster.test_workload_cluster_name }}'

  workload-capo-cluster-resources:
    enabled: '{{ and (.Values.cluster.capi_providers.infra_provider | eq "capo") (tuple . (index .Values.units "workload-cluster" "enabled") | include "interpret-for-test") }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      healthChecks:
        # until the heat operator is improved to conform to kstatus specs
        # see https://gitlab.com/sylva-projects/sylva-core/-/issues/104
        - apiVersion: v1
          kind: ConfigMap
          namespace: default
          name: workload-capo-cluster-resources
      _patches:
        - target:
            kind: HeatStack
          patch: |
            kind: HeatStack
            metadata:
              name: _unused_
            spec:
              namePrefix: workload-cluster
              configmapName: workload-capo-cluster-resources
        - target:
            kind: HeatStack
          patch: |
            - op: replace
              path: /metadata/name
              value: heatstack-workload-capo-cluster-resources
        - target:
            kind: HeatStack
          patch: |
            - op: add
              path: /spec/template/outputs
              value:
                CLUSTER_EXTERNAL_IP:
                  value: { get_attr: [port, fixed_ips, 0, ip_address] }
                CONTROL_PLANE_SERVERGROUP_ID:
                  value: { get_resource: srvgroup-ctrl-plane }
                WORKER_SERVERGROUP_ID:
                  value: { get_resource: srvgroup-worker }
            - op: replace
              path: /spec/template/parameters/control_plane_rules
              value:
                type: json
                description: "Security group rules associated with the control plane VMs"
                default:
                  direction: [ingress, ingress, ingress, egress]
                  port: ['80', '443', '22', '53']
                  protocol: [tcp, tcp, tcp, udp]
                  remote_ip_prefix: [0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0]
            - op: replace
              path: /spec/template/parameters/worker_rules
              value:
                type: json
                description: "Security group rules associated with the worker VMs"
                default:
                  direction: [ingress, ingress, ingress, egress]
                  port: ['80', '443', '22', '53']
                  protocol: [tcp, tcp, tcp, udp]
                  remote_ip_prefix: [0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0]

      postBuild:
        substitute:
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.cluster.capo.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.cluster.capo.worker_affinity_policy }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'workload-cluster-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'workload-cluster-security-group-worker-{{ .Values.cluster.capo.resources_tag }}'

  workload-cluster:
    enabled: '{{ .Values.env_type_ci }}'
    repo: sylva-core
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      workload-capo-cluster-resources: '{{ index .Values.units "workload-capo-cluster-resources" "enabled" }}'
    kustomization_spec:
      path: ./kustomize-units/cluster-manifests/kubeadm-capd/base
      postBuild:
        substitute:
          CLUSTER_NAME: '{{ .Values.cluster.test_workload_cluster_name }}'
          K8S_VERSION: '{{ .Values.cluster.k8s_version }}'
          CIS_PROFILE: '{{ tuple .Values.cluster.cis_profile
                    (and .Values.cluster.cis_profile (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr")) | include "set-only-if" }}'
          MACHINE_IMAGE: '{{ .Values.cluster.image }}'
          WORKER_MACHINE_IMAGE: '{{ .Values.cluster.worker_machine_image | default .Values.cluster.image }}'
          K8SGATEWAY_LB_IP: '{{ .Values.cluster.cluster_external_ip }}'   # NOTE: same is used for k8s-gateway DNS svc in management cluster
          CLUSTER_EXTERNAL_DOMAIN: '{{ .Values.cluster.cluster_external_domain }}'
          CAPO_NETWORK_ID: '{{ tuple .Values.cluster.capo.network_id (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          VIRTIO_VNIC_NETWORK_ID: '{{ tuple .Values.cluster.capo.additional_networks.virtio_network_id (and (.Values.cluster.capi_providers.infra_provider | eq "capo") (gt (len .Values.cluster.capo.additional_networks.virtio_network_id) 0 )) | include "set-only-if" }}'
          DIRECT_VNIC_NETWORK_ID: '{{ tuple .Values.cluster.capo.additional_networks.direct_network_id (and (.Values.cluster.capi_providers.infra_provider | eq "capo") (gt (len .Values.cluster.capo.additional_networks.direct_network_id) 0 )) | include "set-only-if" }}'
          SSH_KEY_NAME: '{{ tuple .Values.cluster.capo.ssh_key_name (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          CAPO_TAG: '{{ tuple .Values.cluster.capo.resources_tag (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          WORKER_FLAVOR_NAME: '{{ tuple "m1.large" (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          CONTROL_PLANE_REPLICAS: '{{ tuple "1" (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          WORKER_REPLICAS: '{{ tuple "0" (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          WORKER_AZ: '{{ tuple "nova" (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "set-only-if" }}'
          CONTROL_PLANE_AZ_LIST: '{{ tuple (.Values.cluster.capo.control_plane_az | toJson)
                    (and .Values.cluster.capo.control_plane_az (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
          DISK_SIZE: '{{ tuple (.Values.cluster.capo.rootVolume.diskSize | toString)
                    (and .Values.cluster.capo.rootVolume (eq .Values.cluster.capi_providers.infra_provider "capo"))  | include "set-only-if" }}'
          VOLUME_TYPE: '{{ tuple .Values.cluster.capo.rootVolume.volumeType
                    (and .Values.cluster.capo.rootVolume (eq .Values.cluster.capi_providers.infra_provider "capo"))  | include "set-only-if" }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'workload-cluster-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'workload-cluster-security-group-worker-{{ .Values.cluster.capo.resources_tag }}'
        substituteFrom:
          - kind: ConfigMap
            name: proxy-env-vars
            optional: true
          - kind: ConfigMap
            name: workload-capo-cluster-resources
            optional: true
      wait: true
      _components:
        - '{{ tuple "../../components/dns-client/kind/KubeadmControlPlane"
                    (and (eq .Values.cluster.capi_providers.infra_provider "capd") (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk")) | include "set-only-if" }}'
        - '{{ tuple "../../components/dns-client/kind/RKE2ControlPlane"
                    (and (eq .Values.cluster.capi_providers.infra_provider "capd") (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
        - '{{ tuple "../../components/proxies/kubeadm"
                    (and .Values.proxies.http_proxy (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk")) | include "set-only-if" }}'
        - '{{ tuple "../../components/proxies/rke2"
                    (and .Values.proxies.http_proxy (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
        - '{{ tuple "../../components/dns-client/Ubuntu/KubeadmControlPlane"
                    (and (ne .Values.cluster.capi_providers.infra_provider "capd") (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk")) | include "set-only-if" }}'
        - '{{ tuple "../../components/dns-client/Ubuntu/RKE2ControlPlane"
                    (and (ne .Values.cluster.capi_providers.infra_provider "capd") (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-root-volume"
                    (and .Values.cluster.capo.rootVolume (eq .Values.cluster.capi_providers.infra_provider "capo"))  | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-additional-networks/virtio"
                    (get .Values.cluster.capo.additional_networks "virtio_network_id") | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-additional-networks/direct"
                    (get .Values.cluster.capo.additional_networks "direct_network_id") | include "set-only-if" }}'
        - '{{ tuple "../../components/capo-control-plane-az"
                    (and .Values.cluster.capo.control_plane_az (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
        - '{{ tuple "../../components/rke2-cis-profile"
                    (and .Values.cluster.cis_profile (eq .Values.cluster.capi_providers.bootstrap_provider "cabpr")) | include "set-only-if" }}'
      _patches:
        - '{{ tuple (.Files.Get "elements/controlplane-registry-mirrors.tpl" | fromYaml) (.Values.registry_mirrors | dig "hosts_config" "") | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/configtemplate-registry-mirrors.tpl" | fromYaml) (.Values.registry_mirrors | dig "hosts_config" "") | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/controlplane-ntp-servers.tpl" | fromYaml) (not (eq .Values.cluster.capi_providers.infra_provider "capd")) | include "set-only-if" }}'
        - '{{ tuple (.Files.Get "elements/configtemplate-ntp-servers.tpl" | fromYaml) (not (eq .Values.cluster.capi_providers.infra_provider "capd")) | include "set-only-if" }}'

  workload-cluster-calico:
    enabled: '{{ and (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") (tuple . (index .Values.units "workload-cluster" "enabled") | include "interpret-for-test") }}'
    depends_on:
      workload-cluster: true
    helm_repo_url: https://projectcalico.docs.tigera.io/charts
    helmrelease_spec:
      targetNamespace: tigera-operator
      install:
        createNamespace: true
      chart:
        spec:
          chart: tigera-operator
          version: v3.24.5
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.test_workload_cluster_name }}-kubeconfig'

  workload-cluster-multus:
    enabled: false
    depends_on:
      workload-cluster: true
      workload-cluster-calico: '{{ index .Values.units "workload-cluster-calico" "enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io/
    helmrelease_spec:
      chart:
        spec:
          chart: "rke2-multus"
          version: "v3.9.3-build2023010901"
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.test_workload_cluster_name }}-kubeconfig'
      targetNamespace: kube-system
      install:
        createNamespace: false
      values:
        rke2-whereabouts:
          enabled: true
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: DaemonSet
                apiVersion: apps/v1
                metadata:
                  name: workload-cluster-multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts
              - kind: ServiceAccount
                apiVersion: v1
                metadata:
                  name: workload-cluster-multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts

  workload-cluster-import:
    enabled: '{{ index .Values.units "workload-cluster" "enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
      workload-cluster: true
      workload-cluster-calico: '{{ index .Values.units "workload-cluster-calico" "enabled" }}'
    kustomization_spec:
      path: ./kustomize-units/cluster-import
      postBuild:
        substitute:
          CLUSTER_FLAVOR: '{{ upper .Values.cluster.capi_providers.bootstrap_provider }} {{ upper .Values.cluster.capi_providers.infra_provider }}'
          CLUSTER_MONITORING: "enabled"
          CLUSTER_NAME: '{{ .Values.cluster.test_workload_cluster_name }}'
      wait: true

  workload-cluster-monitoring-crd:
    enabled: '{{ index .Values.units "workload-cluster-monitoring" "enabled" }}'
    depends_on:
      workload-cluster: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 101.0.0+up19.0.3
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.test_workload_cluster_name }}-kubeconfig'

  workload-cluster-monitoring:
    enabled: '{{ index .Values.units "workload-cluster" "enabled" }}'
    depends_on:
      workload-cluster-monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      chart:
        spec:
          chart: rancher-monitoring
          version: 101.0.0+up19.0.3
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.test_workload_cluster_name }}-kubeconfig'

  coredns:
    enabled: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/coredns
      wait: true
      _patches:
        - target:
            kind: ConfigMap
            name: coredns
          patch: |
            - op: replace
              path: /metadata/name
              value: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpk") | ternary "coredns" "rke2-coredns-rke2-coredns" }}'
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster.cluster_external_ip }}'
          CLUSTER_EXTERNAL_DOMAIN: '{{ .Values.cluster.cluster_external_domain }}'


## stuff related to the 'cluster' unit

cluster:
  name: management-cluster

  # cis profile to be used. Curently supported only for rke2 clusters. "cis-1.6" for k8s prior to 1.25, "cis-1.23" for 1.25+
  cis_profile: cis-1.6

  # Admin password that will be configured by default on various units
  admin_password: '{{ .Values._internal.default_password }}'

  # image reference depends provider, used for control plane nodes
  image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/rke2-in-docker:v1-24-12-rke2r1

  # worker_machine_image: "Ubuntu 20.04 Pack"         # Openstack image for worker nodes

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capd      # capd, capo, capm3 or capv
    bootstrap_provider: cabpk # cabpr (RKE2) or cabpk (kubeadm)

  # cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened
  cis_benchmark_scan_profile: '{{ eq .Values.cluster.capi_providers.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

  # kubernetes version to be used
  k8s_version: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.24.12+rke2r1" "v1.24.12" }}'

  # Nodes number for controlplane and workers
  control_plane_replicas: 3
  worker_replicas: 0

  capo:
    flavor_name: m1.large # Openstack flavor name
    worker_flavor_name: m1.large
    # worker_az only required to be provided when .Values.cluster.worker_replicas > 0
    ssh_key_name: # OpenStack VM SSH key
    network_id:  # OpenStack network used for nodes and VIP port
    #external_network_id # Can be provided if a FIP is needed in order to reach the management cluster VIP
    floating_ip: ""
    additional_networks:
    # Leave these parameters empty or specify additional networks to be used. Only one network of each type can be added and supported types are virtio or direct
      virtio_network_id: ""
      direct_network_id: ""
    rootVolume: {} # Let this parameter empty if you don't intent to use root volume
    # otherwise, provide following values
    # diskSize: 20 # Size of the VMs root disk
    # volumeType: '__DEFAULT__' # Type of volume to be created
    control_plane_affinity_policy: soft-anti-affinity
    worker_affinity_policy: soft-anti-affinity
    #control_plane_az: # list of OpenStack availability zones to deploy control planes nodes to, otherwise all would be candidates
    clouds_yaml: # (this is a dict, not a YAML string)
      clouds:
        capo_cloud:
          auth:
            auth_url: # replace me
            user_domain_name: # replace me
            project_domain_name: # replace me
            project_name: # replace me
            username: # replace me
            password: # replace me
          region_name: # optional
          verify: # e.g. false
    #cacert: # cert used to validate CA of OpenStack APIs
    storageClass:
      name: cinder-csi  # name of the storageClass to be created
      #type: xxx  # please provide the cinder volume type, e.g. 'ceph_sas' (must exist in OpenStack)
    resources_tag: '{{ eq .Values.cluster.capi_providers.infra_provider "capo" | ternary (printf "sylva-%s" .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username) "" }}' # tag set for OpenStack resources in management and test workload cluster


  cluster_external_ip: 55.55.55.55
  cluster_external_domain: sylva
  display_external_ip: '{{ .Values.cluster.capo.floating_ip | eq "" | ternary .Values.cluster.cluster_external_ip .Values.cluster.capo.floating_ip }}'

  capd:
    docker_host: unix:///var/run/docker.sock

  flux_webui:
    admin_user: admin
    # admin password is to a random value by default, just to avoid setting a default that everyone would know
    # /!\ but the password set here will be generated by Helm and then hashed (see passwordHash in flux-webui
    # unit definition) so this random password will be unknown & unusable
    admin_password: '{{ .Values.cluster.admin_password }}'    # Note: to use OIDC login via Keycloak for Flux webUI, this variable needs to be provided by the stack user

  rancher:
    external_hostname: 'rancher.{{ .Values.cluster.cluster_external_domain }}'

  vault:
    external_hostname: 'vault.{{ .Values.cluster.cluster_external_domain }}'

  keycloak:
    external_hostname: 'keycloak.{{ .Values.cluster.cluster_external_domain }}'

  flux:
    external_hostname: 'flux.{{ .Values.cluster.cluster_external_domain }}'

  test_workload_cluster_name: first-workload-cluster

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""

# disable default values for no_proxy (localhost,.svc,.cluster.local.,.cluster.local,.sylva)
# Ex: localhost: false
no_proxy_additional: {}

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
registry_mirrors:
  default_settings:                          # <<< These settings will apply to all configured mirrors
    capabilities: ["pull", "resolve"]
#   skip_verify: true
#   override_path: true
# hosts_config:
#   docker.io:
#   - mirror_url: http://your.mirror/docker
#     registry_settings:                     # <<< Host settings can be used to override default_settings
#       skip_verify: false
#   registry.k8s.io:
#   - mirror_url: ...
#   _default:
#   - mirror_url: ...

# boolean conditioning which unit we automatically enable in CI tests
env_type_ci: false

# set NTP servers by IP or FQDN and enable their usage for control plane nodes
ntp:
  enabled: false
  servers:
    - 1.2.3.4
    - europe.pool.ntp.org

_internal:
  default_password: '{{ randAlphaNum 64 }}'
  rancher_and_keycloak: '{{ and (tuple . .Values.units.rancher.enabled | include "interpret-for-test") (tuple . .Values.units.keycloak.enabled | include "interpret-for-test") }}'

  default_storage_class_unit: >-
    {{- if tuple . (index .Values.units "cinder-csi" "enabled") | include "interpret-for-test" -}}
      cinder-csi
    {{- else -}}
      local-path-provisioner
    {{- end -}}
