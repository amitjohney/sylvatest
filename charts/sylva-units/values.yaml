# Default values for sylva-units.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret: {}
  # when using gitlab, git creds can be used as registry creds
  #registry.gitlab.com:
  #  username: your_user_name
  #  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s

oci_repo_spec_default:
  interval: 60m0s

source_templates: # template to generate Flux GitRepository/OCIRepository resources
  # <repo-name>:
  #   kind:  GitRepository/OCIRepository
  #   spec:  # partial spec for a Flux resource
  #     url: https://gitlab.com/sylva-projects/sylva-core.git
  #     #secretRef: # is autogenerated based on 'auth'
  #     ref: # can be overridden per-unit, with 'ref_override'
  #       branch: main
  #   auth: # optional 'username'/'password' dict containing git authentication information
  #   existing_source: # optional, when this value is set the specified GitRepository or OCIRepository will be used instead of creating one based on 'spec'
  #     name: sylva-core
  #     kind: GitRepository or OCIRepository

  sylva-core:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-core.git
      ref:
        # this is provided only for reference
        # in practice the sylva-core framework will always override this ref so that the
        # currently checked out commit of sylva-core is used by sylva-units
        # (you can grep the code for "CURRENT_COMMIT" to find out why)
        branch: main

  capi-rancher-import:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capi-rancher-import.git
      ref:
        tag: 0.1.4

  weave-gitops:
    kind: GitRepository
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.36.0

  metal3:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3.git
      ref:
        tag: 0.7.0

  metal3-suse:
    kind: GitRepository
    spec:
      url: https://github.com/suse-edge/charts.git
      ref:
        tag: 0.2.0-metal3

  sylva-capi-cluster:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-capi-cluster.git
      ref:
        tag: 0.1.18

  local-path-provisioner:
    kind: GitRepository
    spec:
      url: https://github.com/rancher/local-path-provisioner.git
      ref:
        tag: v0.0.24

  sriov-resources:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sriov-resources.git
      ref:
        tag: 0.0.3

  os-image-server:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/os-image-server.git
      ref:
        tag: 1.7.0

  capo-contrail-bgpaas:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capo-contrail-bgpaas.git
      ref:
        tag: 1.0.2

  libvirt-metal:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal.git
      ref:
        tag: 0.1.6

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'units'
unit_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: 2
      remediateLastFailure: false

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated by a unit with a "helmrelease_spec" field
unit_helmrelease_kustomization_spec_default:
  path: ./kustomize-units/helmrelease-generic
  sourceRef: >-
    {{- $existing_source := get (index .Values.source_templates "sylva-core") "existing_source" -}}
    {{- if $existing_source -}}
      {{- $existing_source | include "preserve-type" -}}
    {{- else -}}
      {{- dict "kind" (index .Values.source_templates "sylva-core" | dig "kind" "GitRepository") "name" "sylva-core" | include "preserve-type" -}}
    {{- end -}}
  wait: true

# default value used if units.xxx.enabled is not specified
units_enabled_default: false

# unit_template define unit settings
#   a unit can inherit from one of those
unit_templates:
  sylva-units:
    repo: sylva-core

# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository or OCIRepository (and Secret, TODO: the Secret don't need to be generated for each unit)
units:
  # <unit-name>:
  #   enabled: boolean or GoTPL
  #   repo: <name of the repo under 'source_templates'> (for use with kustomization_spec)
  #   helm_repo_url: URL of the Helm repo to use (for use with helmrelease_spec, but not mandatory, 'repo' can be used as well to use a git repo)
  #   labels: (optional) dict holding labels to add to the resources for this unit
  #   ref_override: optional, if defined, this dict will be used for the GitRepository or OCIRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: dict defining the dependencies of this unit, keys are unit names, values are booleans
  #               (these dependencies are injected in the unit Kustomization via 'spec.dependsOn')
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from unit_helmrelease_spec_default
  #                      # and from other fields in the unit definition
  #     _postRenderers: # this field can be used in this file, it will be merged into user-provided 'postRenderers'
  #   helm_chart_artifact_name: optional, if specified, when deploying the Helm chart from an OCI artifact,
  #                             helm_chart_artifact_name will be used as chart name instead of helmrelease_spec.chart.spec.chart last path item
  #                             this is required if helmrelease_spec.chart.spec.chart is empty, '.' or '/'
  #                             (also used by tools/oci/push-helm-chart to generate the artifact)
  #   helm_chart_versions: optional, if specified, when deploying the Helm chart from an OCI artifact or Helm registry,
  #                             it will drive the version to be used from a dict of <version>:<boolean>
  #                             in case if helmrelease_spec.chart.spec.chart.version is not set
  #                             (also used by tools/oci/push-helm-charts-artifacts.sh to generate the artifact)
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from unit_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-unit-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <unit>.spec.path  (if <git repo template> has spec.url defined)
  #     _patches: # this field can be used in this file, it will be merged into user-provided 'patches'
  #     _components: # this field can be used in this file, it will be merged into user-provided 'components'
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret
  #   unit_template: optional, today used only for units that use sylva-units itself
  #                  the settings for the unit are inherited from the corresponding entry under unit_templates

  flux-system:
    # note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
    # this units contains Flux definitions *to manage the Flux system itself via gitops*
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/flux-system/base
      targetNamespace: flux-system
      wait: true
      # prevent Flux from uninstalling itself
      prune: false
      _components:
      - '{{ tuple "../components/extra-ca" .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
          EXTRA_CA_CERTS: '{{ tuple (.Values.oci_registry_extra_ca_certs | default "" | b64enc) .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  cert-manager:
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.13.2
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true

  trivy-operator:
    enabled: no
    helm_repo_url: https://aquasecurity.github.io/helm-charts/
    helmrelease_spec:
      chart:
        spec:
          chart: trivy-operator
          version: 0.18.4
      targetNamespace: trivy-system
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 10000
          runAsUser: 10000
        serviceAccount:
          annotations: {}
          create: true
          name: trivy-operator
        trivy:
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" . }}'
          severity: UNKNOWN,HIGH,CRITICAL
        trivyOperator:
          scanJobPodTemplatePodSecurityContext:
            runAsGroup: 10000
            runAsUser: 10000


# sylva-ca provides a Certificate Authority for Components of the management cluster
  sylva-ca:
    depends_on:
      cert-manager: true
      external-secrets-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca
      targetNamespace: cert-manager
      wait: true

# namespace-defs creates the namespaces to be used by various units
  namespace-defs:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/namespace-defs
      wait: true
      prune: false

# Postgresql deployment for keycloak
  postgres:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql
          version: 12.12.10
      targetNamespace: keycloak
      values:
        global:
          postgresql:
            auth:
              database: keycloak
          auth:
            username: postgres
        nameOverride: postgres

  kubevirt:
    helm_repo_url: https://suse-edge.github.io/charts
    helmrelease_spec:
      chart:
        spec:
          chart: kubevirt
          version: 0.1.0

  kubevirt-test-vms:
    depends_on:
      kubevirt: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kubevirt-test-vms
      wait: true
      targetNamespace: kubevirt-tests

  vault-operator:
    helm_repo_url: https://kubernetes-charts.banzaicloud.com
    helmrelease_spec:
      chart:
        spec:
          chart: vault-operator
          version: 1.19.0
      targetNamespace: vault
      install:
        createNamespace: true

# Vault assumes that the certificate vault-tls has been issued
  vault:
    depends_on:
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      vault-operator: true
    annotations:
      sylvactl/readyMessage: "Vault UI can be reached at https://{{ .Values.vault.external_hostname }} ({{ .Values.vault.external_hostname }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_substitute_secrets:
      ADMIN_PASSWORD: '{{ .Values.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/vault
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.vault.external_hostname }}'
          VAULT_REPLICAS: '{{ .Values._internal.vault_replicas }}'
          MAX_POD_UNAVAILABLE: '{{ int .Values._internal.vault_replicas | eq 1 | ternary 0 1 }}'
      healthChecks:  # sometimes this kustomization seems correctly applied while vault pod is not running, see https://gitlab.com/sylva-projects/sylva-core/-/issues/250
      # so we replace wait:true by checking for the Vault components health
        - apiVersion: apps/v1
          kind: StatefulSet
          name: vault
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-configurer
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-configurer
          namespace: vault

  vault-config-operator:
    depends_on:
      namespace-defs: true
      cert-manager: true
      monitoring: '{{ .Values.units | dig "vault-config-operator" "helmrelease_spec" "values" "enableMonitoring" true }}'
    helm_repo_url: https://redhat-cop.github.io/vault-config-operator
    helmrelease_spec:
      chart:
        spec:
          chart: vault-config-operator
          version: v0.8.24
      targetNamespace: vault
      values:
        enableCertManager: true
        enableMonitoring: false

  vault-secrets: # generate random secrets in vault, configure password policy, authentication backends, etc...
    repo: sylva-core
    depends_on:
      vault: true
      vault-config-operator: true
    kustomization_spec:
      path: ./kustomize-units/vault-secrets
      wait: true

  vault-oidc:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak-resources: true
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/vault-oidc
      wait: true
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.vault.external_hostname }}'
          KEYCLOAK_DNS: '{{ .Values.keycloak.external_hostname }}'

  external-secrets-operator:
    helm_repo_url: https://charts.external-secrets.io
    helmrelease_spec:
      chart:
        spec:
          chart: external-secrets
          version: 0.9.9
      targetNamespace: external-secrets
      install:
        createNamespace: true
      values:
        installCRDs: true

  eso-secret-stores:
    depends_on:
      external-secrets-operator: true
      vault: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/eso-secret-stores
      wait: true

  cis-operator-crd:
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: 4.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator-crd: true
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: 4.0.0
      targetNamespace: cis-operator-system
      values:
        global:
          cattle:
            psp:
              enabled: '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") | include "preserve-type" }}'

  # this allows for running a CIS scan for management cluster
  # generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
  cis-operator-scan:
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    depends_on:
      cis-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cis-operator-scan
      wait: false
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cis_benchmark_scan_profile }}'

  neuvector-init:
    enabled_conditions:
    - '{{ tuple . "neuvector" | include "unit-enabled" }}'
    depends_on:
      sylva-ca: true
      vault: true
      vault-config-operator: true
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/neuvector-init
      wait: true
      postBuild:
        substitute:
          NEUVECTOR_DNS: '{{ .Values.neuvector.external_hostname }}'

  neuvector:
    enabled: no
    depends_on:
      neuvector-init: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    annotations:
      sylvactl/readyMessage: "Neuvector UI can be reached at https://{{ .Values.neuvector.external_hostname }} ({{ .Values.neuvector.external_hostname }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://neuvector.github.io/neuvector-helm
    helm_chart_artifact_name: neuvector-core
    helmrelease_spec:
      chart:
        spec:
          chart: core
          version: 2.6.6
      targetNamespace: neuvector
      values:
        psp: '{{ .Values.cluster.cis_profile | eq "cis-1.6" | include "as-bool" }}'
        internal:
          certmanager:
            enabled: true
            secretname: neuvector-internal
        controller:
          replicas: 1  # PVC only works for 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1
          image:
            repository: neuvector/controller
          pvc:
            enabled: true # setting PVC to true imposes 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
            accessModes:
              - ReadWriteOnce
        enforcer:
          image:
            repository: neuvector/enforcer
        manager:
          image:
            repository: neuvector/manager
          runAsUser: 10000
          ingress:
            enabled: true
            host: '{{ .Values.neuvector.external_hostname }}'
            ingressClassName: nginx
            path: /
            annotations:
              nginx.ingress.kubernetes.io/backend-protocol: https
            tls: true
            secretName: neuvector-tls
        cve:
          updater:
            image:
              repository: neuvector/updater
          scanner:
            image:
              repository: neuvector/scanner
              env:
                - name: https_proxy
                  value: '{{ .Values.proxies.https_proxy }}'
                - name: no_proxy
                  value: '{{ include "sylva-units.no_proxy" . }}'
        containerd:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" | include "as-bool" }}'
          path: /var/run/containerd/containerd.sock
        k3s:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | include "as-bool" }}'
          runtimePath: /run/k3s/containerd/containerd.sock
        resources:
          limits:
            cpu: 400m
            memory: 2792Mi
          requests:
            cpu: 100m
            memory: 2280Mi

  keycloak:
    depends_on:
      sylva-ca: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      postgres: true
      synchronize-secrets: true # make sure that the secret keycloak-initial-admin is ready to be consummed
    annotations:
      sylvactl/readyMessage: "Keycloak admin console can be reached at https://{{ .Values.keycloak.external_hostname }}/admin/master/console, user 'admin', password in Vault at secret/keycloak ({{ .Values.keycloak.external_hostname }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.keycloak.external_hostname }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
      healthChecks:  # cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        # the Keycloak StatefulSet set is produced, by the combination of Keycloak operator
        # and a Keycloak custom resource, it relies on the postgres DB also deployed by this unit
        # hence, checking for the health of this component can be done by checking this StatefulSet
        - apiVersion: apps/v1
          kind: StatefulSet
          name: keycloak
          namespace: keycloak
      _components:
        - '{{ tuple "components/keycloak-operator-proxies" (.Values.proxies.https_proxy) | include "set-only-if" }}'
      _patches:
        - patch: |
            - op: replace
              path: /spec/template/spec/containers/0/securityContext
              value:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                privileged: false
                runAsNonRoot: true
                runAsGroup: 1000
                runAsUser: 1000
                seccompProfile:
                  type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-operator

  keycloak-legacy-operator:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      vault-secrets: true  # the credential-external-keycloak Secret use by the legacy operator is generated from ES/Vault secret/data/keycloak
      keycloak: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-legacy-operator
      targetNamespace: keycloak
      wait: true
      _patches:
        - patch: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: keycloak-realm-operator
            spec:
              template:
                spec:
                  containers:
                    - name: keycloak-realm-operator
                      securityContext:
                        runAsUser: 10000
                        allowPrivilegeEscalation: false
                        capabilities:
                          drop:
                            - ALL
                        runAsNonRoot: true
                        seccompProfile:
                          type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-realm-operator

  keycloak-resources:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
    repo: sylva-core
    kustomization_substitute_secrets:
      SSO_PASSWORD: '{{ .Values.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak-resources
      targetNamespace: keycloak
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.rancher.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.flux.external_hostname }}'
          VAULT_DNS: '{{ .Values.vault.external_hostname }}'
          EXPIRE_PASSWORD_DAYS: '{{ int .Values.keycloak.keycloak_expire_password_days }}'
      healthChecks:  #  cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakRealm
          name: sylva
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-rancher-client  # this secret is a byproduct of the rancher-client KeycloakClient resource
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-flux-webui-client  # this secret is a byproduct of the flux-webui-client KeycloakClient resource
          namespace: keycloak

  # this is a Job to manually add a custom client-scope to sylva realm (on top of default ones)
  # while CRD option does not yet provide good results (overrides defaults)
  keycloak-add-client-scope:
    enabled_conditions:
    - '{{ tuple . "flux-webui" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      keycloak-resources: true
      keycloak: true  # this unit defines the keycloak-initial-admin Secret used by the script
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: keycloak-add-client-scope-job
          JOB_TARGET_NAMESPACE: keycloak
      _patches:
      - target:
          kind: Job
        patch: |
          - op: replace
            path: /spec/backoffLimit
            value: 15
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: keycloak-add-client-scope-job-keycloak-cm
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/keycloak-add-client-scope.sh" | indent 4 }}

  keycloak-oidc-external-secrets:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-oidc-external-secrets
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.keycloak.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.flux.external_hostname }}'
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: oidc-auth
          namespace: flux-system

  kyverno:
    helm_repo_url: https://kyverno.github.io/kyverno
    helmrelease_spec:
      chart:
        spec:
          chart: kyverno
          version: 3.1.0
      targetNamespace: kyverno
      install:
        createNamespace: true
      values:
        webhooksCleanup:
          enabled: false
        features:
          policyExceptions:
            enabled: true
        backgroundController:
          rbac:
            clusterRole:
              extraResources:
              - apiGroups: ["helm.toolkit.fluxcd.io"]
                resources:
                  - "helmreleases"
                verbs: ["get", "list", "watch", "patch", "update"]
              - apiGroups: [""]
                resources:
                  - "nodes"
                verbs: ["get", "list", "watch", "patch", "update"]
        config:
          # -- Resource types to be skipped by the Kyverno policy engine, removed [Node,*,*] [Node/*,*,*] entries
          resourceFilters:
            - '[Event,*,*]'
            - '[*/*,kube-system,*]'
            - '[*/*,kube-public,*]'
            - '[*/*,kube-node-lease,*]'
            - '[APIService,*,*]'
            - '[APIService/*,*,*]'
            - '[TokenReview,*,*]'
            - '[SubjectAccessReview,*,*]'
            - '[SelfSubjectAccessReview,*,*]'
            - '[Binding,*,*]'
            - '[Pod/binding,*,*]'
            - '[ReplicaSet,*,*]'
            - '[ReplicaSet/*,*,*]'
            - '[AdmissionReport,*,*]'
            - '[AdmissionReport/*,*,*]'
            - '[ClusterAdmissionReport,*,*]'
            - '[ClusterAdmissionReport/*,*,*]'
            - '[BackgroundScanReport,*,*]'
            - '[BackgroundScanReport/*,*,*]'
            - '[ClusterBackgroundScanReport,*,*]'
            - '[ClusterBackgroundScanReport/*,*,*]'

  kyverno-policies:
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kyverno-policies
      wait: true

  rancher-monitoring-clusterid-inject:
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      kyverno: '{{ tuple . "kyverno" | include "unit-enabled" }}'
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/rancher-monitoring-clusterid-inject
      wait: true

  shared-workload-clusters-settings:
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/shared-workload-clusters-settings
      wait: true
      targetNamespace: default
      _patches:
        - target:
            kind: ConfigMap
          patch: |
            - op: replace
              path: /data/values
              value: |
            {{ index (tuple . .Values.shared_workload_clusters_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | indent 4 }}
    kustomization_substitute_secrets:
      SECRET_VALUES: '{{ index (tuple . .Values.shared_workload_clusters_secret_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | b64enc }}'

  node-annotation-from-label:
    enabled: false # it blocks RKE2 node registrations when bootstrap is slower (for CAPM3 deployments) than Kyverno ClusterPolicy webhook install, https://gitlab.com/sylva-projects/sylva-core/-/issues/660
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      kyverno: '{{ tuple . "kyverno" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/node-annotation-from-label
      wait: true
      postBuild:
        substitute:
          NODE_LABEL_KEY_LIST: '{{ mergeOverwrite (.Values.cluster | dig "rke2" "node_annotations" dict) (.Values.cluster | dig "control_plane" "rke2" "node_annotations" dict) (.Values.cluster | dig "machine_deployment_default" "rke2" "node_annotations" dict) | keys | toJson }}' # need to address .cluster.machine_deployments.X.rke2.node_annotations

  capi:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capi
      wait: true

  capd:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: '{{ ternary "./kustomize-units/capd/base" "./kustomize-units/capd/rke2" (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") }}'
      postBuild:
        substitute:
          CAPD_DOCKER_HOST: '{{ .Values.capd_docker_host }}'
      wait: true

  capo:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capo
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: capo-controller-manager
            namespace: capo-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL

  capm3:
    repo: sylva-core
    depends_on:
      cert-manager: true
      '{{ .Values._internal.metal3_unit }}': true
    kustomization_spec:
      path: ./kustomize-units/capm3
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  capv:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capv
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  cabpk:  # kubeadm
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpk
      wait: true

  cabpr:  # RKE2
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpr
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-bootstrap-controller-manager
            namespace: rke2-bootstrap-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-control-plane-controller-manager
            namespace: rke2-control-plane-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
  metal3-suse:
    enabled: false
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: metal3-suse
    helm_chart_artifact_name: metal3
    helmrelease_spec:
      chart:
        spec:
          chart: charts/metal3/0.2.0
      install:
        createNamespace: true
      targetNamespace: metal3-system
      values:
        global:
          ironicIP: '{{ .Values.cluster_external_ip }}'
          provisioningInterface: eth0
        metal3-baremetal-operator:
          securityContext:
            allowPrivilegeEscalation: false
            runAsUser: 11000
        metal3-ironic:
          ingress:
            enabled: false
          service:
            type: LoadBalancer
            externalIPs:
            - '{{ .Values.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          podSecurityContext:
            runAsUser: 10475
            fsGroup: 10475
          baremetaloperator:
            ironichostNetwork: false
          persistence:
            ironic:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              accessMode: ReadWriteOnce
        metal3-mariadb:
          podSecurityContext:
            runAsUser: 10060
            fsGroup: 10060
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'

  metal3-init:
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-init
      wait: true

  metal3-sync-secrets:
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      metal3-init: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-sync-secrets
      wait: true

  metal3:
    enabled: false
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
      metal3-sync-secrets: true # make sure that the password secrets are ready to be consumed
    kustomization_spec:
      postBuild:
        substitute:
          # Force substitution in order to be consistent with what is done in bootstrap,
          # See https://gitlab.com/sylva-projects/sylva-core/-/issues/659
          var_substitution_enabled: "true"
    repo: metal3
    helm_chart_artifact_name: metal3
    helmrelease_spec:
      timeout: 10m
      chart:
        spec:
          chart: .
      install:
        createNamespace: false
      targetNamespace: metal3-system
      values:  # see https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3/-/blob/main/values.yaml
        global:
          storageClass: '{{ .Values._internal.default_storage_class }}'
        # ironicIPADownloaderBaseURI:
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        images:
          baremetalOperator:
            repository: quay.io/metal3-io/baremetal-operator
            tag: "v0.4.0"
          ironic:
            repository: quay.io/metal3-io/ironic
            tag: "capm3-v1.5.1"
          ironicIPADownloader:
            repository: registry.gitlab.com/sylva-projects/sylva-elements/container-images/ironic-ipa-downloader/ipa
            tag: "0.2.0"
        persistence:
          ironic:
            size: "10Gi"
            accessMode: "ReadWriteOnce"
        services:
          ironic:
            # Specify the IP address used by Ironic service
            ironicIP: '{{ .Values.cluster_external_ip }}'
            # We allow shared IPs because we are exposing (by design) a single cluster external IP
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
        mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'
        podSecurityContext:
          runAsNonRoot: true
          runAsUser: 1000
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
      valuesFrom:
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-root-secret
          targetPath: mariadb.auth.rootPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-replication-secret
          targetPath: mariadb.auth.replicationPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: mariadb.auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-inspector-secret
          targetPath: auth.ironicInspectorPassword
          optional: false

  # this unit is here only to have a single thing to look at to determine
  # if everything is ready for pivot (see bootstrap.values.yaml pivot unit)
  capi-providers-pivot-ready:
    repo: sylva-core
    # we copy the dependencies of the 'cluster' unit
    # (with the exception of "capo-cluster-resources" which we don't need, given how what it produces is consumed)
    depends_on: '{{ omit .Values.units.cluster.depends_on "capo-cluster-resources" | include "preserve-type" }}'
    kustomization_spec:
      path: ./kustomize-units/dummy  # does not create any relevant resource
      wait: false

  local-path-provisioner:
    repo: local-path-provisioner
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/chart/local-path-provisioner
      targetNamespace: kube-system
      install:
        createNamespace: true
      values:
        storageClass:
          defaultClass: '{{ .Values._internal.default_storage_class | eq "local-path" | include "as-bool" }}'
        helperImage:
          repository: docker.io/library/busybox
          tag: 1.36.1
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: local-path-provisioner
                spec:
                  template:
                    spec:
                      containers:
                      - name: local-path-provisioner
                        securityContext:
                          runAsUser: 1000

  cluster:
    repo: sylva-capi-cluster
    helm_chart_artifact_name: sylva-capi-cluster
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      '{{ .Values._internal.metal3_unit }}': '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
      default/os-image-server: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
    labels:
      suspend-on-pivot: "yes"  # this unit must be suspended before pivot
    kustomization_spec:
      healthChecks:
        # wait for OpenStackCluster when this provider is used, as Cluster will be seen as non-ready when it will be paused (as observedGeneration will differ from generation)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "infrastructure.cluster.x-k8s.io/v1alpha6" "cluster.x-k8s.io/v1beta1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "OpenStackCluster"                         "Cluster" }}'
          name: '{{ .Values.cluster.name }}'
          namespace: '{{ .Release.Namespace }}'
        # wait for the control plane to be ready (the api/version/kind to look for depends on the bootstrap provider in use)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "controlplane.cluster.x-k8s.io/v1beta1" "controlplane.cluster.x-k8s.io/v1alpha1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "KubeadmControlPlane"                   "RKE2ControlPlane" }}'
          name: '{{ .Values.cluster.name }}-control-plane'
          namespace: '{{ .Release.Namespace }}'
        # all the above is subject to a race condition: if Flux checks the status too early
        # it concludes, because CAPI resources aren't fully kstatus compliant, that the resource is ready
        # waiting for the cluster kubeconfig Secret is a workaround
        - apiVersion: v1
          kind: Secret
          name: '{{ .Values.cluster.name }}-kubeconfig'
          namespace: '{{ .Release.Namespace }}'
    helmrelease_spec:
      targetNamespace: '{{ .Release.Namespace }}'
      chart:
        spec:
          chart: .
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: cluster_external_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_fip
          targetPath: cluster_public_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: control_plane_servergroup_id
          targetPath: control_plane.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: worker_servergroup_id
          targetPath: machine_deployment_default.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
    # we pass everything that is under `cluster` to this unit that uses sylva-capi-cluster chart
    # (we do it via a secret because some of the values are credentials in many scenarios)
    helm_secret_values: '{{ .Values.cluster | include "preserve-type" }}'

  heat-operator:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/heat-operator
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: heat-operator-controller-manager
            namespace: heat-operator-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext/seccompProfile
              value:
                type: RuntimeDefault
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: heat-operator-controller-manager
            namespace: heat-operator-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/1/securityContext/seccompProfile
              value:
                type: RuntimeDefault

  capo-cluster-resources:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      wait: true
      targetNamespace: '{{ .Release.Namespace }}'
      postBuild:
        substitute:
          STACK_NAME_PREFIX: '{{ .Values.cluster.name }}-{{ tuple . .Values.cluster.capo.resources_tag | include "interpret-as-string" | replace "." "-" }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.openstack.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.openstack.worker_affinity_policy }}'
          CAPO_EXTERNAL_NETWORK_ID: '{{ tuple .Values.openstack.external_network_id .Values.openstack.external_network_id | include "set-only-if" }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}'
          CAPO_CREATE_IRONIC_SECURITY_GROUP: '{{ tuple . (and (.Values.units.metal3.enabled | default false) (.Values.cluster.capi_providers.infra_provider | eq "capo")) "true" "false" | include "interpret-ternary" }}'
    kustomization_substitute_secrets:
      CAPO_CLOUD_YAML: '{{ .Values.cluster.capo.clouds_yaml  | toYaml | b64enc }}'
      CAPO_CACERT: '{{ (.Values.cluster.capo.cacert|default "") | b64enc }}'


  pause-cluster-reconciliation:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # Following condition is required in bootstrap phase, when sylva-units is installed in management-cluster
    # with cluster unit disabled before pivoting, this unit can't depend on a disabled unit in that case
    - '{{ tuple . "cluster" | include "unit-enabled" }}'
    depends_on:
      cluster: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: '{{ .Values.cluster.name }}-pause-cluster-job'
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
      _patches:
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: '{{ .Values.cluster.name }}-pause-cluster-job-{{ .Release.Namespace }}-cm'
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash
              set -e
              kubectl -n $TARGET_NAMESPACE wait MachineDeployment -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }} --for condition=Ready --timeout 600s
              kubectl -n $TARGET_NAMESPACE patch cluster.cluster.x-k8s.io  {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": true}}'

  resume-cluster-reconciliation:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # This unit should not be enabled on first run when cluster kustomisation is not yet created
    - '{{ not (lookup "kustomize.toolkit.fluxcd.io/v1" "Kustomization" .Release.Namespace "cluster" | empty) }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: '{{ .Values.cluster.name }}-resume-cluster-job'
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
      _patches:
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: '{{ .Values.cluster.name }}-resume-cluster-job-{{ .Release.Namespace }}-cm'
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash
              kubectl -n $TARGET_NAMESPACE patch cluster.cluster.x-k8s.io {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": false}}'

  calico-crd:
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-calico-crd
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-calico-crd
          version: "" # will be defined by helm_chart_versions


  # this unit is here to allow for upgrading Calico chart when upgrading cluster
  # v1.25.x to v1.26.x, see https://gitlab.com/sylva-projects/sylva-core/-/issues/664
  tigera-clusterrole:
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/tigera-clusterrole
      wait: true
      force: true

  calico:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'  # installed by RKE2 when RKE2 is used
    depends_on:
      calico-crd: true
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-calico
          version: "" # will be defined by helm_chart_versions
      values:
        installation:
          calicoNetwork:
            bgp: Enabled
          registry: UseDefault

  metallb:
    enabled_conditions:
      - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    depends_on:
      calico: true
    helm_repo_url: https://metallb.github.io/metallb
    helmrelease_spec:
      targetNamespace: metallb-system
      chart:
        spec:
          chart: metallb
          version: 0.13.12
      values:
        controller:
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
        speaker:
          frr:
            enabled: false
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane

  cinder-csi-psp:
    enabled_conditions:
    - '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") }}'
    - '{{ tuple . "cinder-csi" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cinder-csi-psp
      targetNamespace: cinder-csi
      wait: true

  cinder-csi:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    depends_on:
      namespace-defs: true
      cinder-csi-psp: '{{ tuple . "cinder-csi-psp" | include "unit-enabled" }}'
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.28.1
      targetNamespace: cinder-csi
      install:
        createNamespace: false
      values:
        clusterID: '{{ .Values.cluster.capo.resources_tag }}'
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.openstack.storageClass.name }}"
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.openstack.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
            [Global]
            auth-url = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url | quote }}
            tenant-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name | quote }}
            domain-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name | quote }}
            username = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username | quote }}
            password = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password | quote }}
            region = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.region_name | quote }}
            tls-insecure = {{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}
            [BlockStorage]
            ignore-volume-az = true
            {{- end -}}

  sylva-ca-certs:
    depends_on:
      namespace-defs: true
      cert-manager: true
      sylva-ca: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca-certs
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.rancher.external_hostname }}'
          VAULT_DNS: '{{ .Values.vault.external_hostname }}'
          KEYCLOAK_DNS: '{{ .Values.keycloak.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.flux.external_hostname }}'
      wait: true

  synchronize-secrets:
    depends_on:
      eso-secret-stores: true
      vault-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/synchronize-secrets
      wait: true
      postBuild:
        substitute:
          FLUX_ADMIN_USERNAME: '{{ .Values.flux_webui.admin_user }}'

  rancher:
    depends_on:
      cert-manager: true
      k8s-gateway: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      synchronize-secrets: true
    annotations:
      sylvactl/readyMessage: "Rancher UI can be reached at https://{{ .Values.rancher.external_hostname }} ({{ .Values.rancher.external_hostname }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://releases.rancher.com/server-charts/latest
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: 2.7.8
      targetNamespace: cattle-system
      interval: 10m0s
      upgrade:
        remediation:
          retries: 3
      values:
        privateCA: true
        useBundledSystemChart: true
        hostname: '{{ .Values.rancher.external_hostname }}'
        ingress:
          enabled: true
          ingressClassName: nginx
          tls:
            source: secret
            secretName: rancher-tls
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        postDelete:
          namespaceList:
            - cattle-fleet-system
            - rancher-operator-system
        extraEnv:
          - name: CATTLE_BOOTSTRAP_PASSWORD
            valueFrom:
              secretKeyRef:
                name: bootstrap-secret
                key: bootstrapPassword
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: rancher
                spec:
                  template:
                    spec:
                      volumes:
                        - name: tls-ca-volume
                          secret:
                            defaultMode: 256
                            secretName: rancher-tls
                            items:
                              - key: ca.crt
                                path: cacerts.pem
                      # this is to avoid that the too-short default liveness probe
                      # prevents the Rancher installation from finishing before the pod is killed:
                      containers:
                        - name: rancher
                          livenessProbe:
                            initialDelaySeconds: 120
                            periodSeconds: 30
                            failureThreshold: 20
              - kind: Pod
              apiVersion: pod/v1
              metadata:
                name: rancher-webhook
              spec:
                template:
                  spec:
                    containers:
                      - name: rancher-webhook
                        readinessProbe:
                          httpGet:
                            path: /healthz
                            port: https
                            scheme: HTTPS
                          httpHeaders:
                          - name: Custom-Header
                            value: Awesome
                          initialDelaySeconds: 3
                          periodSeconds: 3

    kustomization_spec:
      # these healthchecks are added so that this unit does not become ready before
      # a few things that Rancher sets up behind the scene is ready
      healthChecks:
        - apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          name: clusters.provisioning.cattle.io  # this is because capi-rancher-import needs this
        - apiVersion: apps/v1
          kind: Deployment
          name: rancher-webhook
          namespace: cattle-system
        - apiVersion: v1
          kind: Service
          name: rancher-webhook
          namespace: cattle-system

  rancher-keycloak-oidc-provider:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
      keycloak: true
      keycloak-resources: true
      keycloak-oidc-external-secrets: true
    kustomization_spec:
      path: ./kustomize-units/rancher-keycloak-oidc-provider
      postBuild:
        substitute:
          KEYCLOAK_EXTERNAL_URL: '{{ .Values.keycloak.external_hostname }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.rancher.external_hostname }}'
      wait: true

  k8s-gateway:
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: 2.0.4
      targetNamespace: k8s-gateway
      install:
        createNamespace: true
      values:
        domain: '{{ .Values.cluster_external_domain }}'
        replicaCount: 3
        service:
          loadBalancerIP: '{{ .Values.cluster_external_ip }}'
          annotations:
            metallb.universe.tf/allow-shared-ip: cluster-external-ip
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: k8s-gateway
                spec:
                  template:
                    spec:
                      containers:
                      - name: k8s-gateway
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  capd-metallb-config:
    enabled_conditions:
    - '{{ tuple . "metallb" | include "unit-enabled" }}'
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      metallb: true
    kustomization_spec:
      path: ./kustomize-units/metallb-config
      wait: true
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster_external_ip }}'

  capi-rancher-import:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: capi-rancher-import
    depends_on:
      rancher: true
      k8s-gateway: true
    helmrelease_spec:
      chart:
        spec:
          chart: charts/capi-rancher-import
      targetNamespace: capi-rancher-import
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          privileged: false
          runAsNonRoot: true
          runAsGroup: 1
          runAsUser: 1
          seccompProfile:
            type: RuntimeDefault
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.rancher.external_hostname }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: capi-rancher-import
            namespace: default
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      4.5.202: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      4.6.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-ingress-nginx
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-ingress-nginx
          version: "" # will be defined by helm_chart_versions
      values:
        fullnameOverride: rke2-ingress-nginx
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            externalIPs:
            - '{{ .Values.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          publishService:
            enabled: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: rke2-ingress-nginx-controller
          namespace: kube-system

  first-login-rancher:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/first-login-rancher
      postBuild:
        substitute:
          RANCHER_EXTERNAL_URL: '{{ .Values.rancher.external_hostname }}'
          CURRENT_TIME: '{{ now | date "2006-01-02T15:04:05.999Z" }}'
      wait: true

  flux-webui:
    depends_on:
      flux-system: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      keycloak-add-client-scope: true
      keycloak-oidc-external-secrets: true
    annotations:
      sylvactl/readyMessage: "Flux Web UI can be reached at https://{{ .Values.flux.external_hostname }} ({{ .Values.flux.external_hostname }} must resolve to {{ .Values.display_external_ip }})"
    repo: weave-gitops
    helm_chart_artifact_name: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        - name: WEAVE_GITOPS_FEATURE_OIDC_BUTTON_LABEL
          value: "Log in with Keycloak"
        installCRDs: true
        adminUser:
          create: true
          username: '{{ .Values.flux_webui.admin_user }}'
          createSecret: false
        rbac:
          impersonationResourceNames: ["admin", "sylva-admin@example.com"] # the Keycloak username set in unit keycloak-resources; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "watch" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: '{{ .Values.flux.external_hostname }}'
              paths:
                - path: /   # setting this to another value like '/flux-webui' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
          tls:
            - secretName: flux-webui-tls
              hosts:
                - '{{ .Values.flux.external_hostname }}'
        extraVolumes:
          - name: custom-ca-cert
            secret:
              secretName: flux-webui-tls
              items:
                - key: ca.crt
                  path: ca.crt
        extraVolumeMounts:
          - name: custom-ca-cert
            mountPath: /etc/ssl/certs
            readOnly: true
        oidcSecret:
          create: false
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: ClusterRoleBinding
                apiVersion: rbac.authorization.k8s.io/v1
                metadata:
                  name: '{{ .Values.flux_webui.admin_user }}-user-read-resources-cr'
                subjects:
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: '{{ .Values.flux_webui.admin_user }}'
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: sylva-admin@example.com  # add same RBAC for the SSO user, so that when flux-webui SA impersonates it has privileges; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: flux-webui-weave-gitops
                spec:
                  template:
                    spec:
                      containers:
                      - name: weave-gitops
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          privileged: false
                          readOnlyRootFilesystem: true
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  monitoring-crd:
    enabled_conditions:
    - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true
      values:
        global:
          cattle:
            psp:
              enabled: '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") | include "as-bool" }}'

  monitoring:
    depends_on:
      monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true
      values:
        global:
          cattle:
            psp:
              enabled: '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") | include "as-bool" }}'
      # Disable drift detection for rancher-monitoring webhooks
      _postRenderers:
        - kustomize:
            patches:
            - target:
                kind: (ValidatingWebhookConfiguration|MutatingWebhookConfiguration)
              patch: |
                - op: add
                  path: /metadata/annotations/helm.toolkit.fluxcd.io~1driftDetection
                  value: disabled

  multus:
    enabled: false
    depends_on:
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io/
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-multus
          version: v3.9.3-build2023010901
      targetNamespace: kube-system
      install:
        createNamespace: false
      values:
        rke2-whereabouts:
          enabled: true
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: DaemonSet
                apiVersion: apps/v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts
              - kind: ServiceAccount
                apiVersion: v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts

  multus-ready:
    # Perform healthchecks outside of the multus unit,
    # in order to properly target workload cluster when we deploy multus in it
    enabled_conditions:
      - '{{ tuple . "multus" | include "unit-enabled" }}'
    depends_on:
      multus: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/dummy
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-ds
          namespace: kube-system
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-rke2-whereabouts
          namespace: kube-system

  sriov-crd:
    enabled_conditions:
    - '{{ tuple . "sriov" | include "unit-enabled" }}'
    depends_on:
      multus-ready: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov-crd
      targetNamespace: cattle-sriov-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: sriov-crd
          version: 102.0.0+up0.1.0

  sriov:
    enabled: false
    depends_on:
      sriov-crd: true
      sriov-psp: '{{ tuple . "sriov-psp" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov
      targetNamespace: cattle-sriov-system
      chart:
        spec:
          chart: sriov
          version: 102.0.0+up0.1.0
      values:
        cert_manager: true
        rancher-nfd:
          image:
            tag: v0.13.2-build20230605
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: sriov
                  namespace: cattle-sriov-system
                spec:
                  template:
                    spec:
                      securityContext:
                        runAsGroup: 3000
                        runAsUser: 2000

  sriov-psp:
    enabled_conditions:
    - '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") }}'
    - '{{ (tuple . "sriov" | include "unit-enabled") }}'
    depends_on:
      sriov-crd: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/workload-cluster-sriov-psp
      wait: true

  sriov-resources:
    enabled: false
    depends_on:
      sriov: true
    repo: sriov-resources
    helm_chart_artifact_name: sriov-resources
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: cattle-sriov-system
      values:
        node_policies: '{{ .Values.sriov.node_policies | include "preserve-type" }}'

  coredns:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/coredns
      wait: true
      _patches:
        - target:
            kind: ConfigMap
            name: coredns
          patch: |
            - op: replace
              path: /metadata/name
              value: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpk") | ternary "coredns" "rke2-coredns-rke2-coredns" }}'
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster_external_ip }}'
          CLUSTER_EXTERNAL_DOMAIN: '{{ .Values.cluster_external_domain }}'

  longhorn-crd:
    enabled_conditions:
    - '{{ tuple . "longhorn" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn-crd
      targetNamespace: longhorn-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: longhorn-crd
          version: 102.2.3+up1.4.4

  longhorn:
    enabled: false
    depends_on:
      longhorn-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn
      targetNamespace: longhorn-system
      install:
        createNamespace: false
      chart:
        spec:
          chart: longhorn
          version: 102.2.3+up1.4.4
      values:
        enablePSP: '{{ and (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.cis_profile | eq "cis-1.6") | include "preserve-type" }}'
        defaultSettings:
          createDefaultDiskLabeledNodes: true
          defaultDataPath: /var/lib/longhorn
          deletingConfirmationFlag: true
          autoCleanupSystemGeneratedSnapshot: true

  cluster-creator-login:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-login
      wait: false
      force: true
      postBuild:
        substitute:
          JOB_NAME: cluster-creator-login
          JOB_TARGET_NAMESPACE: flux-system
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.rancher.external_hostname }}'
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: cluster-creator-kubeconfig
          namespace: flux-system
        - apiVersion: batch/v1
          kind: Job
          name: cluster-creator-login-flux-system
          namespace: kube-job

  cluster-creator-policy:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      cluster-creator-login: true
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-policy
      wait: true
      force: true

  os-image-server:
    # this units allows to deploy web server on management cluster
    # which serves OS image file for baremetal workload cluster
    enabled_conditions:
      - '{{ tuple . .Values._internal.metal3_unit | include "unit-enabled" }}'
    depends_on:
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: os-image-server
    annotations:
      sylvactl/readyMessage: |
        OS images are served at:
        {{- $osImageFqdn := (coalesce .Values.os_image_server.external_hostname .Values.display_external_ip) -}}
        {{- range $imageName, $imageParams := (coalesce .Values.os_images .Values._internal.default_os_images) }}
          * https://{{ $osImageFqdn }}/{{ $imageParams.filename }}(.sha256sum)
        {{- end }}
        {{- if not (eq $osImageFqdn .Values.display_external_ip)}}
        ({{ .Values.os_image_server.external_hostname }} must resolve to {{ .Values.display_external_ip }})
        {{- end }}
    helmrelease_spec:
      chart:
        spec:
          chart: ./charts/os-image-server
      targetNamespace: os-images
      install:
        createNamespace: true
      timeout: 168h  # leave plenty of time to download OS images in initContainers
      values:
        downloader:
          proxy: '{{ get .Values.proxies "https_proxy" }}'
          no_proxy: '{{ include "sylva-units.no_proxy" . }}'
          extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        nginx:
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        ingress:
          className: nginx
          hosts:
            - host: '{{ .Values.os_image_server.external_hostname }}'
        osImages: '{{ coalesce .Values.os_images .Values._internal.default_os_images | include "preserve-type" }}'
        osImagePersistenceDefaults:
          enabled: true
          size: 3Gi
          storageClass: '{{ .Values._internal.default_storage_class }}'

  capo-contrail-bgpaas:
    enabled: false
    repo: capo-contrail-bgpaas
    helm_chart_artifact_name: capo-contrail-bgpaas
    depends_on:
      heat-operator: '{{ tuple . "heat-operator" | include "unit-enabled" }}'
      capo: '{{ tuple . "capo" | include "unit-enabled" }}'
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: capo-contrail-bgpaas-system
      install:
        createNamespace: true
      values:
        conf:
          env:
            DEFAULT_PORT: '0'
            DEFAULT_ASN: '64512'

  vsphere-cloud-controller-manager:
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: sylva-core
    kustomization_spec:
      path: kustomize-units/vsphere-cloud-controller-manager
      targetNamespace: kube-system
      wait: true
      postBuild:
        substitute:
          SERVER: '{{ .Values.cluster.capv.server }}'
          TLS_THUMBPRINT: '{{ .Values.cluster.capv.tlsThumbprint }}'
          DATACENTER: '{{ .Values.cluster.capv.dataCenter }}'
    kustomization_substitute_secrets:
      USERNAME: '{{ .Values.cluster.capv.username }}'
      PASSWORD: '{{ .Values.cluster.capv.password }}'


  vsphere-csi-driver:
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: sylva-core
    kustomization_spec:
      path: kustomize-units/vsphere-csi-driver
      targetNamespace: kube-system
      wait: true
      postBuild:
        substitute:
          SERVER: '{{ .Values.cluster.capv.server }}'
          DATACENTER: '{{ .Values.cluster.capv.dataCenter }}'
          NETWORK: '{{ .Values.cluster.capv.network }}'
          CLUSTER_ID: '{{ printf "%s-%s" .Values.cluster.name (randAlphaNum 10) | trunc 64 }}'
          STORAGE_POLICY_NAME: '{{ .Values.cluster.capv.storagePolicyName | default "" }}'
    kustomization_substitute_secrets:
      USERNAME: '{{ .Values.cluster.capv.username }}'
      PASSWORD: '{{ .Values.cluster.capv.password }}'

# creates the sandbox namespaces used to perform privileged operations like debugging a node
  sandbox-privileged-namespace:
    enabled: false
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sandbox-privileged-namespace
      wait: true
      prune: true

##### stuff related to the 'cluster' unit #####
#
# all these values under 'cluster' are passed as values to sylva-capi-cluster chart

cluster:
  name: management-cluster

  # can be set to true to do an RKE2 deployment disconnected from the Internet:
  air_gapped: false

  # cis profile to be used. Curently supported only for rke2 clusters. "cis-1.6" for k8s prior to 1.25, "cis-1.23" for 1.25+
  cis_profile: cis-1.23

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capd      # capd, capo, capm3 or capv
    bootstrap_provider: cabpk # cabpr (RKE2) or cabpk (kubeadm)

  # kubernetes version to be used
  k8s_version: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.26.9+rke2r1" "v1.26.9" }}'

  # kube_vip version to be used for kubeadm deployments
  images:
    kube_vip:
      repository: ghcr.io/kube-vip/kube-vip
      tag: "v0.6.3"

  # Nodes number for control-plane
  control_plane_replicas: 3

  capo:
    # flavor_name: m1.large # Openstack flavor name
    # image_name: # OpenStack image name
    # ssh_key_name: # OpenStack VM SSH key
    # network_id: # OpenStack network used for nodes and VIP port
    # rootVolume: {} # Let this parameter empty if you don't intent to use root volume
    #   # otherwise, provide following values
    #   diskSize: 20 # Size of the VMs root disk
    #   volumeType: '__DEFAULT__' # Type of volume to be created
    # #control_plane_az: # list of OpenStack availability zones to deploy control planes nodes to, otherwise all would be candidates
    # clouds_yaml: # (this is a dict, not a YAML string)
    #   clouds:
    #     capo_cloud:
    #       auth:
    #         auth_url: # replace me
    #         user_domain_name: # replace me
    #         project_domain_name: # replace me
    #         project_name: # replace me
    #         username: # replace me
    #         password: # replace me
    #       region_name: # replace me
    #       verify: # e.g. false
    # cacert: # cert used to validate CA of OpenStack APIs

    # tag set for OpenStack resources in management cluster:
    resources_tag: >-
      {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
        sylva-{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}
      {{- end -}}

  control_plane:
    capo:
      security_group_name: "capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}"

  machine_deployment_default:
    capo:
      security_group_name: "capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}"

  cluster_external_ip: '{{ .Values.cluster_external_ip }}'

  cluster_public_ip: '{{ tuple (.Values.openstack.floating_ip) (not (eq .Values.openstack.floating_ip "")) | include "set-only-if" }}'
  cluster_services_cidrs:
    - 10.43.0.0/16
  cluster_pods_cidrs:
    - 10.42.0.0/16

  dns_resolver: false
  ntp: '{{ .Values.ntp | include "preserve-type" }}'
  proxies:
    http_proxy: '{{ .Values.proxies.http_proxy }}'
    https_proxy: '{{ .Values.proxies.https_proxy }}'
    no_proxy: '{{ include "sylva-units.no_proxy" . }}'

  registry_mirrors: '{{ .Values.registry_mirrors | include "preserve-type" }}'

  metallb_helm_oci_url: ""  # for an OCI-based deployment this is overridden in use-oci-artifacts.values.yaml
  metallb_helm_version: '{{ .Values.units.metallb.helmrelease_spec.chart.spec.version }}'
  metallb_helm_extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'

  capv:
    # image_name: # vSphere image name
    # username: ""
    # password: ""
    # dataCenter: ""
    # server: ""
    # dataStore: ""
    # tlsThumbprint: ""
    # folder: ""
    # resourcePool: ""
    # storagePolicyName: ""
    # networks:
    #   default:
    #     networkName: ""
    # ssh_key: ''
    numCPUs: 4

  capm3:
    nodeReuse: '{{ not (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | include "as-bool" }}'  # until https://gitlab.com/sylva-projects/sylva-core/-/issues/705 is solved
    machine_image_checksum_type: sha256

  enable_longhorn: '{{ or (tuple . "longhorn" | include "unit-enabled") (tuple . .Values.units.longhorn.enabled | include "interpret-for-test") | include "as-bool" }}'

capd_docker_host: unix:///var/run/docker.sock

openstack:
  #external_network_id # Can be provided if a FIP is needed in order to reach the management cluster VIP
  floating_ip: ""  # will typically be set by capo-cluster-resources
  storageClass:
    name: cinder-csi  # name of the storageClass to be created
    #type: xxx  # please provide the cinder volume type, e.g. 'ceph_sas' (must exist in OpenStack)
  control_plane_affinity_policy: soft-anti-affinity
  worker_affinity_policy: soft-anti-affinity

metal3:
  provider: sylva # sylva or suse
  # external_bootstrap_ip:
  # bootstrap_ip:

cluster_external_ip: 55.55.55.55

# Admin password that will be configured by default on various units  # FIXME, only used for SSO today see https://gitlab.com/sylva-projects/sylva-core/-/issues/503
admin_password: '{{ .Values._internal.default_password }}'

flux_webui:
  admin_user: admin

display_external_ip: '{{ .Values.openstack.floating_ip | eq "" | ternary .Values.cluster_external_ip .Values.openstack.floating_ip }}'

cluster_external_domain: sylva

rancher:
  external_hostname: 'rancher.{{ .Values.cluster_external_domain }}'

vault:
  external_hostname: 'vault.{{ .Values.cluster_external_domain }}'

keycloak:
  external_hostname: 'keycloak.{{ .Values.cluster_external_domain }}'

flux:
  external_hostname: 'flux.{{ .Values.cluster_external_domain }}'

neuvector:
  external_hostname: 'neuvector.{{ .Values.cluster_external_domain }}'

os_image_server:
  external_hostname: ''

# cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened
cis_benchmark_scan_profile: '{{ eq .Values.cluster.capi_providers.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

# osImages images that should be served and from where they should be downloaded
# if empty default value are used (default_os_images)
os_images: {}

# to configure the SR-IOV VFs on the supported NICs of cluster nodes
sriov:
  node_policies: {}
#   mypolicy1:
#     nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
#     resourceName: ""
#     numVfs: ""
#     deviceType: ""  # supported values: "netdevice" or "vfio-pci"
#     nicSelector:
#       deviceID: ""
#       vendor: ""
#       pfNames: []
#       rootDevices: []

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""

# disable default values for no_proxy (localhost,.svc,.cluster.local.,.cluster.local,.sylva)
# Ex: localhost: false
no_proxy_additional: {}

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
registry_mirrors:
  default_settings:                          # <<< These settings will apply to all configured mirrors
    capabilities: ["pull", "resolve"]
#   skip_verify: true
#   override_path: true
# hosts_config:
#   docker.io:
#   - mirror_url: http://your.mirror/docker
#     registry_settings:                     # <<< Host settings can be used to override default_settings
#       skip_verify: false
#   registry.k8s.io:
#   - mirror_url: ...
#   _default:
#   - mirror_url: ...

# deploy emulated baremetal nodes in bootstrap cluster
libvirt_metal:
  image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal:{{ .Values.source_templates | dig "libvirt-metal" "spec" "ref" "tag" "_undefined_"}}
  nodes: {}
  #management-cp:
  #  memGB: 12
  #  numCPUs: 6
  #
  #workload-cp:
  #  memGB: 4
  #  numCPUs: 2
  #
  #workload-md:
  #  memGB: 2
  #  numCPUs: 2


# set the type of environment between 3 possible values: dev, ci and prod
env_type: prod

# set NTP servers by IP or FQDN and enable their usage for control plane nodes
ntp:
  enabled: false
  servers:
    - 1.2.3.4
    - europe.pool.ntp.org

# These two sylva_core_oci_registry/sylva_base_oci_registry values govern which OCI repos are used
#
# This matters for:
# 1) OCI deployments
# 2) for non-OCI deployments for retrieving artifacts such as metal3 OS images
#
# For (1) sylva_base_oci_registry is automatically derived from the OCI repo used for sylva-unit HelmRelease.
# For (2) sylva_base_oci_registry can be customized, to use an OCI registry other than registry.gitlab.com/sylva-projects
#
# It should in general not be any need to override sylva_core_oci_registry, which is derived
# from sylva_base_oci_registry.
#
# sylva_base_oci_registry defaults to oci://registry.gitlab.com/sylva-projects
sylva_base_oci_registry:
  '{{
  regexReplaceAll
    "/sylva-core/?$"
    (lookup "source.toolkit.fluxcd.io/v1beta2" "HelmRepository" "default" "sylva-core" | dig "spec" "url" "")
    ""
  | default "oci://registry.gitlab.com/sylva-projects"
  }}'
sylva_core_oci_registry: '{{ .Values.sylva_base_oci_registry }}/sylva-core'


_internal:
  # unless the default password has already been generated by a previous run, we generate a new one randomly
  default_password: '{{ lookup "v1" "Secret" "default" "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "default_password" (randAlphaNum 64) }}'

  default_storage_class: >-
    {{- if (tuple . "cinder-csi" | include "unit-enabled") -}}
      {{ .Values.openstack.storageClass.name }}
    {{- else if (tuple . "vsphere-csi-driver" | include "unit-enabled") -}}
      vsphere-csi
    {{- else if (tuple . "longhorn" | include "unit-enabled") -}}
      longhorn
    {{- else -}}
      local-path
    {{- end -}}

  default_storage_class_unit: >-
    {{- if (tuple . "cinder-csi" | include "unit-enabled") -}}
      cinder-csi
    {{- else if (tuple . "vsphere-csi-driver" | include "unit-enabled") -}}
      vsphere-csi-driver
    {{- else if (tuple . "longhorn" | include "unit-enabled") -}}
      longhorn
    {{- else -}}
      local-path-provisioner
    {{- end -}}

  vault_replicas: >-
    {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}

  # this is used in source_templates.sylva-core below
  # and in any unit using sylva-units Helm chart
  sylva_core_version: ''

  sylva_core_existing_source: '{{ .Values.unit_helmrelease_kustomization_spec_default.sourceRef }}'

  # We need to retrieve bootstrap_node_ip in management-cluster while using libvirt-metal emulation, it is stored in a ConfigMap by libvirt-metal unit in that case
  bootstrap_node_ip: '{{ lookup "v1" "ConfigMap" "default" "cluster-public-endpoint" | dig "data" "address" "not found" | default "not found" }}'

  # These values are used only if `os_images` is empty
  default_os_images:
    ubuntu-jammy-plain-rke2-1-26-9:
      uri: "{{ .Values.sylva_base_oci_registry }}/sylva-elements/diskimage-builder/ubuntu-jammy-plain-rke2-1.26.9:0.0.12"
      filename: ubuntu-jammy-plain-rke2-1.26.9.qcow2
      checksum: ffdc81fcdc0104151aa792a508eefe0d47660b18683949edcd734b3a4f938f20

  metal3_unit: '{{ .Values.metal3.provider | eq "suse" | ternary "metal3-suse" "metal3" }}'

  k8s_post_1_26_6: '{{ semverCompare ">1.26.6" (tuple . .Values.cluster.k8s_version | include "interpret-as-string") | include "preserve-type" }}'

shared_workload_clusters_values:
  ntp: '{{ .Values.ntp | include "set-if-defined" }}'
  proxies: '{{ .Values.proxies | include "set-if-defined" }}'
  registry_mirrors: '{{ .Values.registry_mirrors | include "set-if-defined" }}'

  sylva_base_oci_registry: '{{ .Values.sylva_base_oci_registry }}'

  cluster:
    air_gapped: '{{ .Values.air_gapped | include "set-if-defined" }}'
    capi_providers: '{{ .Values.cluster.capi_providers | include "preserve-type" }}'
    mgmt_cluster_external_ip: '{{ .Values.cluster_external_ip }}'
    mgmt_cluster_external_domain: '{{ .Values.cluster_external_domain }}'
    dns_resolver: true
    capo:
      image_name: '{{ .Values.cluster.capo.image_name | include "set-if-defined" }}'
      network_id: '{{ .Values.cluster.capo.network_id | include "set-if-defined" }}'
  units:
    cluster-import:
      enabled: '{{ tuple . "rancher" | include "unit-enabled" }}'
  metal3:
    provider: '{{ .Values.metal3.provider }}'

shared_workload_clusters_secret_values:
  cluster:
    capo: '{{ tuple (dict "clouds_yaml" (.Values.cluster | dig "capo" "clouds_yaml" dict)) (.Values.cluster | dig "capo" "clouds_yaml" dict) | include "set-only-if" }}'
