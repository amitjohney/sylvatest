# Default values for sylva-units.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret: {}
  # when using gitlab, git creds can be used as registry creds
  #registry.gitlab.com:
  #  username: your_user_name
  #  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s

oci_repo_spec_default:
  interval: 60m0s

source_templates: # template to generate Flux GitRepository/OCIRepository resources
  # <repo-name>:
  #   kind:  GitRepository/OCIRepository
  #   spec:  # partial spec for a Flux resource
  #     url: https://gitlab.com/sylva-projects/sylva-core.git
  #     #secretRef: # is autogenerated based on 'auth'
  #     ref: # can be overridden per-unit, with 'ref_override'
  #       branch: main
  #   auth: # optional 'username'/'password' dict containing git authentication information
  #   existing_source: # optional, when this value is set the specified GitRepository or OCIRepository will be used instead of creating one based on 'spec'
  #     name: sylva-core
  #     kind: GitRepository or OCIRepository

  sylva-core:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-core.git
      ref:
        # this is provided only for reference
        # in practice the sylva-core framework will always override this ref so that the
        # currently checked out commit of sylva-core is used by sylva-units
        # (you can grep the code for "CURRENT_COMMIT" to find out why)
        branch: main

  capi-rancher-import:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capi-rancher-import.git
      ref:
        tag: 0.1.4

  weave-gitops:
    kind: GitRepository
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.38.0

  metal3:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3.git
      ref:
        tag: 0.7.0

  sylva-capi-cluster:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-capi-cluster.git
      ref:
        tag: 0.1.25

  local-path-provisioner:
    kind: GitRepository
    spec:
      url: https://github.com/rancher/local-path-provisioner.git
      ref:
        tag: v0.0.24

  sriov-resources:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sriov-resources.git
      ref:
        tag: 0.0.3

  os-image-server:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/os-image-server.git
      ref:
        tag: 1.8.0

  capo-contrail-bgpaas:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capo-contrail-bgpaas.git
      ref:
        tag: 1.0.3

  libvirt-metal:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal.git
      ref:
        tag: 0.1.7

  vault-operator:
    kind: GitRepository
    spec:
      url: https://github.com/bank-vaults/vault-operator.git
      ref:
        tag: v1.21.2

  cloud-provider-vsphere:
    kind: GitRepository
    spec:
      url: https://github.com/kubernetes/cloud-provider-vsphere.git
      ref:
        tag: v1.26.2

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'units'
unit_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: 2
      remediateLastFailure: false

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated by a unit with a "helmrelease_spec" field
unit_helmrelease_kustomization_spec_default:
  path: ./kustomize-units/helmrelease-generic
  sourceRef: >-
    {{- $existing_source := get (index .Values.source_templates "sylva-core") "existing_source" -}}
    {{- if $existing_source -}}
      {{- $existing_source | include "preserve-type" -}}
    {{- else -}}
      {{- dict "kind" (index .Values.source_templates "sylva-core" | dig "kind" "GitRepository") "name" "sylva-core" | include "preserve-type" -}}
    {{- end -}}
  wait: true

# default value used if units.xxx.enabled is not specified
units_enabled_default: false

# unit_template define unit settings
#   a unit can inherit from one of those
unit_templates:
  sylva-units:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/

  # dummy unit template is used to have a Kustomization
  # to add dependencies
  dummy:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/dummy
      wait: false
      postBuild:
        substitute:
          UNIT_NAME: '{{ .Values._unit_name_ }}'

# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository or OCIRepository (and Secret, TODO: the Secret don't need to be generated for each unit)
units:
  # <unit-name>:
  #   info: # unit metadata mainly for documentation purpose
  #     description: <short unit description>
  #     details: <more detailed data about unit purpose and usage>
  #     maturity: <level of integration in sylva stack of corresponding component>
  #     internal: true <for internal sylva units>
  #     version: <force declarative version, not recommended>
  #   enabled: boolean or GoTPL
  #   repo: <name of the repo under 'source_templates'> (for use with kustomization_spec)
  #   helm_repo_url: URL of the Helm repo to use (for use with helmrelease_spec, but not mandatory, 'repo' can be used as well to use a git repo)
  #   labels: (optional) dict holding labels to add to the resources for this unit
  #   ref_override: optional, if defined, this dict will be used for the GitRepository or OCIRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: dict defining the dependencies of this unit, keys are unit names, values are booleans
  #               (these dependencies are injected in the unit Kustomization via 'spec.dependsOn')
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from unit_helmrelease_spec_default
  #                      # and from other fields in the unit definition
  #     _postRenderers: # this field can be used in this file, it will be merged into user-provided 'postRenderers'
  #   helm_chart_artifact_name: optional, if specified, when deploying the Helm chart from an OCI artifact,
  #                             helm_chart_artifact_name will be used as chart name instead of helmrelease_spec.chart.spec.chart last path item
  #                             this is required if helmrelease_spec.chart.spec.chart is empty, '.' or '/'
  #                             (also used by tools/oci/push-helm-chart to generate the artifact)
  #   helm_chart_versions: optional, if specified, when deploying the Helm chart from an OCI artifact or Helm registry,
  #                             it will drive the version to be used from a dict of <version>:<boolean>
  #                             in case if helmrelease_spec.chart.spec.chart.version is not set
  #                             (also used by tools/oci/push-helm-charts-artifacts.sh to generate the artifact)
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from unit_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-unit-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <unit>.spec.path  (if <git repo template> has spec.url defined)
  #     _patches: # this field can be used in this file, it will be merged into user-provided 'patches'
  #     _components: # this field can be used in this file, it will be merged into user-provided 'components'
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret
  #   unit_template: optional, today used only for units that use sylva-units itself
  #                  the settings for the unit are inherited from the corresponding entry under unit_templates

  flux-system:
    info:
      description: contains Flux definitions *to manage the Flux system itself via gitops*
      details: Note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
      maturity: core-component
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/flux-system/base
      targetNamespace: flux-system
      wait: true
      # prevent Flux from uninstalling itself
      prune: false
      _components:
      - '{{ tuple "../components/extra-ca" .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
          EXTRA_CA_CERTS: '{{ tuple (.Values.oci_registry_extra_ca_certs | default "" | b64enc) .Values.oci_registry_extra_ca_certs | include "set-only-if" }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  cert-manager:
    info:
      description: installs cert-manager, an X.509 certificate controller
      maturity: core-component
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.13.3
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true
        replicaCount: >-
          {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 2 1 }}
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: cert-manager
                    app.kubernetes.io/component: controller
                topologyKey: kubernetes.io/hostname
        podDisruptionBudget:
          enabled: true
          minAvailable: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}
        webhook:
          replicaCount: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/instance: cert-manager
                      app.kubernetes.io/component: webhook
                  topologyKey: kubernetes.io/hostname
          podDisruptionBudget:
            enabled: true
            minAvailable: >-
              {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}
        cainjector:
          replicaCount: >-
            {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 2 1 }}
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      app.kubernetes.io/instance: cert-manager
                      app.kubernetes.io/component: cainjector
                  topologyKey: kubernetes.io/hostname
          podDisruptionBudget:
            enabled: true
            minAvailable: >-
              {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 1 0 }}

  trivy-operator:
    info:
      description: installs Trivy operator
      maturity: beta
    enabled: no
    helm_repo_url: https://aquasecurity.github.io/helm-charts/
    helmrelease_spec:
      chart:
        spec:
          chart: trivy-operator
          version: 0.20.0
      targetNamespace: trivy-system
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 10000
          runAsUser: 10000
        serviceAccount:
          annotations: {}
          create: true
          name: trivy-operator
        trivy:
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" . }}'
          severity: UNKNOWN,HIGH,CRITICAL
        trivyOperator:
          scanJobPodTemplatePodSecurityContext:
            runAsGroup: 10000
            runAsUser: 10000

  sylva-ca:
    info:
      description: provides a Certificate Authority for units of the Sylva stack
      internal: true
    depends_on:
      cert-manager: true
      external-secrets-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca
      targetNamespace: cert-manager
      wait: true


  namespace-defs:
    info:
      description: creates sylva-system namespace and other namespaces to be used by various units
      internal: true
    depends_on:
      kyverno-policies: '{{ tuple . "kyverno-policies" | include "unit-enabled" }}' # make sure that the policy disable-automountserviceaccounttoken applies
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/namespace-defs
      wait: true
      prune: false
      _components:
        - '{{ tuple "components/cinder-csi" (tuple . "cinder-csi" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/metal3" (tuple . "metal3" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/longhorn" (tuple . "longhorn" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/vault" (tuple . "vault" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/rancher" (tuple . "rancher" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/cattle-monitoring" (tuple . "monitoring" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/gitea" (tuple . "gitea" | include "unit-enabled") | include "set-only-if" }}'

# Postgresql deployment for keycloak
  postgres-init:
    info:
      description: initializes Postgresql for Keycloak
      internal: true
    enabled_conditions:
    - '{{ tuple . "postgres" | include "unit-enabled"  }}'
    depends_on:
      namespace-defs: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/postgres-init
      wait: true
      postBuild:
        substitute:
          MAX_POD_UNAVAILABLE: '{{ int .Values.cluster.control_plane_replicas | gt 3 | ternary 0 1 }}'

  postgres:
    info:
      description: installs Postgresql for Keycloak
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      postgres-init: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql
          version: 13.2.28
      targetNamespace: keycloak
      values:
        global:
          postgresql:
            auth:
              database: keycloak
        architecture: replication
        nameOverride: postgres
        serviceAccount:
          create: true
          name: postgresql
        readReplicas:
          replicaCount: '{{ int .Values.cluster.control_plane_replicas | gt 3 | ternary 1 3 }}'
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchLabels:
                      app.kubernetes.io/name: postgres
                      app.kubernetes.io/component: read
                  topologyKey: kubernetes.io/hostname

  kubevirt:
    info:
      description: installs kubevirt
      maturity: beta
    helm_repo_url: https://suse-edge.github.io/charts
    helmrelease_spec:
      chart:
        spec:
          chart: kubevirt
          version: 0.2.1
      targetNamespace: kubevirt
      install:
        createNamespace: true

  kubevirt-test-vms:
    info:
      description: deploys kubevirt VMs for testing
      internal: true
    depends_on:
      kubevirt: true
      multus: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kubevirt-test-vms
      wait: true
      targetNamespace: kubevirt-tests

  harbor:
    info:
      description: installs Harbor
      maturity: beta
    depends_on:
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: "Harbor UI can be reached at https://{{ .Values.external_hostnames.harbor }} ({{ .Values.external_hostnames.harbor }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://helm.goharbor.io
    helmrelease_spec:
      chart:
        spec:
          chart: harbor
          version: 1.14.0
      targetNamespace: harbor
      values:
        persistence:
          enabled: true
          resourcePolicy: "keep"
          persistentVolumeClaim:
            registry:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 32Gi
            chartmuseum:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 32Gi
            jobservice:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            database:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            redis:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
            #trivy:
              #storageClass: '{{ .Values._internal.default_storage_class }}'
              #size: 8Gi
        expose:
          ingress:
            enabled: true
            hosts:
              core: '{{ .Values.external_hostnames.harbor }}'
        notary:
          enabled: false
        trivy:
          enabled: false
      _postRenderers:
        - kustomize:
            patches:
              - target:
                  kind: Deployment|StatefulSet
                patch: |-
                  - op: add
                    path: /spec/template/spec/containers/0/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
              - target:
                  kind: Deployment
                  name: harbor-registry
                patch: |-
                  - op: add
                    path: /spec/template/spec/containers/1/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
              - target:
                  kind: StatefulSet
                  name: harbor-database
                patch: |-
                  - op: add
                    path: /spec/template/spec/initContainers/0/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
                  - op: add
                    path: /spec/template/spec/initContainers/1/securityContext
                    value:
                      runAsNonRoot: true
                      seccompProfile:
                        type: RuntimeDefault
                      allowPrivilegeEscalation: false
                      capabilities:
                        drop:
                          - ALL
      install:
        createNamespace: true

  vault-operator:
    info:
      description: installs Vault operator
      maturity: stable
    repo: vault-operator
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/charts/vault-operator
      targetNamespace: vault
      install:
        createNamespace: true
      values:
        image:
          repository: ghcr.io/bank-vaults/vault-operator
          tag: '{{ .Values.source_templates | dig "vault-operator" "spec" "ref" "tag" "" | required "source_templates.vault-operator.spec.ref.tag is unset" }}'

  vault:
    info:
      description: installs Vault
      details: |
        Vault assumes that the certificate vault-tls has been issued
      maturity: stable
    depends_on:
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      vault-operator: true
    annotations:
      sylvactl/readyMessage: "Vault UI can be reached at https://{{ .Values.external_hostnames.vault }} ({{ .Values.external_hostnames.vault }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_substitute_secrets:
      ADMIN_PASSWORD: '{{ .Values.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/vault
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          VAULT_REPLICAS: '{{ .Values._internal.vault_replicas }}'
          MAX_POD_UNAVAILABLE: '{{ int .Values._internal.vault_replicas | eq 1 | ternary 0 1 }}'
          AFFINITY: '{{ and (eq (int .Values.cluster.control_plane_replicas) 1) (ne .Values._internal.default_storage_class_unit "local-path") | ternary (.Values._internal.vault_no_affinity | toJson | indent 12 ) (.Values._internal.vault_affinity | toJson| indent 12)  }}'
      healthChecks:  # sometimes this kustomization seems correctly applied while vault pod is not running, see https://gitlab.com/sylva-projects/sylva-core/-/issues/250
      # so we replace wait:true by checking for the Vault components health
        - apiVersion: apps/v1
          kind: StatefulSet
          name: vault
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-configurer
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-configurer
          namespace: vault

  vault-config-operator:
    info:
      description: installs Vault config operator
      maturity: stable
    depends_on:
      namespace-defs: true
      cert-manager: true
      monitoring: '{{ .Values.units | dig "vault-config-operator" "helmrelease_spec" "values" "enableMonitoring" true }}'
    helm_repo_url: https://redhat-cop.github.io/vault-config-operator
    helmrelease_spec:
      chart:
        spec:
          chart: vault-config-operator
          version: v0.8.25
      targetNamespace: vault
      values:
        enableCertManager: true
        enableMonitoring: false

  vault-secrets:
    info:
      description: generates random secrets in vault, configure password policy, authentication backends, etc...
      internal: true
    repo: sylva-core
    depends_on:
      vault: true
      vault-config-operator: true
    kustomization_spec:
      path: ./kustomize-units/vault-secrets
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'

  vault-oidc:
    info:
      description: configures Vault to be used with OIDC
      internal: true
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak-resources: true
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/vault-oidc
      wait: true
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'

  external-secrets-operator:
    info:
      description: installs the External Secrets operator
      maturity: stable
    helm_repo_url: https://charts.external-secrets.io
    helmrelease_spec:
      chart:
        spec:
          chart: external-secrets
          version: 0.9.11
      targetNamespace: external-secrets
      install:
        createNamespace: true
      values:
        installCRDs: true

  eso-secret-stores:
    info:
      description: defines External Secrets stores
      internal: true
    depends_on:
      external-secrets-operator: true
      vault: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/eso-secret-stores
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'

  cis-operator-crd:
    info:
      description: install CIS operator CRDs
      maturity: stable
      hidden: true
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: 4.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    info:
      description: install CIS operator
      maturity: stable
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator-crd: true
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: 4.0.0
      targetNamespace: cis-operator-system

  cis-operator-scan:
    info:
      description: allows for running a CIS scan for management cluster
      details: |
        it generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
      internal: true
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    depends_on:
      cis-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cis-operator-scan
      wait: false
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cis_benchmark_scan_profile }}'

  neuvector-init:
    info:
      description: sets up Neuvector prerequisites
      details: |
        it generates namespace, certificate, admin password, policy exception for using latest tag images (required for the pod managing the database of vulnerabilities since this DB is updated often)
      internal: true
    enabled_conditions:
    - '{{ tuple . "neuvector" | include "unit-enabled" }}'
    depends_on:
      sylva-ca: true
      vault: true
      vault-config-operator: true
      kyverno: true
      keycloak-add-client-scope: true
      keycloak-oidc-external-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/neuvector-init
      wait: true
      postBuild:
        substitute:
          NEUVECTOR_DNS: '{{ .Values.external_hostnames.neuvector }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'

  neuvector:
    info:
      description: installs Neuvector
      maturity: beta
    enabled: no
    depends_on:
      neuvector-init: true
      ingress-nginx: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'
    annotations:
      sylvactl/readyMessage: "Neuvector UI can be reached at https://{{ .Values.external_hostnames.neuvector }} ({{ .Values.external_hostnames.neuvector }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://neuvector.github.io/neuvector-helm
    helm_chart_artifact_name: neuvector-core
    helmrelease_spec:
      chart:
        spec:
          chart: core
          version: 2.6.6
      targetNamespace: neuvector
      values:
        leastPrivilege: true
        internal:
          certmanager:
            enabled: true
            secretname: neuvector-internal
        controller:
          replicas: 1  # PVC only works for 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
          strategy:
            type: RollingUpdate
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1
          image:
            repository: neuvector/controller
          pvc:
            enabled: true # setting PVC to true imposes 1 replica https://github.com/neuvector/neuvector-helm/issues/110#issuecomment-1251921734
            accessModes:
              - ReadWriteOnce
        enforcer:
          image:
            repository: neuvector/enforcer
        manager:
          image:
            repository: neuvector/manager
          runAsUser: 10000
          ingress:
            enabled: true
            host: '{{ .Values.external_hostnames.neuvector }}'
            ingressClassName: nginx
            path: /
            annotations:
              nginx.ingress.kubernetes.io/backend-protocol: https
            tls: true
            secretName: neuvector-tls
        cve:
          updater:
            image:
              repository: neuvector/updater
          scanner:
            image:
              repository: neuvector/scanner
              env:
                - name: https_proxy
                  value: '{{ .Values.proxies.https_proxy }}'
                - name: no_proxy
                  value: '{{ include "sylva-units.no_proxy" . }}'
        containerd:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" | include "as-bool" }}'
          path: /var/run/containerd/containerd.sock
        k3s:
          enabled: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | include "as-bool" }}'
          runtimePath: /run/k3s/containerd/containerd.sock
        resources:
          limits:
            cpu: 400m
            memory: 2792Mi
          requests:
            cpu: 100m
            memory: 2280Mi
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: neuvector-updater-pod
                  namespace: neuvector
                spec:
                  startingDeadlineSeconds: 21600

  keycloak:
    info:
      description: initializes and configures Keycloak
      maturity: stable
    depends_on:
      sylva-ca: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      postgres: true
      synchronize-secrets: true # make sure that the secret keycloak-initial-admin is ready to be consummed
    annotations:
      sylvactl/readyMessage: "Keycloak admin console can be reached at https://{{ .Values.external_hostnames.keycloak }}/admin/master/console, user 'admin', password in Vault at secret/keycloak ({{ .Values.external_hostnames.keycloak }} must resolve to {{ .Values.display_external_ip }})"
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
      healthChecks:  # cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        # the Keycloak StatefulSet set is produced, by the combination of Keycloak operator
        # and a Keycloak custom resource, it relies on the postgres DB also deployed by this unit
        # hence, checking for the health of this component can be done by checking this StatefulSet
        - apiVersion: apps/v1
          kind: StatefulSet
          name: keycloak
          namespace: keycloak
      _components:
        - '{{ tuple "components/keycloak-operator-proxies" (.Values.proxies.https_proxy) | include "set-only-if" }}'
      _patches:
        - patch: |
            - op: replace
              path: /spec/template/spec/containers/0/securityContext
              value:
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
                privileged: false
                runAsNonRoot: true
                runAsGroup: 1000
                runAsUser: 1000
                seccompProfile:
                  type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-operator

  keycloak-legacy-operator:
    info:
      description: installs Keycloak "legacy" operator
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      vault-secrets: true  # the credential-external-keycloak Secret use by the legacy operator is generated from ES/Vault secret/data/keycloak
      keycloak: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-legacy-operator
      targetNamespace: keycloak
      wait: true
      _patches:
        - patch: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: keycloak-realm-operator
            spec:
              template:
                spec:
                  containers:
                    - name: keycloak-realm-operator
                      securityContext:
                        runAsUser: 10000
                        allowPrivilegeEscalation: false
                        capabilities:
                          drop:
                            - ALL
                        runAsNonRoot: true
                        seccompProfile:
                          type: RuntimeDefault
          target:
            kind: Deployment
            name: keycloak-realm-operator

  keycloak-resources:
    info:
      description: configures keycloak resources
      internal: true
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
    repo: sylva-core
    kustomization_substitute_secrets:
      SSO_PASSWORD: '{{ .Values.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak-resources
      targetNamespace: keycloak
      _components:
        - '{{ tuple "components/neuvector" (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.external_hostnames.rancher }}'
          FLUX_WEBUI_DNS: '{{ .Values.external_hostnames.flux }}'
          NEUVECTOR_DNS: '{{ tuple .Values.external_hostnames.neuvector (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          EXPIRE_PASSWORD_DAYS: '{{ int .Values.keycloak.keycloak_expire_password_days }}'
      healthChecks:  #  cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakRealm
          name: sylva
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-rancher-client  # this secret is a byproduct of the rancher-client KeycloakClient resource
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-flux-webui-client  # this secret is a byproduct of the flux-webui-client KeycloakClient resource
          namespace: keycloak
        - '{{ tuple (dict "apiVersion" "v1" "kind" "Secret" "name" "keycloak-client-secret-neuvector-client" "namespace" "keycloak") (tuple . "neuvector" | include "unit-enabled") | include "set-only-if" }}'  # # this secret is a byproduct of the neuvector client KeycloakClient resource


  keycloak-add-client-scope:
    info:
      description: configures Keycloak client-scope
      details: >
        a job to manually add a custom client-scope to sylva realm (on top of default ones)
        while CRD option does not yet provide good results (overrides defaults)
      internal: true
    enabled_conditions:
    - '{{ tuple . "flux-webui" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      keycloak-resources: true
      keycloak: true  # defines the keycloak-initial-admin Secret used by the script
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: keycloak-add-client-scope-job
          JOB_TARGET_NAMESPACE: keycloak
      _patches:
      - target:
          kind: Job
        patch: |
          - op: replace
            path: /spec/backoffLimit
            value: 15
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: keycloak-add-client-scope-job-keycloak-cm
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/keycloak-add-client-scope.sh" | indent 4 }}

  keycloak-oidc-external-secrets:
    info:
      description: configures OIDC secrets for Keycloack
      internal: true
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-oidc-external-secrets
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          FLUX_WEBUI_DNS: '{{ .Values.external_hostnames.flux }}'
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: oidc-auth
          namespace: flux-system

  kyverno:
    info:
      description: installs Kyverno
      maturity: stable
    helm_repo_url: https://kyverno.github.io/kyverno
    helmrelease_spec:
      chart:
        spec:
          chart: kyverno
          version: 3.1.3
      targetNamespace: kyverno
      install:
        createNamespace: true
      values:
        webhooksCleanup:
          enabled: false
        features:
          policyExceptions:
            enabled: true
        backgroundController:
          rbac:
            clusterRole:
              extraResources:
              - apiGroups: ["helm.toolkit.fluxcd.io"]
                resources:
                  - "helmreleases"
                verbs: ["get", "list", "watch", "patch", "update"]
              - apiGroups: [""]
                resources:
                  - "nodes"
                verbs: ["get", "list", "watch", "patch", "update"]
              - apiGroups: ["external-secrets.io"]
                resources:
                  - "externalsecrets"
                verbs: ["get", "list", "watch", "patch", "update"]
        config:
          # -- Resource types to be skipped by the Kyverno policy engine, removed [Node,*,*] [Node/*,*,*] entries
          resourceFilters:
            - '[Event,*,*]'
            - '[*/*,kube-system,*]'
            - '[*/*,kube-public,*]'
            - '[*/*,kube-node-lease,*]'
            - '[APIService,*,*]'
            - '[APIService/*,*,*]'
            - '[TokenReview,*,*]'
            - '[SubjectAccessReview,*,*]'
            - '[SelfSubjectAccessReview,*,*]'
            - '[Binding,*,*]'
            - '[Pod/binding,*,*]'
            - '[ReplicaSet,*,*]'
            - '[ReplicaSet/*,*,*]'
            - '[AdmissionReport,*,*]'
            - '[AdmissionReport/*,*,*]'
            - '[ClusterAdmissionReport,*,*]'
            - '[ClusterAdmissionReport/*,*,*]'
            - '[BackgroundScanReport,*,*]'
            - '[BackgroundScanReport/*,*,*]'
            - '[ClusterBackgroundScanReport,*,*]'
            - '[ClusterBackgroundScanReport/*,*,*]'
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: kyverno-cleanup-cluster-admission-reports
                  namespace: kyverno
                spec:
                  startingDeadlineSeconds: 180
              - kind: CronJob
                apiVersion: batch/v1
                metadata:
                  name: kyverno-cleanup-admission-reports
                  namespace: kyverno
                spec:
                  startingDeadlineSeconds: 180


  kyverno-policies:
    info:
      description: configures Kyverno policies
      internal: true
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kyverno-policies
      wait: true

  rancher-monitoring-clusterid-inject:
    info:
      description: injects Rancher cluster ID in Helm values of Rancher monitoring chart
      internal: true
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      kyverno: '{{ tuple . "kyverno" | include "unit-enabled" }}'
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/rancher-monitoring-clusterid-inject
      wait: true

  shared-workload-clusters-settings:
    info:
      description: manages parameters which would be shared between management and workload clusters
      internal: true
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/shared-workload-clusters-settings
      wait: true
      targetNamespace: sylva-system
      _patches:
        - target:
            kind: ConfigMap
          patch: |
            - op: replace
              path: /data/values
              value: |
            {{ index (tuple . .Values.shared_workload_clusters_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | indent 4 }}
    kustomization_substitute_secrets:
      SECRET_VALUES: '{{ index (tuple . .Values.shared_workload_clusters_secret_values | include "interpret-inner-gotpl" | fromJson) "result" | toYaml | b64enc }}'

  node-annotation-from-label:
    info:
      description: manages cluster node labels
      internal: true
    enabled: false # it blocks RKE2 node registrations when bootstrap is slower (for CAPM3 deployments) than Kyverno ClusterPolicy webhook install, https://gitlab.com/sylva-projects/sylva-core/-/issues/660
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      kyverno: '{{ tuple . "kyverno" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/node-annotation-from-label
      wait: true
      postBuild:
        substitute:
          NODE_LABEL_KEY_LIST: '{{ mergeOverwrite (.Values.cluster | dig "rke2" "node_annotations" dict) (.Values.cluster | dig "control_plane" "rke2" "node_annotations" dict) (.Values.cluster | dig "machine_deployment_default" "rke2" "node_annotations" dict) | keys | toJson }}' # need to address .cluster.machine_deployments.X.rke2.node_annotations

  capi:
    info:
      description: installs Cluster API core operator
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capi
      wait: true

  capd:
    info:
      description: installs Docker CAPI infra provider
      maturity: core-component
      kustomization_path: ./kustomize-units/capd/base
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: '{{ ternary "./kustomize-units/capd/base" "./kustomize-units/capd/rke2" (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") }}'
      postBuild:
        substitute:
          CAPD_DOCKER_HOST: '{{ .Values.capd_docker_host }}'
      wait: true

  capo:
    info:
      description: installs OpenStack CAPI infra provider
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capo
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: capo-controller-manager
            namespace: capo-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL

  capm3:
    info:
      description: installs Metal3 CAPI infra provider, for baremetal
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
      '{{ .Values._internal.metal3_unit }}': true
    kustomization_spec:
      path: ./kustomize-units/capm3
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  capv:
    info:
      description: installs vSphere CAPI infra provider
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capv
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  cabpk:
    info:
      description: installs Kubeadm CAPI bootstrap provider
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpk
      wait: true

  cabpr:
    info:
      description: installs RKE2 CAPI bootstrap provider
      maturity: core-component
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpr
      wait: true
      _patches:
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-bootstrap-controller-manager
            namespace: rke2-bootstrap-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
        - target:
            group: apps
            version: v1
            kind: Deployment
            name: rke2-control-plane-controller-manager
            namespace: rke2-control-plane-system
          patch: |-
            - op: add
              path: /spec/template/spec/containers/0/securityContext
              value:
                runAsNonRoot: true
                seccompProfile:
                  type: RuntimeDefault
                allowPrivilegeEscalation: false
                capabilities:
                  drop:
                    - ALL
  metal3-suse:
    info:
      description: installs SUSE-maintained Metal3 operator
      maturity: beta
    enabled: false
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://suse-edge.github.io/charts
    helmrelease_spec:
      chart:
        spec:
          chart: metal3
          version: 0.4.3
      install:
        createNamespace: true
      targetNamespace: metal3-system
      values:
        global:
          ironicIP: '{{ .Values.cluster_external_ip }}'
          provisioningInterface: eth0
        metal3-ironic:
          service:
            type: LoadBalancer
            externalIPs:
            - '{{ .Values.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          baremetaloperator:
            ironichostNetwork: false
          persistence:
            ironic:
              storageClass: '{{ .Values._internal.default_storage_class }}'
              accessMode: ReadWriteOnce
        metal3-mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'

  metal3-init:
    info:
      description: generates Metal3 random credentials
      internal: true
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-init
      wait: true

  metal3-sync-secrets:
    info:
      description: configures secrets for Metal3 components
      internal: true
    enabled_conditions:
    - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      metal3-init: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/metal3-sync-secrets
      wait: true

  metal3:
    info:
      description: install Metal3 operator
      maturity: stable
    enabled: false
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
      metal3-sync-secrets: true # make sure that the password secrets are ready to be consumed
    kustomization_spec:
      postBuild:
        substitute:
          # Force substitution in order to be consistent with what is done in bootstrap,
          # See https://gitlab.com/sylva-projects/sylva-core/-/issues/659
          var_substitution_enabled: "true"
    repo: metal3
    helm_chart_artifact_name: metal3
    helmrelease_spec:
      timeout: 10m
      chart:
        spec:
          chart: .
      install:
        createNamespace: false
      targetNamespace: metal3-system
      values:  # see https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3/-/blob/main/values.yaml
        global:
          storageClass: '{{ .Values._internal.default_storage_class }}'
        # ironicIPADownloaderBaseURI:
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        images:
          baremetalOperator:
            repository: quay.io/metal3-io/baremetal-operator
            tag: "v0.4.0"
          ironic:
            repository: quay.io/metal3-io/ironic
            tag: "capm3-v1.5.2"
          ironicIPADownloader:
            repository: registry.gitlab.com/sylva-projects/sylva-elements/container-images/ironic-ipa-downloader/ipa
            tag: "0.2.0"
        persistence:
          ironic:
            size: "10Gi"
            accessMode: "ReadWriteOnce"
        services:
          ironic:
            # Specify the IP address used by Ironic service
            ironicIP: '{{ .Values.display_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
        mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'
        podSecurityContext:
          runAsNonRoot: true
          runAsUser: 1000
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
      valuesFrom:
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-root-secret
          targetPath: mariadb.auth.rootPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-mariadb-replication-secret
          targetPath: mariadb.auth.replicationPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: mariadb.auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-secret
          targetPath: auth.ironicPassword
          optional: false
        - kind: Secret
          name: metal3-secrets
          valuesKey: metal3-ironic-inspector-secret
          targetPath: auth.ironicInspectorPassword
          optional: false

  capi-providers-pivot-ready:
    info:
      description: checks if management cluster is ready for pivot
      details: >
        This unit only has dependencies, but does not create resources.
        It is here only to have a single thing to look at to determine
        if everything is ready for pivot (see bootstrap.values.yaml pivot unit)
      internal: true
    unit_template: dummy
    # we copy the dependencies of the 'cluster' unit
    # (with the exception of "capo-cluster-resources" which we don't need, given how what it produces is consumed)
    depends_on: '{{ omit .Values.units.cluster.depends_on "capo-cluster-resources" | include "preserve-type" }}'

  local-path-provisioner:
    info:
      description: installs local-path CSI
      maturity: stable
    repo: local-path-provisioner
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/chart/local-path-provisioner
      targetNamespace: kube-system
      install:
        createNamespace: true
      values:
        storageClass:
          defaultClass: '{{ .Values._internal.default_storage_class | eq "local-path" | include "as-bool" }}'
        helperImage:
          repository: docker.io/library/busybox
          tag: 1.36.1
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: local-path-provisioner
                spec:
                  template:
                    spec:
                      containers:
                      - name: local-path-provisioner
                        securityContext:
                          runAsUser: 1000

  cluster:
    info:
      description: holds the Cluster API definition for the cluster
      maturity: core-component
    repo: sylva-capi-cluster
    helm_chart_artifact_name: sylva-capi-cluster
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      '{{ .Values._internal.metal3_unit }}': '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
      sylva-system/os-image-server: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
    labels:
      suspend-on-pivot: "yes"  # must be suspended before pivot
    kustomization_spec:
      healthChecks:
        # wait for OpenStackCluster when this provider is used, as Cluster will be seen as non-ready when it will be paused (as observedGeneration will differ from generation)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "infrastructure.cluster.x-k8s.io/v1alpha6" "cluster.x-k8s.io/v1beta1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "OpenStackCluster"                         "Cluster" }}'
          name: '{{ .Values.cluster.name }}'
          namespace: '{{ .Release.Namespace }}'
        # wait for the control plane to be ready (the api/version/kind to look for depends on the bootstrap provider in use)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "controlplane.cluster.x-k8s.io/v1beta1" "controlplane.cluster.x-k8s.io/v1alpha1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "KubeadmControlPlane"                   "RKE2ControlPlane" }}'
          name: '{{ .Values.cluster.name }}-control-plane'
          namespace: '{{ .Release.Namespace }}'
        # all the above is subject to a race condition: if Flux checks the status too early
        # it concludes, because CAPI resources aren't fully kstatus compliant, that the resource is ready
        # waiting for the cluster kubeconfig Secret is a workaround
        - apiVersion: v1
          kind: Secret
          name: '{{ .Values.cluster.name }}-kubeconfig'
          namespace: '{{ .Release.Namespace }}'
    helmrelease_spec:
      targetNamespace: '{{ .Release.Namespace }}'
      chart:
        spec:
          chart: .
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: cluster_external_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_fip
          targetPath: cluster_public_ip
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: control_plane_servergroup_id
          targetPath: control_plane.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: worker_servergroup_id
          targetPath: machine_deployment_default.capo.server_group_id
          optional: '{{ not (.Values.cluster.capi_providers.infra_provider | eq "capo") | include "as-bool" }}'
    # we pass everything that is under `cluster` to this unit, a helm release of the sylva-capi-cluster chart
    # (we do it via a secret because some of the values are credentials in many scenarios)
    helm_secret_values: '{{ .Values.cluster | include "preserve-type" }}'

  cluster-garbage-collector:
    info:
      description: installs cronjob responsible for unused CAPI resources cleaning
      internal: true
    depends_on:
      cluster: false # we can't depend directly on 'cluster' unit, since it's being disabled in 'management-sylva-units' and re-enabled by 'pivot'
      capi: true
      capd: '{{ tuple . "capd" | include "unit-enabled" }}'
      capv: '{{ tuple . "capv" | include "unit-enabled" }}'
      capo: '{{ tuple . "capo" | include "unit-enabled" }}'
      capm3: '{{ tuple . "capm3" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-garbage-collector
      wait: true
      _components:
        - '{{ tuple "components/dev-ci-cronjob-schedule" (list "dev" "ci" | has .Values.env_type) | include "set-only-if" }}'

  cluster-reachable:
    info:
      internal: true
      description: ensure that created clusters are reachable, and make failure a bit more explicit if it is not the case
      details: >
        This unit will be enabled in bootstrap cluster to check connectivity to management cluster
        and in various workload-cluster namespaces in management cluster to check connectivity to workload clusters
    unit_template: dummy
    enabled: false
    depends_on:
      cluster: true
    kustomization_spec:
      targetNamespace: default
      kubeConfig:
        secretRef:
          name: '{{ .Values.cluster.name }}-kubeconfig'

  heat-operator:
    info:
      description: installs OpenStack Heat operator
      maturity: core-component
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/heat-operator
      wait: true

  sylva-units-operator:
    info:
      description: installs sylva-units operator
      maturity: experimental
    repo: sylva-core
    depends_on:
      flux-system: true
    kustomization_spec:
      path: ./kustomize-units/sylva-units-operator
      wait: true
      images:
        - name: controller
          newName: registry.gitlab.com/sylva-projects/sylva-elements/sylva-units-operator
          newTag: 0.0.0-pre3

  workload-cluster-operator:
    info:
      description: installs Sylva operator for managing workload clusters
      maturity: experimental
    repo: sylva-core
    depends_on:
      sylva-units-operator: true
    kustomization_spec:
      path: ./kustomize-units/workload-cluster-operator
      wait: true
      images:
        - name: controller
          newName: registry.gitlab.com/sylva-projects/sylva-elements/workload-cluster-operator
          newTag: 0.0.0-pre3

  capo-cluster-resources:
    info:
      description: installs OpenStack Heat stack for CAPO cluster prerequisites
      internal: true
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      wait: true
      targetNamespace: '{{ .Release.Namespace }}'
      postBuild:
        substitute:
          STACK_NAME_PREFIX: '{{ .Values.cluster.name }}-{{ tuple . .Values.cluster.capo.resources_tag | include "interpret-as-string" | replace "." "-" }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.openstack.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.openstack.worker_affinity_policy }}'
          CAPO_EXTERNAL_NETWORK_ID: '{{ tuple .Values.openstack.external_network_id .Values.openstack.external_network_id | include "set-only-if" }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}'
          CAPO_CREATE_IRONIC_SECURITY_GROUP: '{{ tuple . (and (tuple . "metal3" | include "unit-enabled") (.Values.cluster.capi_providers.infra_provider | eq "capo")) "true" "false" | include "interpret-ternary" }}'
          COMMON_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}'
    kustomization_substitute_secrets:
      CAPO_CLOUD_YAML: '{{ .Values.cluster.capo.clouds_yaml  | toYaml | b64enc }}'
      CAPO_CACERT: '{{ (.Values.cluster.capo.cacert|default "") | b64enc }}'


  pause-cluster-reconciliation:
    info:
      description: (specific to CAPO:) makes sure cluster reconciliation is paused during some operation
      details: workaround for https://gitlab.com/sylva-projects/sylva-core/-/issues/309
      internal: true
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # Following condition is required in bootstrap phase, when sylva-units is installed in management-cluster
    # with cluster unit disabled before pivoting, this unit can't depend on a disabled unit in that case
    - '{{ tuple . "cluster" | include "unit-enabled" }}'
    depends_on:
      cluster: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: '{{ .Values.cluster.name }}-pause-cluster'
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
      _patches:
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: '{{ .Values.cluster.name }}-pause-cluster-{{ .Release.Namespace }}-cm'
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash
              set -e
              MD=$(kubectl -n $TARGET_NAMESPACE get MachineDeployment -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }}  -o json | jq '.items|length')
              if [ $MD -gt 0 ];then
                kubectl -n $TARGET_NAMESPACE wait MachineDeployment -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }} --for condition=Ready --timeout 600s
              fi
              kubectl -n $TARGET_NAMESPACE patch cluster.cluster.x-k8s.io  {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": true}}'

  resume-cluster-reconciliation:
    info:
      description: makes sure cluster reconciliation is on
      details: workaround for https://gitlab.com/sylva-projects/sylva-core/-/issues/309
      internal: true
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # should not be enabled on first run when cluster unit is not yet created
    - '{{ not (lookup "kustomize.toolkit.fluxcd.io/v1" "Kustomization" .Release.Namespace "cluster" | empty) }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      postBuild:
        substitute:
          JOB_NAME: '{{ .Values.cluster.name }}-resume-cluster'
          JOB_TARGET_NAMESPACE: '{{ .Release.Namespace }}'
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
      _patches:
      - target:
          kind: ConfigMap
        patch: |
          - op: replace
            path: /metadata/name
            value: '{{ .Values.cluster.name }}-resume-cluster-{{ .Release.Namespace }}-cm'
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash
              kubectl -n $TARGET_NAMESPACE get cluster.cluster.x-k8s.io {{ .Values.cluster.name }} || { echo "Cluster does not exist yet, nothing to do."; exit 0; }
              kubectl -n $TARGET_NAMESPACE patch cluster.cluster.x-k8s.io {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": false}}'

  calico-crd:
    info:
      description: installs Calico CRDs
      maturity: stable
      hidden: true
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-calico-crd
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-calico-crd
          version: "" # will be defined by helm_chart_versions

  tigera-clusterrole:
    info:
      description: is here to allow for upgrading Calico chart when upgrading cluster
      details: |
        For v1.25.x to v1.26.x, see https://gitlab.com/sylva-projects/sylva-core/-/issues/664
      internal: true
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/tigera-clusterrole
      wait: true
      force: true

  calico:
    info:
      description: install Calico CNI
      maturity: stable
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpk" }}'  # installed by RKE2 when RKE2 is used
    depends_on:
      calico-crd: true
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      v3.25.001: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      v3.26.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-calico
          version: "" # will be defined by helm_chart_versions
      values:
        installation:
          calicoNetwork:
            bgp: Enabled
          registry: UseDefault

  metallb:
    info:
      description: installs MetalLB operator
      maturity: stable
    enabled_conditions:
      - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    depends_on:
      calico: true
    helm_repo_url: https://metallb.github.io/metallb
    helmrelease_spec:
      targetNamespace: metallb-system
      chart:
        spec:
          chart: metallb
          version: 0.13.12
      values:
        controller:
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
        speaker:
          frr:
            enabled: false
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane

  cinder-csi:
    info:
      description: installs OpenStack Cinder CSI
      maturity: stable
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.28.1
      targetNamespace: cinder-csi
      install:
        createNamespace: false
      values:
        clusterID: '{{ .Values.cluster.capo.resources_tag }}'
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.openstack.storageClass.name }}"
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.openstack.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
            [Global]
            auth-url = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url | quote }}
            tenant-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name | quote }}
            domain-name = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name | quote }}
            username = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username | quote }}
            password = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password | quote }}
            region = {{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.region_name | quote }}
            tls-insecure = {{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}
            [BlockStorage]
            ignore-volume-az = true
            {{- end -}}

  sylva-ca-certs:
    info:
      description: configures Sylva internal certificate authority
      internal: true
    depends_on:
      namespace-defs: true
      cert-manager: true
      sylva-ca: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca-certs
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.external_hostnames.rancher }}'
          VAULT_DNS: '{{ .Values.external_hostnames.vault }}'
          KEYCLOAK_DNS: '{{ .Values.external_hostnames.keycloak }}'
          FLUX_WEBUI_DNS: '{{ .Values.external_hostnames.flux }}'
      wait: true
      _components:
        - '{{ tuple "components/keycloak-tls" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/rancher-tls" (tuple . "rancher" | include "unit-enabled") | include "set-only-if" }}'
        - '{{ tuple "components/vault-tls" (tuple . "vault" | include "unit-enabled") | include "set-only-if" }}'

  synchronize-secrets:
    info:
      description: allows secrets from Vault to be consumed other units, relies on ExternalSecrets
      internal: true
    depends_on:
      eso-secret-stores: true
      vault-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/synchronize-secrets
      wait: true
      _components:
        - '{{ tuple "components/keycloak" (tuple . "keycloak" | include "unit-enabled") | include "set-only-if" }}'
      postBuild:
        substitute:
          FLUX_ADMIN_USERNAME: '{{ .Values.flux_webui.admin_user }}'

  rancher:
    info:
      description: installs Rancher
      maturity: stable
    depends_on:
      cert-manager: true
      k8s-gateway: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      synchronize-secrets: true
    annotations:
      sylvactl/readyMessage: "Rancher UI can be reached at https://{{ .Values.external_hostnames.rancher }} ({{ .Values.external_hostnames.rancher }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://releases.rancher.com/server-charts/latest
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: 2.7.9
      targetNamespace: cattle-system
      interval: 10m0s
      upgrade:
        remediation:
          retries: 3
      values:
        privateCA: true
        useBundledSystemChart: true
        hostname: '{{ .Values.external_hostnames.rancher }}'
        ingress:
          enabled: true
          ingressClassName: nginx
          tls:
            source: secret
            secretName: rancher-tls
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        postDelete:
          namespaceList:
            - cattle-fleet-system
            - rancher-operator-system
        extraEnv:
          - name: CATTLE_BOOTSTRAP_PASSWORD
            valueFrom:
              secretKeyRef:
                name: bootstrap-secret
                key: bootstrapPassword
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: rancher
                spec:
                  template:
                    spec:
                      volumes:
                        - name: tls-ca-volume
                          secret:
                            defaultMode: 256
                            secretName: rancher-tls
                            items:
                              - key: ca.crt
                                path: cacerts.pem
                      # this is to avoid that the too-short default liveness probe
                      # prevents the Rancher installation from finishing before the pod is killed:
                      containers:
                        - name: rancher
                          livenessProbe:
                            initialDelaySeconds: 120
                            periodSeconds: 30
                            failureThreshold: 20

    kustomization_spec:
      # these healthChecks are added so that does not become ready before
      # a few things that Rancher sets up behind the scene are ready
      healthChecks:
        - apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          name: clusters.provisioning.cattle.io  # this is because capi-rancher-import needs this
        - apiVersion: apps/v1
          kind: Deployment
          name: rancher-webhook
          namespace: cattle-system
        - apiVersion: v1
          kind: Service
          name: rancher-webhook
          namespace: cattle-system

  rancher-keycloak-oidc-provider:
    info:
      description: configures Rancher for Keycloak OIDC integration
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
      keycloak: true
      keycloak-resources: true
      keycloak-oidc-external-secrets: true
    kustomization_spec:
      path: ./kustomize-units/rancher-keycloak-oidc-provider
      postBuild:
        substitute:
          KEYCLOAK_EXTERNAL_URL: '{{ .Values.external_hostnames.keycloak }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
      wait: true

  k8s-gateway:
    info:
      description: installs k8s gateway (coredns + plugin to resolve external service names to ingress IPs)
      details: >
        is here only to allow for DNS resolution of Ingress hosts (FQDNs), used for importing workload clusters into Rancher and for flux-webui to use Keycloak SSO
      maturity: stable
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: 2.1.0
      targetNamespace: k8s-gateway
      install:
        createNamespace: true
      values:
        domain: '{{ .Values.cluster_external_domain }}'
        replicaCount: 3
        service:
          loadBalancerIP: '{{ .Values.cluster_external_ip }}'
          annotations:
            metallb.universe.tf/allow-shared-ip: cluster-external-ip
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: k8s-gateway
                spec:
                  template:
                    spec:
                      containers:
                      - name: k8s-gateway
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  capd-metallb-config:
    info:
      description: configures MetalLB in a capd context
      internal: true
    enabled_conditions:
    - '{{ tuple . "metallb" | include "unit-enabled" }}'
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      metallb: true
    kustomization_spec:
      path: ./kustomize-units/metallb-config
      wait: true
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster_external_ip }}'

  capi-rancher-import:
    info:
      description: installs the capi-rancher-import operator, which let's us import Cluster AIP workload clusters in management cluster's Rancher
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: capi-rancher-import
    depends_on:
      rancher: true
      k8s-gateway: true
    helmrelease_spec:
      chart:
        spec:
          chart: charts/capi-rancher-import
      targetNamespace: capi-rancher-import
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          privileged: false
          runAsNonRoot: true
          runAsGroup: 1
          runAsUser: 1
          seccompProfile:
            type: RuntimeDefault
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.external_hostnames.rancher }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: capi-rancher-import
            namespace: sylva-system
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    info:
      description: installs Nginx ingress controller
      maturity: stable
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helm_chart_versions:
      4.5.202: '{{ not .Values._internal.k8s_post_1_26_6  }}'
      4.6.101: '{{ .Values._internal.k8s_post_1_26_6 }}'
    helmrelease_spec:
      releaseName: rke2-ingress-nginx
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-ingress-nginx
          version: "" # will be defined by helm_chart_versions
      values:
        fullnameOverride: rke2-ingress-nginx
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            externalIPs:
            - '{{ .Values.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: cluster-external-ip
          publishService:
            enabled: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: rke2-ingress-nginx-controller
          namespace: kube-system

  first-login-rancher:
    info:
      description: configure Rancher authentication for admin
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/first-login-rancher
      postBuild:
        substitute:
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
          CURRENT_TIME: '{{ now | date "2006-01-02T15:04:05.999Z" }}'
      wait: true

  flux-webui:
    info:
      description: installs Weave GitOps Flux web GUI
      maturity: stable
    depends_on:
      flux-system: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      coredns: '{{ tuple . "keycloak" | include "unit-enabled" }}' # see https://gitlab.com/sylva-projects/sylva-core/-/merge_requests/1023#note_1694289969
      keycloak-add-client-scope: '{{ tuple . "keycloak" | include "unit-enabled" }}'
      keycloak-oidc-external-secrets: '{{ tuple . "keycloak" | include "unit-enabled" }}'
    annotations:
      sylvactl/readyMessage: "Flux Web UI can be reached at https://{{ .Values.external_hostnames.flux }} ({{ .Values.external_hostnames.flux }} must resolve to {{ .Values.display_external_ip }})"
    repo: weave-gitops
    helm_chart_artifact_name: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        - name: WEAVE_GITOPS_FEATURE_OIDC_BUTTON_LABEL
          value: "Log in with Keycloak"
        installCRDs: true
        adminUser:
          create: true
          username: '{{ .Values.flux_webui.admin_user }}'
          createSecret: false
        rbac:
          impersonationResourceNames: ["admin", "sylva-admin@example.com"] # the Keycloak username set in unit keycloak-resources; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "watch" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: '{{ .Values.external_hostnames.flux }}'
              paths:
                - path: /   # setting this to another value like '/flux-webui' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
          tls:
            - secretName: flux-webui-tls
              hosts:
                - '{{ .Values.external_hostnames.flux }}'
        extraVolumes:
          - name: custom-ca-cert
            secret:
              secretName: flux-webui-tls
              items:
                - key: ca.crt
                  path: ca.crt
        extraVolumeMounts:
          - name: custom-ca-cert
            mountPath: /etc/ssl/certs
            readOnly: true
        oidcSecret:
          create: false
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: ClusterRoleBinding
                apiVersion: rbac.authorization.k8s.io/v1
                metadata:
                  name: '{{ .Values.flux_webui.admin_user }}-user-read-resources-cr'
                subjects:
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: '{{ .Values.flux_webui.admin_user }}'
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: sylva-admin@example.com  # add same RBAC for the SSO user, so that when flux-webui SA impersonates it has privileges; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: flux-webui-weave-gitops
                spec:
                  template:
                    spec:
                      containers:
                      - name: weave-gitops
                        securityContext:
                          allowPrivilegeEscalation: false
                          capabilities:
                            drop:
                            - ALL
                          privileged: false
                          readOnlyRootFilesystem: true
                          runAsNonRoot: true
                          runAsGroup: 1000
                          runAsUser: 1000
                          seccompProfile:
                            type: RuntimeDefault

  monitoring-crd:
    info:
      description: installs monitoring stack CRDs
      maturity: stable
      hidden: true
    enabled_conditions:
    - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true

  monitoring:
    info:
      description: installs monitoring stack
      maturity: stable
    depends_on:
      monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      storageNamespace: cattle-monitoring-system # see https://gitlab.com/sylva-projects/sylva-core/-/issues/443
      chart:
        spec:
          chart: rancher-monitoring
          version: 102.0.2+up40.1.2
      install:
        createNamespace: true
      # Disable drift detection for rancher-monitoring webhooks
      _postRenderers:
        - kustomize:
            patches:
            - target:
                kind: (ValidatingWebhookConfiguration|MutatingWebhookConfiguration)
              patch: |
                - op: add
                  path: /metadata/annotations/helm.toolkit.fluxcd.io~1driftDetection
                  value: disabled

  multus:
    info:
      description: installs Multus
      maturity: stable
    enabled: false
    depends_on:
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io/
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-multus
          version: v3.9.3-build2023010901
      targetNamespace: kube-system
      install:
        createNamespace: false
      values:
        rke2-whereabouts:
          enabled: true
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: DaemonSet
                apiVersion: apps/v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts
              - kind: ServiceAccount
                apiVersion: v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts

  multus-ready:
    info:
      description: checks that Multus is ready
      details: >
        This unit only has dependencies, it does not create resources.
        It performs healthchecks outside of the multus unit,
        in order to properly target workload cluster when we deploy multus in it.
      internal: true
    unit_template: dummy
    enabled_conditions:
      - '{{ tuple . "multus" | include "unit-enabled" }}'
    depends_on:
      multus: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-ds
          namespace: kube-system
        - apiVersion: apps/v1
          kind: DaemonSet
          name: multus-rke2-whereabouts
          namespace: kube-system

  sriov-crd:
    info:
      description: installs SRIOV CRDs
      maturity: stable
      hidden: true
    enabled_conditions:
    - '{{ tuple . "sriov" | include "unit-enabled" }}'
    depends_on:
      multus-ready: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov-crd
      targetNamespace: cattle-sriov-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: sriov-crd
          version: 102.1.0+up0.1.0

  sriov:
    info:
      description: installs SRIOV operator
      maturity: stable
    enabled: false
    depends_on:
      sriov-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov
      targetNamespace: cattle-sriov-system
      chart:
        spec:
          chart: sriov
          version: 102.1.0+up0.1.0
      values:
        cert_manager: true
        rancher-nfd:
          image:
            tag: v0.13.2-build20230605
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: sriov
                  namespace: cattle-sriov-system
                spec:
                  template:
                    spec:
                      securityContext:
                        runAsGroup: 3000
                        runAsUser: 2000

  sriov-resources:
    info:
      description: configures SRIOV resources
      internal: true
    enabled: false
    depends_on:
      sriov: true
    repo: sriov-resources
    helm_chart_artifact_name: sriov-resources
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: cattle-sriov-system
      values:
        node_policies: '{{ .Values.sriov.node_policies | include "preserve-type" }}'

  coredns:
    info:
      description: configures DNS inside cluster
      internal: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/coredns
      wait: true
      _patches:
        - target:
            kind: ConfigMap
            name: coredns
          patch: |
            - op: replace
              path: /metadata/name
              value: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpk") | ternary "coredns" "rke2-coredns-rke2-coredns" }}'
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster_external_ip }}'
          CLUSTER_EXTERNAL_DOMAIN: '{{ .Values.cluster_external_domain }}'

  longhorn-crd:
    info:
      description: installs Longhorn CRDs
      maturity: stable
      hidden: true
    enabled_conditions:
    - '{{ tuple . "longhorn" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn-crd
      targetNamespace: longhorn-system
      chart:
        spec:
          chart: longhorn-crd
          version: 102.2.3+up1.4.4

  longhorn:
    info:
      description: installs Longhorn CSI
      maturity: stable
    enabled: false
    depends_on:
      longhorn-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn
      targetNamespace: longhorn-system
      install:
        createNamespace: false
      chart:
        spec:
          chart: longhorn
          version: 102.2.3+up1.4.4
      values:
        defaultSettings:
          createDefaultDiskLabeledNodes: true
          defaultDataPath: /var/lib/longhorn
          autoCleanupSystemGeneratedSnapshot: true
          allowVolumeCreationWithDegradedAvailability: false
          storageMinimalAvailablePercentage: 10

  cluster-creator-login:
    info:
      description: configures Rancher account used for workload cluster imports
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      rancher: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-login
      wait: false
      force: true
      postBuild:
        substitute:
          JOB_NAME: cluster-creator-login
          JOB_TARGET_NAMESPACE: flux-system
          JOB_CHECKSUM: '{{ .Values | toJson | sha256sum }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.external_hostnames.rancher }}'
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: cluster-creator-kubeconfig
          namespace: flux-system
        - apiVersion: batch/v1
          kind: Job
          name: cluster-creator-login-flux-system
          namespace: kube-job

  cluster-creator-policy:
    info:
      description: Kyverno policy for cluster creator
      details: |
        This units defines a Kyverno policy to distribute the Kubeconfig of cluster creator
        in all workload cluster namespaces, to allow the import of workload clusters in
        Rancher.
      internal: true
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    depends_on:
      cluster-creator-login: true
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cluster-creator-policy
      wait: true
      force: true

  os-image-server:
    info:
      description: >
        Deploys a web server on management cluster
        which serves OS images for baremetal clusters.
      maturity: stable
    enabled_conditions:
      - '{{ tuple . .Values._internal.metal3_unit | include "unit-enabled" }}'
    depends_on:
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: os-image-server
    annotations:
      sylvactl/readyMessage: |
        OS images are served at:
        {{- $osImageFqdn := (coalesce .Values.external_hostnames.os_image_server .Values.display_external_ip) -}}
        {{- range $imageName, $imageParams := (coalesce .Values.os_images .Values._internal.default_os_images) }}
          * https://{{ $osImageFqdn }}/{{ $imageParams.filename }}(.sha256sum)
        {{- end }}
        {{- if not (eq $osImageFqdn .Values.display_external_ip)}}
        ({{ .Values.external_hostnames.os_image_server }} must resolve to {{ .Values.display_external_ip }})
        {{- end }}
    helmrelease_spec:
      chart:
        spec:
          chart: ./charts/os-image-server
      targetNamespace: os-images
      install:
        createNamespace: true
      timeout: 168h  # leave plenty of time to download OS images in initContainers
      values:
        downloader:
          proxy: '{{ get .Values.proxies "https_proxy" }}'
          no_proxy: '{{ include "sylva-units.no_proxy" . }}'
          extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        nginx:
          securityContext:
            capabilities:
              drop:
              - ALL
            runAsNonRoot: true
            runAsUser: 10000
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
        ingress:
          className: nginx
          hosts:
            - host: '{{ .Values.external_hostnames.os_image_server }}'
        osImages: '{{ coalesce .Values.os_images .Values._internal.default_os_images | include "preserve-type" }}'
        osImagePersistenceDefaults:
          enabled: true
          size: 8Gi
          storageClass: '{{ .Values._internal.default_storage_class }}'

  capo-contrail-bgpaas:
    info:
      description: installs CAPO Contrail BGPaaS controller
      maturity: stable
    enabled: false
    repo: capo-contrail-bgpaas
    helm_chart_artifact_name: capo-contrail-bgpaas
    depends_on:
      heat-operator: '{{ tuple . "heat-operator" | include "unit-enabled" }}'
      capo: '{{ tuple . "capo" | include "unit-enabled" }}'
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: capo-contrail-bgpaas-system
      install:
        createNamespace: true
      values:
        conf:
          env:
            DEFAULT_PORT: '0'
            DEFAULT_ASN: '64512'

  vsphere-cpi:
    info:
      description: configures Vsphere Cloud controller manager
      internal: true
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: cloud-provider-vsphere
    helmrelease_spec:
      chart:
        spec:
          chart: charts/vsphere-cpi
      install:
        remediation:
          retries: -1
      releaseName: vsphere-cpi
      storageNamespace: kube-system
      targetNamespace: kube-system
      values:
        config:
          enabled: true
          vcenter: '{{ .Values.cluster.capv.server }}'
          datacenter: '{{ .Values.cluster.capv.dataCenter }}'
          tlsTumbprint: '{{ .Values.cluster.capv.tlsThumbprint }}'
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: ConfigMap
                apiVersion: v1
                metadata:
                  name: vsphere-cloud-config
                  namespace: kube-system
                data:
                  vsphere.conf: '{{ index (index .Values.vsphere "vsphere-cpi") "vsphere_conf" | toYaml }}'

    helm_secret_values:
      config:
        username: '{{ .Values.cluster.capv.username }}'
        password: '{{ .Values.cluster.capv.password }}'

  vsphere-csi-driver:
    info:
      description: installs Vsphere CSI
      maturity: stable
    enabled_conditions:
      - '{{ eq .Values.cluster.capi_providers.infra_provider "capv" }}'
    repo: sylva-core
    kustomization_spec:
      path: kustomize-units/vsphere-csi-driver
      targetNamespace: vmware-system-csi
      wait: true
      postBuild:
        substitute:
          SERVER: '{{ .Values.cluster.capv.server }}'
          DATACENTER: '{{ .Values.cluster.capv.dataCenter }}'
          CLUSTER_ID: '{{ printf "%s-%s" .Values.cluster.name (randAlphaNum 10) | trunc 64 }}'
          STORAGE_POLICY_NAME: '{{ .Values.cluster.capv.storagePolicyName | default "" }}'
          CONTROLLER_REPLICAS: '{{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}'
      _patches:
        - patch: |
            - op: replace
              path: /spec/template/spec/nodeSelector/node-role.kubernetes.io~1control-plane
              value: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "true" "" }}'
          target:
            group: apps
            version: v1
            kind: Deployment
            name: vsphere-csi-controller
            namespace: vmware-system-csi
    kustomization_substitute_secrets:
      USERNAME: '{{ .Values.cluster.capv.username }}'
      PASSWORD: '{{ .Values.cluster.capv.password }}'

  sandbox-privileged-namespace:
    info:
      description: >
        creates the sandbox namespace used
        to perform privileged operations like debugging a node
      internal: true
    enabled: false
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sandbox-privileged-namespace
      wait: true
      prune: true

  # Gitea-unit
  gitea-secrets:
    info:
      description: >
        create random secret that will be used by gitea application.
        secrets are sync with vault.
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/gitea/secrets
      wait: true

  gitea-eso:
    info:
      description: >
        write secrets in gitea namespace in gitea expected format
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      sylva-ca-certs: true
      eso-secret-stores: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/gitea/eso
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: gitea-keycloak-oidc-auth
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-admin
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-postgres-secrets
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: gitea-redis
          namespace: gitea
        - apiVersion: v1
          kind: Secret
          name: sylva-ca.crt
          namespace: gitea

  gitea-keycloak-resources:
    info:
      description: >
        deploys Gitea OIDC client in Sylva's Keycloak realm
      internal: true
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/gitea/keycloak-resources
      targetNamespace: keycloak
      postBuild:
        substitute:
          GITEA_DNS: '{{ .Values.external_hostnames.gitea }}'
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-gitea-client  # this secret is a byproduct of the gitea-client KeycloakClient resource
          namespace: keycloak

  gitea-redis:
    info:
      description: installs Redis cluster for Gitea
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: redis-cluster
          version: 9.1.1
      targetNamespace: gitea
      releaseName: gitea-redis
      values:
        usePassword: true
        existingSecret: gitea-redis
        existingSecretPasswordKey: password
        global:
          storageClass: "{{ .Values._internal.default_storage_class }}"
        persistence:
          size: 8Gi

  gitea-postgresql-ha:
    info:
      description: installs PostgreSQL HA cluster for Gitea
      maturity: stable
    enabled_conditions:
    - '{{ tuple . "gitea" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql-ha
          version: 11.9.4
      targetNamespace: gitea
      releaseName: gitea-postgres
      values:
        global:
          storageClass: "{{ .Values._internal.default_storage_class }}"
        postgresql:
          username: gitea
          database: gitea
          existingSecret: gitea-postgres-secrets
        pgpool:
          existingSecret: gitea-postgres-secrets
        persistence:
          size: 8Gi

  gitea:
    info:
      description: installs Gitea
      maturity: stable
    enabled: false
    depends_on:
      cert-manager: true
      gitea-eso: true
      gitea-keycloak-resources: true
      gitea-secrets: true
      gitea-redis: true
      gitea-postgresql-ha: true
      namespace-defs: true
    annotations:
      sylvactl/readyMessage: "Gitea can be reached at https://{{ .Values.external_hostnames.gitea }} ({{ .Values.external_hostnames.gitea }} must resolve to {{ .Values.display_external_ip }})"
    helm_repo_url: https://dl.gitea.com/charts/
    helmrelease_spec:
      chart:
        spec:
          chart: gitea
          version: 9.5.1
      targetNamespace: gitea
      releaseName: gitea
      values:
        containerSecurityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
          seccompProfile:
            type: RuntimeDefault
        redis-cluster:
          enabled: false
        postgresql:
          enabled: false
        postgresql-ha:
          enabled: false
        persistence:
          enabled: true
          size: 10Gi
          storageClass: "{{ .Values._internal.default_storage_class }}"
          accessModes:
            - >-
                {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
                  ReadWriteMany
                {{- else -}}
                  ReadWriteOnce
                {{- end -}}
        replicaCount: >-
          {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
          {{ 3 | include "preserve-type" }}
          {{- else -}}
          {{ 1 | include "preserve-type" }}
          {{- end -}}
        strategy:
          type: >-
            {{- if eq .Values._internal.default_storage_class_RWX_support "true" -}}
              RollingUpdate
            {{- else -}}
              Recreate
            {{- end -}}
        gitea:
          admin:
            existingSecret: gitea-admin
          metrics:
            enabled: true
            serviceMonitor:
              enabled: true
          oauth:
            - name: "keycloack-sylva"
              provider: "openidConnect"
              existingSecret: gitea-keycloak-oidc-auth
              autoDiscoverUrl: 'https://{{ .Values.external_hostnames.keycloak }}/realms/sylva/.well-known/openid-configuration'

          config:
            cron:
              ENABLED: false
            cron.GIT_GC_REPOS:
              ENABLED: false
            server:
              ENABLE_PPROF: true
            database:
              DB_TYPE: postgres
              HOST: gitea-postgres-postgresql-ha-pgpool.gitea.svc.cluster.local:5432
              NAME: gitea
              USER: gitea
              # define by env variable: PASSWD
              SCHEMA: public
            session:
              PROVIDER: redis
              # define by env variable: PROVIDER_CONFIG
            cache:
              ADAPTER: redis
              # define by env variable: HOST
            queue:
              TYPE: redis
              # define by env variable: CONN_STR
            indexer:
              REPO_INDEXER_ENABLED: false
              ISSUE_INDEXER_ENABLED: false

          additionalConfigFromEnvs:
            - name: GITEA__DATABASE__PASSWD # define DB password
              valueFrom:
                secretKeyRef:
                  key: password
                  name: gitea-postgres-secrets
            - name: GITEA__QUEUE__CONN_STR # redis connection string for queue
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
            - name: GITEA__SESSION__PROVIDER_CONFIG # redis connection string for session
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
            - name: GITEA__CACHE__HOST # redis connection string for queue
              valueFrom:
                secretKeyRef:
                  key: connection_string
                  name: gitea-redis
        ingress:
          enabled: true
          className: nginx
          annotations:
            cert-manager.io/cluster-issuer: ca-issuer
            nginx.ingress.kubernetes.io/proxy-body-size: 8m
          tls:
          - hosts:
              - '{{ .Values.external_hostnames.gitea }}'
            secretName: gitea-tls
          hosts:
            - host: '{{ .Values.external_hostnames.gitea }}'
              paths:
              - path: /
                pathType: Prefix
        extraVolumes:
          - secret:
              defaultMode: 420
              secretName: sylva-ca.crt
            name: sylva-ca
        extraVolumeMounts:
          - mountPath: /etc/ssl/certs/sylva-ca.crt
            name: sylva-ca
            readOnly: true
            subPath: ca.crt


##### stuff related to the 'cluster' unit #####
#
# all these values under 'cluster' are passed as values to sylva-capi-cluster chart

cluster:
  name: management-cluster

  # can be set to true to do an RKE2 deployment disconnected from the Internet:
  air_gapped: false

  # cis profile to be used. Curently supported only for rke2 clusters. "cis-1.6" for k8s prior to 1.25, "cis-1.23" for 1.25+
  cis_profile: cis-1.23

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capd      # capd, capo, capm3 or capv
    bootstrap_provider: cabpk # cabpr (RKE2) or cabpk (kubeadm)

  # kubernetes version to be used
  k8s_version: '{{ .Values._internal.k8s_version_map | dig .Values.k8s_version_short "" | required (printf "no k8s version defined for %s" .Values.k8s_version_short) }}'

  # kube_vip version to be used for kubeadm deployments
  images:
    kube_vip:
      repository: ghcr.io/kube-vip/kube-vip
      tag: "v0.6.4"

  # Nodes number for control-plane
  control_plane_replicas: 3

  capo:
    # flavor_name: m1.large # Openstack flavor name
    # image_name: # OpenStack image name
    # ssh_key_name: # OpenStack VM SSH key
    # network_id: # OpenStack network used for nodes and VIP port
    # rootVolume: {} # Let this parameter empty if you don't intent to use root volume
    #   # otherwise, provide following values
    #   diskSize: 20 # Size of the VMs root disk
    #   volumeType: '__DEFAULT__' # Type of volume to be created
    # #control_plane_az: # list of OpenStack availability zones to deploy control planes nodes to, otherwise all would be candidates
    # clouds_yaml: # (this is a dict, not a YAML string)
    #   clouds:
    #     capo_cloud:
    #       auth:
    #         auth_url: # replace me
    #         user_domain_name: # replace me
    #         project_domain_name: # replace me
    #         project_name: # replace me
    #         username: # replace me
    #         password: # replace me
    #       region_name: # replace me
    #       verify: # e.g. false
    # cacert: # cert used to validate CA of OpenStack APIs

    # tag set for OpenStack resources in management cluster:
    resources_tag: >-
      {{- if .Values.cluster.capi_providers.infra_provider | eq "capo" -}}
        sylva-{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}
      {{- end -}}

  control_plane:
    capo:
      security_group_names:
      - default
      - capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}
      - capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}

  machine_deployment_default:
    capo:
      security_group_names:
      - default
      - capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}
      - capo-{{ .Values.cluster.name }}-security-group-common-{{ .Values.cluster.capo.resources_tag }}

  cluster_external_ip: '{{ .Values.cluster_external_ip }}'

  cluster_public_ip: '{{ tuple (.Values.openstack.floating_ip) (not (eq .Values.openstack.floating_ip "")) | include "set-only-if" }}'
  cluster_services_cidrs:
    - 10.43.0.0/16
  cluster_pods_cidrs:
    - 10.42.0.0/16

  dns_resolver: false
  ntp: '{{ .Values.ntp | include "preserve-type" }}'
  proxies:
    http_proxy: '{{ .Values.proxies.http_proxy }}'
    https_proxy: '{{ .Values.proxies.https_proxy }}'
    no_proxy: '{{ include "sylva-units.no_proxy" . }}'

  registry_mirrors: '{{ .Values.registry_mirrors | include "preserve-type" }}'

  metallb_helm_oci_url: ""  # for an OCI-based deployment this is overridden in use-oci-artifacts.values.yaml
  metallb_helm_version: '{{ .Values.units.metallb.helmrelease_spec.chart.spec.version }}'
  metallb_helm_extra_ca_certs: '{{ .Values.oci_registry_extra_ca_certs | include "set-if-defined" }}'

  capv:
    # image_name: # vSphere image name
    # username: ""
    # password: ""
    # dataCenter: ""
    # server: ""
    # dataStore: ""
    # tlsThumbprint: ""
    # folder: ""
    # resourcePool: ""
    # storagePolicyName: ""
    # networks:
    #   default:
    #     networkName: ""
    # ssh_key: ''
    numCPUs: 4

  capm3:
    nodeReuse: '{{ not (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | include "as-bool" }}'  # until https://gitlab.com/sylva-projects/sylva-core/-/issues/705 is solved
    machine_image_checksum_type: sha256

  enable_longhorn: '{{ or (tuple . "longhorn" | include "unit-enabled") (tuple . .Values.units.longhorn.enabled | include "interpret-for-test") | include "as-bool" }}'

capd_docker_host: unix:///var/run/docker.sock

openstack:
  #external_network_id # Can be provided if a FIP is needed in order to reach the management cluster VIP
  floating_ip: ""  # will typically be set by capo-cluster-resources
  storageClass:
    name: cinder-csi  # name of the storageClass to be created
    #type: xxx  # please provide the cinder volume type, e.g. 'ceph_sas' (must exist in OpenStack)
  control_plane_affinity_policy: soft-anti-affinity
  worker_affinity_policy: soft-anti-affinity

metal3:
  provider: sylva # sylva or suse
  # external_bootstrap_ip:
  # bootstrap_ip:

vsphere:
  vsphere-cpi:
    vsphere_conf:
      # Global properties in this section will be used for all specified vCenters unless overriden in VirtualCenter section.
      global:
        port: 443
        # set insecure-flag to true if the vCenter uses a self-signed cert
        insecureFlag: true
        # settings for using k8s secret
        secretName: vsphere-cloud-secret
        secretNamespace: kube-system

      # vcenter section
      vcenter:
        '{{ .Values.cluster.capv.server }}':
          server: '{{ .Values.cluster.capv.server }}'
          user: '{{ .Values.cluster.capv.username }}'
          password: '{{ .Values.cluster.capv.password }}'
          datacenters:
            - '{{ .Values.cluster.capv.dataCenter }}'

cluster_external_ip: 55.55.55.55

# Admin password that will be configured by default on various units  # FIXME, only used for SSO today see https://gitlab.com/sylva-projects/sylva-core/-/issues/503
admin_password: '{{ .Values._internal.default_password }}'

flux_webui:
  admin_user: admin

display_external_ip: '{{ .Values.openstack.floating_ip | eq "" | ternary .Values.cluster_external_ip .Values.openstack.floating_ip }}'

cluster_external_domain: sylva

external_hostnames:
  rancher: 'rancher.{{ .Values.cluster_external_domain }}'
  vault: 'vault.{{ .Values.cluster_external_domain }}'
  keycloak: 'keycloak.{{ .Values.cluster_external_domain }}'
  flux: 'flux.{{ .Values.cluster_external_domain }}'
  neuvector: 'neuvector.{{ .Values.cluster_external_domain }}'
  harbor: 'harbor.{{ .Values.cluster_external_domain }}'
  os_image_server: ''
  gitea: 'gitea.{{ .Values.cluster_external_domain }}'

keycloak: {}

# cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened
cis_benchmark_scan_profile: '{{ eq .Values.cluster.capi_providers.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

# osImages images that should be served and from where they should be downloaded
# if empty default value are used (default_os_images)
os_images: {}

# to configure the SR-IOV VFs on the supported NICs of cluster nodes
sriov:
  node_policies: {}
#   mypolicy1:
#     nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
#     resourceName: ""
#     numVfs: ""
#     deviceType: ""  # supported values: "netdevice" or "vfio-pci"
#     nicSelector:
#       deviceID: ""
#       vendor: ""
#       pfNames: []
#       rootDevices: []

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""

# disable default values for no_proxy (localhost,.svc,.cluster.local.,.cluster.local,.sylva)
# Ex: localhost: false
no_proxy_additional: {}

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
registry_mirrors:
  default_settings:                          # <<< These settings will apply to all configured mirrors
    capabilities: ["pull", "resolve"]
#   skip_verify: true
#   override_path: true
# hosts_config:
#   docker.io:
#   - mirror_url: http://your.mirror/docker
#     registry_settings:                     # <<< Host settings can be used to override default_settings
#       skip_verify: false
#   registry.k8s.io:
#   - mirror_url: ...
#   _default:
#   - mirror_url: ...

# deploy emulated baremetal nodes in bootstrap cluster
libvirt_metal:
  image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/libvirt-metal:{{ .Values.source_templates | dig "libvirt-metal" "spec" "ref" "tag" "_undefined_"}}
  nodes: {}
  #management-cp:
  #  memGB: 12
  #  numCPUs: 6
  #
  #workload-cp:
  #  memGB: 4
  #  numCPUs: 2
  #
  #workload-md:
  #  memGB: 2
  #  numCPUs: 2


# set the type of environment between 3 possible values: dev, ci and prod
env_type: prod

# set NTP servers by IP or FQDN and enable their usage for control plane nodes
ntp:
  enabled: false
  servers:
    - 1.2.3.4
    - europe.pool.ntp.org

# These two sylva_core_oci_registry/sylva_base_oci_registry values govern which OCI repos are used
#
# This matters for:
# 1) OCI deployments
# 2) for non-OCI deployments for retrieving artifacts such as metal3 OS images
#
# For (1) sylva_base_oci_registry is automatically derived from the OCI repo used for sylva-unit HelmRelease.
# For (2) sylva_base_oci_registry can be customized, to use an OCI registry other than registry.gitlab.com/sylva-projects
#
# It should in general not be any need to override sylva_core_oci_registry, which is derived
# from sylva_base_oci_registry.
#
# sylva_base_oci_registry defaults to oci://registry.gitlab.com/sylva-projects
sylva_base_oci_registry:
  '{{
  regexReplaceAll
    "/sylva-core/?$"
    (lookup "source.toolkit.fluxcd.io/v1beta2" "HelmRepository" "sylva-system" "sylva-core" | dig "spec" "url" "")
    ""
  | default "oci://registry.gitlab.com/sylva-projects"
  }}'
sylva_core_oci_registry: '{{ .Values.sylva_base_oci_registry }}/sylva-core'

k8s_version_short: "1.26"

_internal:
  # unless the default password has already been generated by a previous run, we generate a new one randomly
  default_password: '{{ lookup "v1" "Secret" "sylva-system" "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "default_password" (randAlphaNum 64) }}'

  default_storage_class: >-
    {{- if (tuple . "cinder-csi" | include "unit-enabled") -}}
      {{ .Values.openstack.storageClass.name }}
    {{- else if (tuple . "vsphere-csi-driver" | include "unit-enabled") -}}
      vsphere-csi
    {{- else if (tuple . "longhorn" | include "unit-enabled") -}}
      longhorn
    {{- else -}}
      local-path
    {{- end -}}

  # maps storage class names to the name of the unit that implements it
  # (a storage class does not need to appear in this map if the required unit has the same name)
  storage_class_unit_map:
    local-path: local-path-provisioner
    longhorn: longhorn
    vsphere-csi: vsphere-csi-driver
    '{{ .Values.openstack.storageClass.name }}': cinder-csi

  default_storage_class_unit: >-
    {{- $default_storage_class := tuple . .Values._internal.default_storage_class | include "interpret-as-string" -}}
    {{- index (tuple . .Values._internal.storage_class_unit_map | include "interpret-inner-gotpl" | fromJson) "result" | dig $default_storage_class (printf "storage class unit not found for %s" $default_storage_class) -}}

  storage_class_RWX_support:
    - longhorn
  default_storage_class_RWX_support: >-
    {{- $default_storage_class := tuple . .Values._internal.default_storage_class | include "interpret-as-string" -}}
    {{- has $default_storage_class .Values._internal.storage_class_RWX_support -}}

  vault_replicas: >-
    {{ or (.Values.cluster.capi_providers.infra_provider | eq "capd") (and (lt (int .Values.cluster.control_plane_replicas) 3) (eq .Values._internal.default_storage_class_unit "local-path")) | ternary 1 3  }}

  vault_affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: vault
              vault_cr: vault
          topologyKey: kubernetes.io/hostname

  vault_no_affinity: {}

  # this is used in source_templates.sylva-core below
  # and in any unit using sylva-units Helm chart
  sylva_core_version: ''

  sylva_core_existing_source: '{{ .Values.unit_helmrelease_kustomization_spec_default.sourceRef }}'

  # We need to retrieve bootstrap_node_ip in management-cluster while using libvirt-metal emulation, it is stored in a ConfigMap by libvirt-metal unit in that case
  bootstrap_node_ip: '{{ lookup "v1" "ConfigMap" "sylva-system" "cluster-public-endpoint" | dig "data" "address" "not_found" | default "not_found" }}'

  # These values are used only if `os_images` is empty
  default_os_images:
    ubuntu-jammy-plain-rke2-1-26-9:
      uri: "{{ .Values.sylva_base_oci_registry }}/sylva-elements/diskimage-builder/ubuntu-jammy-plain-rke2-1.26.9:0.1.2"
      filename: ubuntu-jammy-plain-rke2-1.26.9.raw
      checksum: 800ca32a2bf2cceba01723a165ea6218f22f45a7557a51ae46b225f804aa3d97

  metal3_unit: '{{ .Values.metal3.provider | eq "suse" | ternary "metal3-suse" "metal3" }}'

  k8s_post_1_26_6: '{{ semverCompare ">1.26.6" (tuple . .Values.cluster.k8s_version | include "interpret-as-string") | include "preserve-type" }}'

  k8s_version_map:
    1.26: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.26.9+rke2r1" "v1.26.9" }}'


shared_workload_clusters_values:
  ntp: '{{ .Values.ntp | include "set-if-defined" }}'
  proxies: '{{ .Values.proxies | include "set-if-defined" }}'
  registry_mirrors: '{{ .Values.registry_mirrors | include "set-if-defined" }}'

  sylva_base_oci_registry: '{{ .Values.sylva_base_oci_registry }}'

  cluster:
    air_gapped: '{{ .Values.air_gapped | include "set-if-defined" }}'
    capi_providers: '{{ .Values.cluster.capi_providers | include "preserve-type" }}'
    mgmt_cluster_external_ip: '{{ .Values.display_external_ip }}'
    mgmt_cluster_external_domain: '{{ .Values.cluster_external_domain }}'
    dns_resolver: true
    capo:
      image_name: '{{ .Values.cluster.capo.image_name | include "set-if-defined" }}'
      network_id: '{{ .Values.cluster.capo.network_id | include "set-if-defined" }}'
  units:
    cluster-import:
      enabled: '{{ tuple . "rancher" | include "unit-enabled" }}'
  metal3:
    provider: '{{ .Values.metal3.provider }}'

shared_workload_clusters_secret_values:
  cluster:
    capo: '{{ tuple (dict "clouds_yaml" (.Values.cluster | dig "capo" "clouds_yaml" dict)) (.Values.cluster | dig "capo" "clouds_yaml" dict) | include "set-only-if" }}'
