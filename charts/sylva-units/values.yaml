# Default values for sylva-units.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# generic helm chart release name overrides
nameOverride: ""
fullnameOverride: ""

# registry secrets
registry_secret: {}
  # when using gitlab, git creds can be used as registry creds
  #registry.gitlab.com:
  #  username: your_user_name
  #  password: glpat-XXXXX

git_repo_spec_default:
  interval: 60m0s

oci_repo_spec_default:
  interval: 60m0s

source_templates: # template to generate Flux GitRepository/OCIRepository resources
  # <repo-name>:
  #   kind:  GitRepository/OCIRepository
  #   spec:  # partial spec for a Flux resource
  #     url: https://gitlab.com/sylva-projects/sylva-core.git
  #     #secretRef: # is autogenerated based on 'auth'
  #     ref: # can be overridden per-unit, with 'ref_override'
  #       branch: main
  #   auth: # optional 'username'/'password' dict containing git authentication information
  #   existing_source: # optional, when this value is set the specified GitRepository or OCIRepository will be used instead of creating one based on 'spec'
  #     name: sylva-core
  #     kind: GitRepository or OCIRepository

  sylva-core:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-core.git
      ref:
        # this is provided only for reference
        # in practice the sylva-core framework will always override this ref so that the
        # currently checked out commit of sylva-core is used by sylva-units
        # (you can grep the code for "CURRENT_COMMIT" to find out why)
        branch: main

  capi-rancher-import:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/capi-rancher-import.git
      ref:
        tag: 0.1.3

  weave-gitops:
    kind: GitRepository
    spec:
      url: https://github.com/weaveworks/weave-gitops.git
      ref:
        tag: v0.31.2

  metal3:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3.git
      ref:
        tag: 0.1.1

  sylva-capi-cluster:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-capi-cluster.git
      ref:
        tag: 0.0.18

  local-path-provisioner:
    kind: GitRepository
    spec:
      url: https://github.com/rancher/local-path-provisioner.git
      ref:
        tag: v0.0.24

  sriov-resources:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sriov-resources.git
      ref:
        tag: 0.0.3

  os-image-server:
    kind: GitRepository
    spec:
      url: https://gitlab.com/sylva-projects/sylva-elements/helm-charts/os-image-server.git
      ref:
        tag: 1.4.0

helm_repo_spec_default:
  interval: 60m0s


# this defines the default .spec for a Kustomization resource
# generated for each item of 'units'
unit_kustomization_spec_default: # default .spec for a Kustomization
  force: false
  prune: true
  interval: 15m
  retryInterval: 1m
  timeout: 30s

# this defines the default .spec for a HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_spec_default:  # default for the .spec of a HelmRelease
  interval: 15m
  install:
    remediation:
      retries: 2
      remediateLastFailure: false

# this defines the default .spec for a Kustomization resource containing the HelmRelease resource
# generated a unit with a "helmrelease_spec" field
unit_helmrelease_kustomization_spec_default:
  path: ./kustomize-units/helmrelease-generic
  sourceRef: >-
    {{- $existing_source := get (index .Values.source_templates "sylva-core") "existing_source" -}}
    {{- if $existing_source -}}
      {{- $existing_source | include "preserve-type" -}}
    {{- else -}}
      {{- dict "kind" (index .Values.source_templates "sylva-core" | dig "kind" "GitRepository") "name" "sylva-core" | include "preserve-type" -}}
    {{- end -}}
  wait: true

# default value used if units.xxx.enabled is not specified
units_enabled_default: false

# unit_template define unit settings
#   a unit can inherit from one of those
unit_templates:
  sylva-units:
    repo: sylva-core

# this defines Flux HelmRelease objects, and for each  # FIXME
# a corresponding GitRepository or OCIRepository (and Secret, TODO: the Secret don't need to be generated for each unit)
units:
  # <unit-name>:
  #   enabled: boolean or GoTPL
  #   repo: <name of the repo under 'source_templates'> (for use with kustomization_spec)
  #   helm_repo_url: URL of the Helm repo to use (for use with helmrelease_spec, but not mandatory, 'repo' can be used as well to use a git repo)
  #   labels: (optional) dict holding labels to add to the resources for this unit
  #   ref_override: optional, if defined, this dict will be used for the GitRepository or OCIRepository overriding spec.ref (not used if some helm_repo_* is set)
  #   depends_on: dict defining the dependencies of this unit, keys are unit names, values are booleans
  #               (these dependencies are injected in the unit Kustomization via 'spec.dependsOn')
  #   helmrelease_spec:  # optionnal, contains a partial spec for a FluxCD HelmRelease, all the
  #                      # key things are generated from unit_helmrelease_spec_default
  #                      # and from other fields in the unit definition
  #     _postRenderers: # this field can be used in this file, it will be merged into user-provided 'postRenderers'
  #   helm_chart_artifact_name: optional, if specified, when deploying the Helm chart from an OCI artifact,
  #                             helm_chart_artifact_name will be used as chart name instead of helmrelease_spec.chart.spec.chart last path item
  #                             this is required if helmrelease_spec.chart.spec.chart is empty, '.' or '/'
  #                             (also used by tools/oci/push-helm-chart to generate the artifact)
  #   kustomization_spec:  # contains a partial spec for a FluxCD Kustomization, most of the
  #                        # things are generated from unit_kustomization_spec_default
  #     # sourceRef is generated from .git_repo field
  #     path: ./path-to-unit-under-repo
  #     # the final path will hence be:
  #     # - <git repo template>.spec.url + <unit>.spec.path  (if <git repo template> has spec.url defined)
  #     _patches: # this field can be used in this file, it will be merged into user-provided 'patches'
  #     _components: # this field can be used in this file, it will be merged into user-provided 'components'
  #
  #   helm_secret_values: # (dict), if set what is put here is injected in HelmRelease.valuesFrom as a Secret
  #   kustomization_substitute_secrets: # (dict), if set what is put here is injected in Kustomization.postBuild.substituteFrom as a Secret
  #   unit_template: optional, today used only for units that use sylva-units itself
  #                  the settings for the unit are inherited from the corresponding entry under unit_templates

  flux-system:
    # note that Flux is always installed on the current cluster as a pre-requisite to installing the chart
    # this units contains Flux definitions *to manage the Flux system itself via gitops*
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/flux-system/base
      targetNamespace: flux-system
      wait: true
      postBuild:
        substitute:
          var_substitution_enabled: "true" # To force substitution when configmap does not exist
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
          optional: true

  cert-manager:
    helm_repo_url: https://charts.jetstack.io
    helmrelease_spec:
      chart:
        spec:
          chart: cert-manager
          version: v1.12.4
      targetNamespace: cert-manager
      install:
        createNamespace: true
      values:
        installCRDs: true
        securityContext:
          seccompProfile: null
        webhook:
          securityContext:
            seccompProfile: null
        cainjector:
          securityContext:
            seccompProfile: null
        startupapicheck:
          securityContext:
            seccompProfile: null

  trivy-operator:
    enabled: no
    helm_repo_url: https://aquasecurity.github.io/helm-charts/
    helmrelease_spec:
      chart:
        spec:
          chart: trivy-operator
          version: 0.13.2
      targetNamespace: trivy-system
      install:
        createNamespace: true
      values:
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          privileged: false
          readOnlyRootFilesystem: true
          runAsGroup: 10000
          runAsUser: 10000
        serviceAccount:
          annotations: {}
          create: true
          name: trivy-operator
        trivy:
          httpProxy: '{{ .Values.proxies.https_proxy }}'
          httpsProxy: '{{ .Values.proxies.https_proxy }}'
          noProxy: '{{ include "sylva-units.no_proxy" . }}'
          severity: UNKNOWN,HIGH,CRITICAL
        trivyOperator:
          scanJobPodTemplatePodSecurityContext:
            runAsGroup: 10000
            runAsUser: 10000


# sylva-ca provides a Certificate Authority for Components of the management cluster
  sylva-ca:
    depends_on:
      cert-manager: true
      external-secrets-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca
      targetNamespace: cert-manager
      wait: true

# namespace-defs creates the namespaces to be used by various units
  namespace-defs:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/namespace-defs
      wait: true
      prune: false

# Postgresql deployment for keycloak
  postgres:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    helm_repo_url: https://charts.bitnami.com/bitnami
    helmrelease_spec:
      chart:
        spec:
          chart: postgresql
          version: 12.8.5
      targetNamespace: keycloak
      values:
        global:
          postgresql:
            auth:
              database: keycloak
          auth:
            username: postgres
        nameOverride: postgres
        primary:
          containerSecurityContext:
            seccompProfile: null

  vault-operator:
    helm_repo_url: https://kubernetes-charts.banzaicloud.com
    helmrelease_spec:
      chart:
        spec:
          chart: vault-operator
          version: 1.19.0
      targetNamespace: vault
      install:
        createNamespace: true
      values:
        psp:
          enabled: true

# Vault assumes that the certificate vault-tls has been issued
  vault:
    depends_on:
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      '{{ .Values._internal.default_storage_class_unit }}': true
      vault-operator: true
    annotations:
      sylvactl/readyMessage: "Vault UI can be reached at https://{{ .Values.cluster.vault.external_hostname }} ({{ .Values.cluster.vault.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: sylva-core
    kustomization_substitute_secrets:
      ADMIN_PASSWORD: '{{ .Values.cluster.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/vault
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'
          VAULT_REPLICAS: '{{ .Values._internal.vault_replicas }}'
          MAX_POD_UNAVAILABLE: '{{ int .Values._internal.vault_replicas | eq 1 | ternary 0 1 }}'
      healthChecks:  # sometimes this kustomization seems correctly applied while vault pod is not running, see https://gitlab.com/sylva-projects/sylva-core/-/issues/250
      # so we replace wait:true by checking for the Vault components health
        - apiVersion: apps/v1
          kind: StatefulSet
          name: vault
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-operator
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault
          namespace: vault
        - apiVersion: v1
          kind: Service
          name: vault-configurer
          namespace: vault
        - apiVersion: apps/v1
          kind: Deployment
          name: vault-configurer
          namespace: vault


  vault-config-operator:
    depends_on:
      namespace-defs: true
      cert-manager: true
      monitoring: '{{ .Values.units | dig "vault-config-operator" "helmrelease_spec" "values" "enableMonitoring" true }}'
    helm_repo_url: https://redhat-cop.github.io/vault-config-operator
    helmrelease_spec:
      chart:
        spec:
          chart: vault-config-operator
          version: v0.8.21
      targetNamespace: vault
      values:
        enableCertManager: true
        enableMonitoring: false

  vault-secrets: # generate random secrets in vault, configure password policy, authentication backends, etc...
    repo: sylva-core
    depends_on:
      vault: true
      vault-config-operator: true
    kustomization_spec:
      path: ./kustomize-units/vault-secrets
      wait: true

  vault-oidc:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak-resources: true
      vault: true
      vault-config-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/vault-oidc
      wait: true
      postBuild:
        substitute:
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'

  external-secrets-operator:
    helm_repo_url: https://charts.external-secrets.io
    helmrelease_spec:
      chart:
        spec:
          chart: external-secrets
          version: 0.9.4
      targetNamespace: external-secrets
      install:
        createNamespace: true
      values:
        installCRDs: true
        securityContext:
          seccompProfile: null
        webhook:
          securityContext:
            seccompProfile: null
        certController:
          securityContext:
            seccompProfile: null

  eso-secret-stores:
    depends_on:
      external-secrets-operator: true
      vault: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/eso-secret-stores
      wait: true

  cis-operator-crd:
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark-crd
          version: 4.0.0
      targetNamespace: cis-operator-system
      install:
        createNamespace: true

  cis-operator:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" }}'
    depends_on:
      cis-operator-crd: true
    helm_repo_url: https://charts.rancher.io
    helmrelease_spec:
      chart:
        spec:
          chart: rancher-cis-benchmark
          version: 4.0.0
      targetNamespace: cis-operator-system
      values:
        global:
          cattle:
            psp:
              enabled: true

  # this allows for running a CIS scan for management cluster
  # generates a report which can be viewed and downloaded in CSV from the Rancher UI, at https://rancher.sylva/dashboard/c/local/cis/cis.cattle.io.clusterscan
  cis-operator-scan:
    enabled_conditions:
    - '{{ tuple . "cis-operator" | include "unit-enabled" }}'
    depends_on:
      cis-operator: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cis-operator-scan
      wait: false
      postBuild:
        substitute:
          SCAN_PROFILE: '{{ .Values.cluster.cis_benchmark_scan_profile }}'

  keycloak:
    depends_on:
      sylva-ca: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      postgres: true
      synchronize-secrets: true # make sure that the secret keycloak-initial-admin is ready to be consummed
    annotations:
      sylvactl/readyMessage: "Keycloak admin console can be reached at https://{{ .Values.cluster.keycloak.external_hostname }}/admin/master/console, user 'admin', password in Vault at secret/keycloak ({{ .Values.cluster.keycloak.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak
      targetNamespace: keycloak
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'
        substituteFrom:
        - kind: ConfigMap
          name: proxy-env-vars
      healthChecks:  # cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        # the Keycloak StatefulSet set is produced, by the combination of Keycloak operator
        # and a Keycloak custom resource, it relies on the postgres DB also deployed by this unit
        # hence, checking for the health of this component can be done by checking this StatefulSet
        - apiVersion: apps/v1
          kind: StatefulSet
          name: keycloak
          namespace: keycloak
      _components:
        - '{{ tuple "components/keycloak-operator-proxies" (.Values.proxies.https_proxy) | include "set-only-if" }}'

  keycloak-legacy-operator:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      vault-secrets: true  # the credential-external-keycloak Secret use by the legacy operator is generated from ES/Vault secret/data/keycloak
      keycloak: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-legacy-operator
      targetNamespace: keycloak
      wait: true
      _patches:
        - patch: |
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: keycloak-realm-operator
            spec:
              template:
                spec:
                  securityContext:
                    runAsUser: 10000
          target:
            kind: Deployment
            name: keycloak-realm-operator

  keycloak-resources:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
    repo: sylva-core
    kustomization_substitute_secrets:
      SSO_PASSWORD: '{{ .Values.cluster.admin_password }}'
    kustomization_spec:
      path: ./kustomize-units/keycloak-resources
      targetNamespace: keycloak
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.cluster.rancher.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.cluster.flux.external_hostname }}'
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'
      healthChecks:  #  cannot use __wait: true__ here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/144
        - apiVersion: legacy.k8s.keycloak.org/v1alpha1
          kind: KeycloakRealm
          name: sylva
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-rancher-client  # this secret is a byproduct of the rancher-client KeycloakClient resource
          namespace: keycloak
        - apiVersion: v1
          kind: Secret
          name: keycloak-client-secret-flux-webui-client  # this secret is a byproduct of the flux-webui-client KeycloakClient resource
          namespace: keycloak

  # this is a Job to manually add a custom client-scope to sylva realm (on top of default ones)
  # while CRD option does not yet provide good results (overrides defaults)
  keycloak-add-client-scope:
    enabled_conditions:
    - '{{ tuple . "flux-webui" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      keycloak-resources: true
      keycloak: true  # this unit defines the keycloak-initial-admin Secret used by the script
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: keycloak-add-client-scope
          - op: replace
            path: /spec/backoffLimit
            value: 10
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/keycloak-add-client-scope.sh" | indent 4 }}

  keycloak-oidc-external-secrets:
    enabled_conditions:
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    depends_on:
      keycloak: true
      keycloak-legacy-operator: true
      keycloak-resources: true
      eso-secret-stores: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/keycloak-oidc-external-secrets
      postBuild:
        substitute:
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.cluster.flux.external_hostname }}'
      wait: false
      healthChecks:
        - apiVersion: v1
          kind: Secret
          name: oidc-auth
          namespace: flux-system

  kyverno:
    helm_repo_url: https://kyverno.github.io/kyverno
    helmrelease_spec:
      chart:
        spec:
          chart: kyverno
          version: 3.0.5
      targetNamespace: kyverno
      install:
        createNamespace: true
      values:
        admissionController:
          container:
            securityContext:
              seccompProfile: null
          initContainer:
            securityContext:
              seccompProfile: null
        test:
          securityContext:
            seccompProfile: null
        cleanupController:
          securityContext:
            seccompProfile: null
        cleanupJobs:
          admissionReports:
            securityContext:
              seccompProfile: null
        clusterAdmissionReports:
          securityContext:
            seccompProfile: null
        backgroundController:
          securityContext:
            seccompProfile: null
        reportsController:
          securityContext:
            seccompProfile: null

  kyverno-policies:
    enabled_conditions:
    - '{{ tuple . "kyverno" | include "unit-enabled" }}'
    depends_on:
      kyverno: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kyverno-policies
      wait: true

  capi:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capi
      wait: true

  capd:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: '{{ ternary "./kustomize-units/capd/base" "./kustomize-units/capd/rke2" (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") }}'
      postBuild:
        substitute:
          CAPD_DOCKER_HOST: '{{ .Values.cluster.capd.docker_host }}'
      wait: true

  capo:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capo
      wait: true

  capm3:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capm3
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  capv:
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/capv
      postBuild:
        substitute:
          force_var_substitution_enabled: "true"  # dummy value to ensure substitution of defaults
      wait: true

  cabpk:  # kubeadm
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpk
      wait: true

  cabpr:  # RKE2
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/cabpr
      wait: true

  metal3:
    depends_on:
      cert-manager: true
      '{{ .Values._internal.default_storage_class_unit }}': true
    repo: metal3
    helm_chart_artifact_name: metal3
    helmrelease_spec:
      chart:
        spec:
          chart: .
      install:
        createNamespace: true
      targetNamespace: metal3-system
      values:  # see https://gitlab.com/sylva-projects/sylva-elements/helm-charts/metal3/-/blob/main/values.yaml
        global:
          storageClass: '{{ .Values._internal.default_storage_class }}'
        # ironicIPADownloaderBaseURI:
        httpProxy: '{{ .Values.proxies.https_proxy }}'
        httpsProxy: '{{ .Values.proxies.https_proxy }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        images:
          baremetalOperator:
            repository: quay.io/metal3-io/baremetal-operator
            tag: "v0.1.1"
          ironic:
            repository: quay.io/metal3-io/ironic
            tag: "capm3-v1.2.0"
          ironicIPADownloader:
            repository: registry.gitlab.com/sylva-projects/sylva-elements/container-images/ironic-ipa-downloader/ipa
            tag: "0.2.0"
        persistence:
          ironic:
            size: "10Gi"
            accessMode: "ReadWriteOnce"
        services:
          ironic:
            # Specify the IP address used by Ironic service
            ironicIP: '{{ .Values.cluster.cluster_external_ip }}'
            # We allow shared IPs because we are exposing (by design) a single cluster external IP
            annotations:
              metallb.universe.tf/allow-shared-ip: '{{ .Values.cluster.cluster_external_ip }}'
        mariadb:
          persistence:
            storageClass: '{{ .Values._internal.default_storage_class }}'
        podSecurityContext:
          runAsNonRoot: true
          runAsUser: 1000
        securityContext:
          runAsNonRoot: true
          runAsUser: 1000
    helm_secret_values:
      mariadb:
        auth:
          # if rootPassword or replicationPassword are left empty, we use a randomly generated password
          rootPassword: '{{ .Values._internal.default_password | trunc 32 }}'
          replicationPassword: '{{ .Values._internal.default_password | trunc -32 }}'
          ironicPassword: '{{ .Values._internal.default_password }}'
      auth:
        ironicPassword: '{{ .Values._internal.default_password }}'
        ironicInspectorPassword: '{{ .Values._internal.default_password }}'

  # this unit is here only to have a single thing to look at to determine
  # if everything is ready for pivot (see bootstrap.values.yaml pivot unit)
  capi-providers-pivot-ready:
    repo: sylva-core
    # we copy the dependencies of the 'cluster' unit
    # (with the exception of "capo-cluster-resources" which we don't need, given how what it produces is consumed)
    depends_on: '{{ omit .Values.units.cluster.depends_on "capo-cluster-resources" | include "preserve-type" }}'
    kustomization_spec:
      path: ./kustomize-units/dummy  # does not create any relevant resource
      wait: false

  local-path-provisioner:
    repo: local-path-provisioner
    helmrelease_spec:
      chart:
        spec:
          chart: deploy/chart/local-path-provisioner
      targetNamespace: kube-system
      install:
        createNamespace: true
      values:
        storageClass:
          defaultClass: '{{ .Values._internal.default_storage_class | eq "local-path" | include "as-bool" }}'
        helperImage:
          repository: docker.io/library/busybox
          tag: 1.36.1
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: local-path-provisioner
                spec:
                  template:
                    spec:
                      containers:
                      - name: local-path-provisioner
                        securityContext:
                          runAsUser: 1000

  cluster:
    repo: sylva-capi-cluster
    helm_chart_artifact_name: sylva-capi-cluster
    depends_on:
      capi: true
      '{{ .Values.cluster.capi_providers.infra_provider }}': true
      '{{ .Values.cluster.capi_providers.bootstrap_provider }}': true
      capo-cluster-resources: '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      metal3: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
      default/os-image-server: '{{ .Values.cluster.capi_providers.infra_provider | eq "capm3" }}'
    labels:
      suspend-on-pivot: "yes"  # this unit must be suspended before pivot
    kustomization_spec:
      healthChecks:
        # wait for OpenStackCluster when this provider is used, as Cluster will be seen as non-ready when it will be paused (as observedGeneration will differ from generation)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "infrastructure.cluster.x-k8s.io/v1alpha6" "cluster.x-k8s.io/v1beta1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.infra_provider "capo") | ternary "OpenStackCluster"                         "Cluster" }}'
          name: '{{ .Values.cluster.name }}'
          namespace: '{{ .Release.Namespace }}'
        # wait for the control plane to be ready (the api/version/kind to look for depends on the bootstrap provider in use)
        - apiVersion: '{{ (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "controlplane.cluster.x-k8s.io/v1beta1" "controlplane.cluster.x-k8s.io/v1alpha1" }}'
          kind: '{{       (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "KubeadmControlPlane"                   "RKE2ControlPlane" }}'
          name: '{{ .Values.cluster.name }}-control-plane'
          namespace: '{{ .Release.Namespace }}'
        # all the above is subject to a race condition: if Flux checks the status too early
        # it concludes, because CAPI resources aren't fully kstatus compliant, that the resource is ready
        # waiting for the cluster kubeconfig Secret is a workaround
        - apiVersion: v1
          kind: Secret
          name: '{{ .Values.cluster.name }}-kubeconfig'
          namespace: '{{ .Release.Namespace }}'
    helmrelease_spec:
      targetNamespace: '{{ .Release.Namespace }}'
      chart:
        spec:
          chart: .
      values:
        name: '{{ .Values.cluster.name }}'
        air_gapped: '{{ .Values.cluster.air_gapped  | include "preserve-type" }}'
        cis_profile: '{{ .Values.cluster.cis_profile }}'
        capi_providers: '{{ .Values.cluster.capi_providers | include "preserve-type" }}'
        image: '{{ .Values.cluster.image }}'
        worker_machine_image: '{{ .Values.cluster.worker_machine_image | default .Values.cluster.image }}'
        k8s_version: '{{ .Values.cluster.k8s_version }}'
        control_plane_replicas: '{{ .Values.cluster.control_plane_replicas | include "preserve-type" }}'
        cluster_external_ip: '{{ tuple .Values.cluster.cluster_external_ip (not (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
        cluster_public_ip: '{{ tuple (.Values.cluster.capo.floating_ip) (not (eq .Values.cluster.capo.floating_ip "")) | include "set-only-if" }}'
        cluster_services_cidrs:
          - 10.43.0.0/16
        cluster_pods_cidrs:
          - 10.42.0.0/16
        capo:
          flavor_name: '{{ .Values.cluster.capo.flavor_name }}'
          worker_flavor_name: '{{ .Values.cluster.capo.worker_flavor_name }}'
          ssh_key_name: '{{ .Values.cluster.capo.ssh_key_name }}'
          network_id: '{{ .Values.cluster.capo.network_id }}'
          rootVolume: '{{ .Values.cluster.capo.rootVolume | include "preserve-type" }}'
          control_plane_security_group_name: "capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}"
          worker_security_group_name: "capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}"
          control_plane_az: '{{ tuple (.Values.cluster | dig "capo" "control_plane_az" list | include "preserve-type") (.Values.cluster | dig "capo" "control_plane_az" nil) | include "set-only-if" }}'
          #cacert: # cert used to validate CA of OpenStack APIs
          resources_tag: '{{ .Values.cluster.capo.resources_tag }}'
        capv:
          dataCenter: '{{ .Values.cluster.capv.dataCenter }}'
          networks:
            default:
              networkName: '{{ .Values.cluster.capv.network }}'
          server: '{{ .Values.cluster.capv.server }}'
          dataStore: '{{ .Values.cluster.capv.dataStore }}'
          tlsThumbprint: '{{ .Values.cluster.capv.tlsThumbprint }}'
          ssh_key: '{{ .Values.cluster.capv.ssh_key }}'
          folder: '{{ .Values.cluster.capv.folder }}'
          resourcePool: '{{ .Values.cluster.capv.resourcePool }}'
          storagePolicyName: '{{ .Values.cluster.capv.storagePolicyName }}'
          control_plane_profile:
            numCPUs: 2
            memoryMiB: 8192
            diskGiB: 25
          storageClass:
            storagePolicyName: '{{ .Values.cluster.capv.storagePolicyName }}'
            fsType: ext4
        machine_deployment_default:
          metadata:
            labels:
              created-by/authors: Project-Sylva
            annotations:
              repo: "https://gitlab.com/sylva-projects/sylva-elements/helm-charts/sylva-capi-cluster"
          network_interfaces: {}
        machine_deployments:
          md0:
            replicas: '{{ .Values.cluster.worker_replicas | include "preserve-type" }}'
            failure_domain: ""
            network_interfaces: {}
        dns_resolver: false
        ntp: '{{ .Values.ntp | include "preserve-type" }}'
        proxies: '{{ .Values.proxies | include "preserve-type" }}'
        registry_mirrors: '{{ .Values.registry_mirrors | include "preserve-type" }}'
        metallb_helm_oci_url: '{{ .Values.metallb_helm_oci_url | include "preserve-type" }}'
        metallb_helm_version: '{{ .Values.units.metallb.helmrelease_spec.chart.spec.version }}'
      valuesFrom:
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: cluster_external_ip
          optional: true
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: control_plane_servergroup_id
          targetPath: capo.control_plane_servergroup_id
          optional: true
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: worker_servergroup_id
          targetPath: capo.worker_servergroup_id
          optional: true
    helm_secret_values:
      capo:
        clouds_yaml: '{{ .Values.cluster | dig "capo" "clouds_yaml" dict | include "preserve-type" }}'  # for CAPO and cinder-csi
      capv:
        username: '{{ .Values.cluster.capv.username }}'
        password: '{{ .Values.cluster.capv.password }}'

  heat-operator:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/heat-operator
      wait: true

  capo-cluster-resources:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    repo: sylva-core
    depends_on:
      heat-operator: true
    kustomization_spec:
      path: ./kustomize-units/capo-cluster-resources
      wait: true
      targetNamespace: default
      postBuild:
        substitute:
          STACK_NAME_PREFIX: 'management-cluster-{{ tuple . .Values.cluster.capo.resources_tag | include "interpret-as-string" | replace "." "-" }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.cluster.capo.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.cluster.capo.worker_affinity_policy }}'
          CAPO_EXTERNAL_NETWORK_ID: '{{ tuple .Values.cluster.capo.external_network_id
            (and .Values.cluster.capo.external_network_id (eq .Values.cluster.capi_providers.infra_provider "capo")) | include "set-only-if" }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}'
          CAPO_CREATE_IRONIC_SECURITY_GROUP: '{{ tuple . (and (.Values.units.metal3.enabled | default false) (.Values.cluster.capi_providers.infra_provider | eq "capo")) "true" "false" | include "interpret-ternary" }}'

  pause-cluster-reconciliation:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # Following condition is required in bootstrap phase, when sylva-units is installed in management-cluster
    # with cluster unit disabled before pivoting, this unit can't depend on a disabled unit in that case
    - '{{ tuple . "cluster" | include "unit-enabled" }}'
    depends_on:
      cluster: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: pause-cluster-reconciliation-job
          - op: replace
            path: /spec/template/metadata/annotations/checksum
            value: '{{ .Values | toJson | sha256sum }}'
      # We need to create at least one RoleBinding per namespace as they are referring to namespaced serviceAccounts
      - target:
          kind: ClusterRoleBinding
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: 'kube-job-{{ .Release.Namespace }}'
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/entrypoint.sh
            value: |
              #!/bin/bash
              $(dirname $0)/kube-job.sh
              echo "All done"
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash

              kubectl wait {{ (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "KubeadmControlPlane" "RKE2ControlPlane" }} \
                {{ .Values.cluster.name }}-control-plane --for condition=Available --timeout 600s
              kubectl wait {{ (eq .Values.cluster.capi_providers.bootstrap_provider "cabpk") | ternary "KubeadmControlPlane" "RKE2ControlPlane" }} \
                {{ .Values.cluster.name }}-control-plane --for condition=Resized   --timeout 600s

              kubectl wait MachineDeployment -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }} --for condition=Available --timeout 600s

              kubectl wait machine -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }} --for condition=Ready --timeout 600s
              kubectl wait machine -l cluster.x-k8s.io/cluster-name={{ .Values.cluster.name }} --for condition=NodeHealthy --timeout 600s

              kubectl patch cluster.cluster.x-k8s.io  {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": true}}'

  resume-cluster-reconciliation:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    # This unit should not be enabled on first run when cluster kustomisation is not yet created
    - '{{ not (lookup "kustomize.toolkit.fluxcd.io/v1" "Kustomization" .Release.Namespace "cluster" | empty) }}'
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: resume-cluster-reconciliation-job
          - op: replace
            path: /spec/template/metadata/annotations/checksum
            value: '{{ .Values | toJson | sha256sum }}'
      # We need to create at least one RoleBinding per namespace as they are referring to namespaced serviceAccounts
      - target:
          kind: ClusterRoleBinding
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: 'kube-job-{{ .Release.Namespace }}'
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/entrypoint.sh
            value: |
              #!/bin/bash
              $(dirname $0)/kube-job.sh
              echo "All done"
          - op: replace
            path: /data/kube-job.sh
            value: |
              #!/bin/bash
              kubectl patch cluster.cluster.x-k8s.io  {{ .Values.cluster.name }} --type merge -p '{"spec":{"paused": false}}'

  calico-crd:
    enabled_conditions:
    - '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helmrelease_spec:
      releaseName: rke2-calico-crd
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-calico-crd
          version: v3.25.001

  calico:
    depends_on:
      calico-crd: true
    helm_repo_url: https://rke2-charts.rancher.io
    helmrelease_spec:
      releaseName: rke2-calico
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-calico
          version: v3.25.001
      values:
        installation:
          calicoNetwork:
            bgp: Enabled

  metallb:
    enabled_conditions:
      - '{{ or (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.capi_providers.infra_provider | eq "capd") }}'
    depends_on:
      calico: true
    helm_repo_url: https://metallb.github.io/metallb
    helmrelease_spec:
      targetNamespace: metallb-system
      chart:
        spec:
          chart: metallb
          version: 0.13.9
      values:
        controller:
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
        speaker:
          nodeSelector:
            node-role.kubernetes.io/control-plane: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") | ternary "true" "" }}'
          tolerations:
          - key: node.cloudprovider.kubernetes.io/uninitialized
            value: "true"
            effect: NoSchedule
          - effect: NoExecute
            key: node-role.kubernetes.io/etcd
          - effect: NoSchedule
            key: node-role.kubernetes.io/master
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane

  cinder-csi-psp:
    enabled_conditions:
    - '{{ .Values.cluster.cis_profile | eq "cis-1.6" }}'
    - '{{ tuple . "cinder-csi" | include "unit-enabled" }}'
    depends_on:
      namespace-defs: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/cinder-csi-psp
      targetNamespace: cinder-csi
      wait: true

  cinder-csi:
    enabled_conditions:
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
    depends_on:
      namespace-defs: true
      cinder-csi-psp: '{{ tuple . "cinder-csi-psp" | include "unit-enabled" }}'
    helm_repo_url: https://kubernetes.github.io/cloud-provider-openstack
    helmrelease_spec:
      chart:
        spec:
          chart: openstack-cinder-csi
          version: 2.28.0
      targetNamespace: cinder-csi
      install:
        createNamespace: false
      values:
        clusterID: '{{ .Values.cluster.capo.resources_tag }}'
        storageClass:
          enabled: false
          delete:
            isDefault: false
            allowVolumeExpansion: true
          retain:
            isDefault: false
            allowVolumeExpansion: true
          custom: |-
            ---
            apiVersion: storage.k8s.io/v1
            kind: StorageClass
            metadata:
              name: "{{ .Values.cluster.capo.storageClass.name }}"
              annotations:
                storageclass.kubernetes.io/is-default-class: "true"
            provisioner: cinder.csi.openstack.org
            volumeBindingMode: Immediate
            reclaimPolicy: Delete
            allowVolumeExpansion: true
            parameters:
              type: "{{ .Values.cluster.capo.storageClass.type }}"
    helm_secret_values:
      secret:
        enabled: "true"
        create: "true"
        name: cinder-csi-cloud-config
        data:
          cloud.conf: |-
            [Global]
            auth-url = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.auth_url }}"
            tenant-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.project_name }}"
            domain-name = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.user_domain_name }}"
            username = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username }}"
            password = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.password }}"
            region = "{{ .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.region_name }}"
            tls-insecure = "{{ not .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.verify }}"
            [BlockStorage]
            ignore-volume-az = true

  imageswap-webhook:
    enabled: no
    repo: sylva-core
    depends_on:
      cert-manager: true
    kustomization_spec:
      path: ./kustomize-units/imageswap-webhook
      wait: true

  sylva-ca-certs:
    depends_on:
      namespace-defs: true
      cert-manager: true
      sylva-ca: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/sylva-ca-certs
      postBuild:
        substitute:
          RANCHER_DNS: '{{ .Values.cluster.rancher.external_hostname }}'
          VAULT_DNS: '{{ .Values.cluster.vault.external_hostname }}'
          KEYCLOAK_DNS: '{{ .Values.cluster.keycloak.external_hostname }}'
          FLUX_WEBUI_DNS: '{{ .Values.cluster.flux.external_hostname }}'
      wait: true

  synchronize-secrets:
    depends_on:
      eso-secret-stores: true
      vault-secrets: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/synchronize-secrets
      wait: true
      postBuild:
        substitute:
          FLUX_ADMIN_USERNAME: '{{ .Values.cluster.flux_webui.admin_user }}'

  rancher:
    depends_on:
      cert-manager: true
      k8s-gateway: true
      sylva-ca-certs: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      synchronize-secrets: true
    annotations:
      sylvactl/readyMessage: "Rancher UI can be reached at https://{{ .Values.cluster.rancher.external_hostname }} ({{ .Values.cluster.rancher.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    helm_repo_url: https://releases.rancher.com/server-charts/latest
    helmrelease_spec:
      chart:
        spec:
          chart: rancher
          version: 2.7.5
      targetNamespace: cattle-system
      interval: 10m0s
      upgrade:
        remediation:
          retries: 3
      values:
        privateCA: true
        useBundledSystemChart: true
        hostname: '{{ .Values.cluster.rancher.external_hostname }}'
        ingress:
          enabled: true
          ingressClassName: nginx
          tls:
            source: secret
            secretName: rancher-tls
        # restrictedAdmin: true
        # negative value will deploy 1 to abs(replicas) depending on available number of nodes
        replicas: -3
        features: embedded-cluster-api=false,provisioningv2=true
        debug: true
        proxy: '{{ get .Values.proxies "https_proxy" }}'
        noProxy: '{{ include "sylva-units.no_proxy" . }}'
        postDelete:
          namespaceList:
            - cattle-fleet-system
            - rancher-operator-system
        extraEnv:
          - name: CATTLE_BOOTSTRAP_PASSWORD
            valueFrom:
              secretKeyRef:
                name: bootstrap-secret
                key: bootstrapPassword
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: rancher
                spec:
                  template:
                    spec:
                      volumes:
                        - name: tls-ca-volume
                          secret:
                            defaultMode: 256
                            secretName: rancher-tls
                            items:
                              - key: ca.crt
                                path: cacerts.pem
                      # this is to avoid that the too-short default liveness probe
                      # prevents the Rancher installation from finishing before the pod is killed:
                      containers:
                        - name: rancher
                          livenessProbe:
                            initialDelaySeconds: 120
                            periodSeconds: 30
                            failureThreshold: 20

    kustomization_spec:
      # these healthchecks are added so that this unit does not become ready before
      # a few things that Rancher sets up behind the scene is ready
      healthChecks:
        - apiVersion: apiextensions.k8s.io/v1
          kind: CustomResourceDefinition
          name: clusters.provisioning.cattle.io  # this is because capi-rancher-import needs this
        - apiVersion: apps/v1
          kind: Deployment
          name: rancher-webhook
          namespace: cattle-system
        - apiVersion: v1
          kind: Service
          name: rancher-webhook
          namespace: cattle-system

  rancher-keycloak-oidc-provider:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    - '{{ tuple . "keycloak" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
      keycloak: true
      keycloak-resources: true
      keycloak-oidc-external-secrets: true
    kustomization_spec:
      path: ./kustomize-units/rancher-keycloak-oidc-provider
      postBuild:
        substitute:
          KEYCLOAK_EXTERNAL_URL: '{{ .Values.cluster.keycloak.external_hostname }}'
          RANCHER_EXTERNAL_URL: '{{ .Values.cluster.rancher.external_hostname }}'
      wait: true

  k8s-gateway:
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
    helm_repo_url: https://ori-edge.github.io/k8s_gateway/
    helmrelease_spec:
      chart:
        spec:
          chart: k8s-gateway
          version: 2.0.4
      targetNamespace: k8s-gateway
      install:
        createNamespace: true
      interval: 1m0s
      values:
        domain: '{{ .Values.cluster.cluster_external_domain }}'
        replicaCount: 3
        service:
          loadBalancerIP: '{{ .Values.cluster.cluster_external_ip }}'
          annotations:
            metallb.universe.tf/allow-shared-ip: '{{ tuple (.Values.cluster.cluster_external_ip) (or (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpr") (.Values.cluster.capi_providers.infra_provider | eq "capd")) | include "set-only-if" }}'

  capd-metallb-config:
    enabled_conditions:
    - '{{ tuple . "metallb" | include "unit-enabled" }}'
    - '{{ .Values.cluster.capi_providers.infra_provider | eq "capd" }}'
    repo: sylva-core
    depends_on:
      metallb: true
    kustomization_spec:
      path: ./kustomize-units/metallb-config
      wait: true
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster.cluster_external_ip }}'

  capi-rancher-import:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: capi-rancher-import
    depends_on:
      rancher: true
      k8s-gateway: true
    helmrelease_spec:
      chart:
        spec:
          chart: charts/capi-rancher-import
      targetNamespace: capi-rancher-import
      install:
        createNamespace: true
      interval: 1m0s
      values:
        securityContext:
          runAsUser: 1
        conf:
          env:
            - name: URL_REMAP
              value: "https://{{ .Values.cluster.rancher.external_hostname }}/ https://rancher.cattle-system.svc.cluster.local/"
            - name: REQUESTS_SSL_VERIFY
              value: "no"
          cattle_agent_kustomize_source_ref:
            kind: GitRepository
            name: capi-rancher-import
            namespace: default
          cattle_agent_kustomize_path: ./cattle-kustomize

  ingress-nginx:
    depends_on:
      capd-metallb-config: '{{ tuple . "capd-metallb-config" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io
    helmrelease_spec:
      releaseName: rke2-ingress-nginx
      targetNamespace: kube-system
      chart:
        spec:
          chart: rke2-ingress-nginx
          version: 4.1.008
      interval: 1m0s
      values:
        fullnameOverride: rke2-ingress-nginx
        controller:
          config:
            use-forwarded-headers: true
          kind: DaemonSet
          service:
            ############## required until we upgrade to RKE2 >= 1.24.14 - see https://gitlab.com/sylva-projects/sylva-core/-/issues/324 #########
            type: LoadBalancer
            enabled: true
            #####################################################################################################################################
            externalIPs:
            - '{{ .Values.cluster.cluster_external_ip }}'
            annotations:
              metallb.universe.tf/allow-shared-ip: '{{ tuple (.Values.cluster.cluster_external_ip) (eq .Values.cluster.capi_providers.infra_provider "capd") | include "set-only-if" }}'
          publishService:
            enabled: true
    kustomization_spec:
      healthChecks:
        - apiVersion: apps/v1
          kind: DaemonSet
          name: rke2-ingress-nginx-controller
          namespace: kube-system

  first-login-rancher:
    enabled_conditions:
    - '{{ tuple . "rancher" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      rancher: true
    kustomization_spec:
      path: ./kustomize-units/first-login-rancher
      postBuild:
        substitute:
          RANCHER_EXTERNAL_URL: '{{ .Values.cluster.rancher.external_hostname }}'
          CURRENT_TIME: '{{ now | date "2006-01-02T15:04:05.999Z" }}'
      wait: true

  flux-webui:
    depends_on:
      flux-system: true
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
      keycloak-add-client-scope: true
      keycloak-oidc-external-secrets: true
    annotations:
      sylvactl/readyMessage: "Flux Web UI can be reached at https://{{ .Values.cluster.flux.external_hostname }} ({{ .Values.cluster.flux.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})"
    repo: weave-gitops
    helm_chart_artifact_name: weave-gitops
    helmrelease_spec:
      chart:
        spec:
          chart: charts/gitops-server
      targetNamespace: flux-system
      install:
        createNamespace: false
      upgrade:
        force: true
      values:
        logLevel: debug
        envVars:
        - name: WEAVE_GITOPS_FEATURE_TENANCY
          value: "true"
        - name: WEAVE_GITOPS_FEATURE_CLUSTER
          value: "false"
        - name: WEAVE_GITOPS_FEATURE_OIDC_BUTTON_LABEL
          value: "Log in with Keycloak"
        installCRDs: true
        adminUser:
          create: true
          username: '{{ .Values.cluster.flux_webui.admin_user }}'
          createSecret: false
        rbac:
          impersonationResourceNames: ["admin", "sylva-admin@example.com"] # the Keycloak username set in unit keycloak-resources; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427
          additionalRules:
            - apiGroups: ["*"]
              resources: ["*"]
              verbs: [ "get", "list", "watch" ]
        ingress:
          enabled: true
          className: nginx
          hosts:
            - host: '{{ .Values.cluster.flux.external_hostname }}'
              paths:
                - path: /   # setting this to another value like '/flux-webui' does not work (URLs coming back from flux webui aren't rewritten by nginx)
                  pathType: Prefix
          tls:
            - secretName: flux-webui-tls
              hosts:
                - '{{ .Values.cluster.flux.external_hostname }}'
        extraVolumes:
          - name: custom-ca-cert
            secret:
              secretName: flux-webui-tls
              items:
                - key: ca.crt
                  path: ca.crt
        extraVolumeMounts:
          - name: custom-ca-cert
            mountPath: /etc/ssl/certs
            readOnly: true
        oidcSecret:
          create: false
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: flux-webui-weave-gitops
                spec:
                  template:
                    spec:
                      containers:
                      - name: weave-gitops
                        securityContext:
                          seccompProfile:
                            $patch: delete
                            type: RuntimeDefault
              - kind: ClusterRoleBinding
                apiVersion: rbac.authorization.k8s.io/v1
                metadata:
                  name: '{{ .Values.cluster.flux_webui.admin_user }}-user-read-resources-cr'
                subjects:
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: '{{ .Values.cluster.flux_webui.admin_user }}'
                  - apiGroup: rbac.authorization.k8s.io
                    kind: User
                    name: sylva-admin@example.com  # add same RBAC for the SSO user, so that when flux-webui SA impersonates it has privileges; cannot use "infra-admins" group here, see https://gitlab.com/sylva-projects/sylva-core/-/issues/427

  monitoring-crd:
    enabled_conditions:
    - '{{ tuple . "monitoring" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring-crd
      targetNamespace: cattle-monitoring-system
      chart:
        spec:
          chart: rancher-monitoring-crd
          version: 101.0.0+up19.0.3
      install:
        createNamespace: true

  monitoring:
    depends_on:
      monitoring-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: rancher-monitoring
      targetNamespace: cattle-monitoring-system
      chart:
        spec:
          chart: rancher-monitoring
          version: 101.0.0+up19.0.3
      install:
        createNamespace: true

  # this is a Job attempt to check successful import of a CAPI workload clusters in Rancher, still incomplete
  # it would be used along with a definition of a 'workload-cluster' unit
  # and is conditioned by .Values.env_type: ci
  check-rancher-clusters:
    enabled_conditions:
    - '{{ tuple . "workload-cluster" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      workload-cluster/cluster-import: true
    kustomization_spec:
      path: ./kustomize-units/kube-job
      wait: true
      force: true
      _patches:
      - target:
          kind: Job
          name: kube-job
        patch: |
          - op: replace
            path: /metadata/name
            value: check-rancher-clusters-job
          - op: replace
            path: /spec/backoffLimit
            value: 10
      - target:
          kind: ConfigMap
          name: job-scripts
        patch: |
          - op: replace
            path: /data/kube-job.sh
            value: |
          {{ .Files.Get "scripts/check-rancher-clusters.sh" | indent 4 }}
      - target:
          kind: Job
          name: kube-job
        patch: |
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ignored
          spec:
            template:
              spec:
                containers:
                - name: run-script
                  env:
                  - name: CLUSTER_NAME
                    value: '{{ .Values.cluster.test_workload_cluster_name }}'

  workload-cluster-cloud-config:
    enabled_conditions:
      - '{{ .Values.cluster.capi_providers.infra_provider | eq "capo" }}'
      - '{{ tuple . "workload-cluster" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      namespace-defs: true
      external-secrets-operator: true
    kustomization_spec:
      path: ./kustomize-units/workload-cluster-cloud-config
      wait: true

  workload-capo-cluster-resources:
    enabled_conditions:
    - '{{ tuple . (.Values.units | dig "workload-cluster" "helmrelease_spec" "values" "cluster" "capi_providers" "infra_provider" "") | include "interpret-as-string" | eq "capo"  }}'
    - '{{ tuple . "workload-cluster" | include "unit-enabled" }}'
    repo: sylva-core
    depends_on:
      namespace-defs: true
      heat-operator: true
      workload-cluster-cloud-config: true
    kustomization_spec:
      targetNamespace: workload-cluster
      path: ./kustomize-units/capo-cluster-resources
      wait: true
      _patches:
        - target:
            kind: HeatStack
          patch: |
            - op: replace
              path: /spec/identityRef
              value: cloud-config  #  cloud-config Secret is produced by workload-cluster-cloud-config unit (ExternalSecret to copy cloud config from default ns to workload-cluster ns)
            - op: replace
              path: /spec/heatStack/template/parameters/control_plane_rules
              value:
                type: json
                description: "Security group rules associated with the control plane VMs"
                default:
                  direction: [ingress, ingress, ingress, egress]
                  port: ['80', '443', '22', '53']
                  protocol: [tcp, tcp, tcp, udp]
                  remote_ip_prefix: [0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0]
            - op: replace
              path: /spec/heatStack/template/parameters/worker_rules
              value:
                type: json
                description: "Security group rules associated with the worker VMs"
                default:
                  direction: [ingress, ingress, ingress, egress]
                  port: ['80', '443', '22', '53']
                  protocol: [tcp, tcp, tcp, udp]
                  remote_ip_prefix: [0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0, 0.0.0.0/0]
      postBuild:
        substitute:
          STACK_NAME_PREFIX: 'workload-cluster-{{ tuple . .Values.cluster.capo.resources_tag | include "interpret-as-string" | replace "." "-" }}'
          CAPO_NETWORK_ID: '{{ .Values.cluster.capo.network_id }}'
          CAPO_TAG: '{{ .Values.cluster.capo.resources_tag }}'
          CONTROL_PLANE_AFFINITY_POLICY: '{{ .Values.cluster.capo.control_plane_affinity_policy }}'
          WORKER_AFFINITY_POLICY: '{{ .Values.cluster.capo.worker_affinity_policy }}'
          CONTROL_PLANE_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.test_workload_cluster_name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}'
          WORKER_SECURITY_GROUP_NAME: 'capo-{{ .Values.cluster.test_workload_cluster_name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}'

  multus:
    enabled: false
    depends_on:
      calico: '{{ tuple . "calico" | include "unit-enabled" }}'
    helm_repo_url: https://rke2-charts.rancher.io/
    helmrelease_spec:
      chart:
        spec:
          chart: rke2-multus
          version: v3.9.3-build2023010901
      targetNamespace: kube-system
      install:
        createNamespace: false
      values:
        rke2-whereabouts:
          enabled: true
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: DaemonSet
                apiVersion: apps/v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts
              - kind: ServiceAccount
                apiVersion: v1
                metadata:
                  name: multus-rke2-whereabouts
                  namespace: kube-system
                  labels:
                    $patch: delete
                    app: whereabouts

  sriov-crd:
    enabled_conditions:
    - '{{ tuple . "sriov" | include "unit-enabled" }}'
    depends_on:
      multus: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov-crd
      targetNamespace: cattle-sriov-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: sriov-crd
          version: 102.0.0+up0.1.0

  sriov:
    enabled: false
    depends_on:
      sriov-crd: true
      sriov-psp: '{{ tuple . "sriov-psp" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: sriov
      targetNamespace: cattle-sriov-system
      chart:
        spec:
          chart: sriov
          version: 102.0.0+up0.1.0
      values:
        cert_manager: true
        rancher-nfd:
          image:
            tag: v0.13.2-build20230605
      _postRenderers:
        - kustomize:
            patchesStrategicMerge:
              - kind: Deployment
                apiVersion: apps/v1
                metadata:
                  name: sriov
                  namespace: cattle-sriov-system
                spec:
                  template:
                    spec:
                      securityContext:
                        runAsGroup: 3000
                        runAsUser: 2000

  sriov-psp:
    enabled_conditions:
    - '{{ .Values.cluster.cis_profile | eq "cis-1.6" }}'
    - '{{ (tuple . "sriov" | include "unit-enabled") }}'
    depends_on:
      sriov-crd: true
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/workload-cluster-sriov-psp
      wait: true

  sriov-resources:
    enabled: false
    depends_on:
      sriov: true
    repo: sriov-resources
    helm_chart_artifact_name: sriov-resources
    helmrelease_spec:
      chart:
        spec:
          chart: .
      targetNamespace: cattle-sriov-system
      values:
        node_policies: '{{ .Values.cluster.sriov.node_policies | include "preserve-type" }}'

  coredns:
    repo: sylva-core
    kustomization_spec:
      path: ./kustomize-units/coredns
      wait: true
      _patches:
        - target:
            kind: ConfigMap
            name: coredns
          patch: |
            - op: replace
              path: /metadata/name
              value: '{{ (.Values.cluster.capi_providers.bootstrap_provider | eq "cabpk") | ternary "coredns" "rke2-coredns-rke2-coredns" }}'
      postBuild:
        substitute:
          CLUSTER_EXTERNAL_IP: '{{ .Values.cluster.cluster_external_ip }}'
          CLUSTER_EXTERNAL_DOMAIN: '{{ .Values.cluster.cluster_external_domain }}'

  longhorn-crd:
    enabled_conditions:
    - '{{ tuple . "longhorn" | include "unit-enabled" }}'
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn-crd
      targetNamespace: longhorn-system
      install:
        createNamespace: true
      chart:
        spec:
          chart: longhorn-crd
          version: 102.2.0+up1.4.1

  longhorn:
    enabled: false
    depends_on:
      longhorn-crd: true
    helm_repo_url: https://charts.rancher.io/
    helmrelease_spec:
      releaseName: longhorn
      targetNamespace: longhorn-system
      chart:
        spec:
          chart: longhorn
          version: 102.2.0+up1.4.1
      values:
        enablePSP: true
        defaultSettings:
          createDefaultDiskLabeledNodes: true
          defaultDataPath: /var/lib/longhorn
          deletingConfirmationFlag: true
          autoCleanupSystemGeneratedSnapshot: true


  # this unit is an example of how to use sylva-units to deploy a workload-cluster
  workload-cluster:
    depends_on:
      namespace-defs: true
      #for capo:
      workload-capo-cluster-resources: '{{ tuple . "workload-capo-cluster-resources" | include "unit-enabled" }}'   # produces the capo-cluster-resources used below
      workload-cluster-cloud-config: '{{ tuple . "workload-cluster-cloud-config" | include "unit-enabled" }}'
    unit_template: sylva-units
    kustomization_spec:
      targetNamespace: workload-cluster  # this ensures that the HelmRelease below is created in 'workload-cluster' ns
      healthChecks:
        - apiVersion: kustomize.toolkit.fluxcd.io/v1
          kind: Kustomization
          name: sylva-units-status
          namespace: workload-cluster
    helmrelease_spec:
      chart:
        spec:
          chart: charts/sylva-units
          reconcileStrategy: Revision
          # copy sourceRef from sylva-units HelmRelease in default ns
          # with an override of namespace field to point to the source livig in the default ns
          sourceRef: '{{ lookup "helm.toolkit.fluxcd.io/v2beta1" "HelmRelease" "default" "sylva-units" | dig "spec" "chart" "spec" "sourceRef" dict | mergeOverwrite (dict "namespace" "default") | include "preserve-type" }}'
          valuesFiles:
            - charts/sylva-units/values.yaml
            - charts/sylva-units/workload-cluster.values.yaml
            # if main sylva-units release uses OCI artifacts, do the same for this release
            - '{{ tuple "use-oci-artifacts.values.yaml" (lookup "helm.toolkit.fluxcd.io/v2beta1" "HelmRelease" "default" "sylva-units" | dig "spec" "chart" "spec" "valuesFiles" list | has "use-oci-artifacts.values.yaml") | include "set-only-if" }}'

      valuesFrom:  # this Helm release receives some parameters from the capo-cluster-resources ConfigMap
                   # which is produced by the workload-capo-cluster-resources unit
                   # ConfigMaps are mapped as optional to circumvent 'not found' errors for infra providers other than capo
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_ip
          targetPath: cluster.cluster_external_ip
          optional: true
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: allocated_fip
          targetPath: cluster.capo.floating_ip
          optional: true
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: control_plane_servergroup_id
          targetPath: cluster.capo.control_plane_servergroup_id
          optional: true
        - kind: ConfigMap
          name: capo-cluster-resources
          valuesKey: worker_servergroup_id
          targetPath: cluster.capo.worker_servergroup_id
          optional: true

      values:

        source_templates:
          sylva-core:
            existing_source: '{{ deepCopy .Values._internal.sylva_core_existing_source | merge (dict "namespace" .Release.Namespace) | include "preserve-type" }}'

        cluster:
          name: '{{ .Values.cluster.test_workload_cluster_name }}'
          image: '{{ .Values.cluster.image }}'
          worker_machine_image: '{{ .Values.cluster.worker_machine_image }}'
          capi_providers:
            infra_provider: '{{ .Values.cluster.capi_providers.infra_provider }}'
            bootstrap_provider: '{{ .Values.cluster.capi_providers.bootstrap_provider }}'
          k8s_version: '{{ .Values.cluster.k8s_version }}'
          cis_profile: '{{ .Values.cluster.cis_profile }}'
          capo:
            ssh_key_name: '{{ .Values.cluster.capo.ssh_key_name }}'
            resources_tag: '{{ tuple . (.Values.cluster | dig "capo" "resources_tag" "") | include "interpret-as-string" }}'
            storageClass: '{{ .Values.cluster | dig "capo" "storageClass" dict | include "preserve-type" }}'
            network_id: '{{ .Values.cluster.capo.network_id | default "" }}'
            flavor_name: '{{ .Values.cluster.capo.flavor_name | default "" }}'
            worker_flavor_name: '{{ .Values.cluster.capo.worker_flavor_name | default "" }}'
            control_plane_az: '{{ tuple (.Values.cluster | dig "capo" "control_plane_az" list | include "preserve-type") (.Values.cluster | dig "capo" "control_plane_az" nil) | include "set-only-if" }}'
            worker_az: '{{ .Values.cluster.capo.worker_az }}'
            rootVolume: '{{ .Values.cluster.capo.rootVolume | include "preserve-type" }}'
          capv:
            dataCenter: '{{ .Values.cluster.capv.dataCenter }}'
            network: '{{ .Values.cluster.capv.network }}'
            server: '{{ .Values.cluster.capv.server }}'
            dataStore: '{{ .Values.cluster.capv.dataStore }}'
            tlsThumbprint: '{{ .Values.cluster.capv.tlsThumbprint }}'
            ssh_key: '{{ .Values.cluster.capv.ssh_key }}'
            folder: '{{ .Values.cluster.capv.folder }}'
            resourcePool: '{{ .Values.cluster.capv.resourcePool }}'
            storagePolicyName: '{{ .Values.cluster.capv.storagePolicyName }}'
          cluster_external_domain: '{{ .Values.cluster.cluster_external_domain }}'
          air_gapped: '{{ .Values.cluster.air_gapped | include "preserve-type" }}'

        metallb_helm_oci_url: '{{ .Values.metallb_helm_oci_url | include "preserve-type" }}'

        units:

          cluster-import:
            enabled: '{{ tuple . "rancher" | include "unit-enabled" }}'

          cluster:
            helmrelease_spec:
              values:
                control_plane_replicas: 1
                enable_longhorn: '{{ .Values.units | dig "workload-cluster" "helmrelease_spec" "values" "units" "longhorn" "enabled" false | include "preserve-type" }}'
                capo:
                  control_plane_security_group_name: "capo-{{ .Values.cluster.test_workload_cluster_name }}-security-group-ctrl-plane-{{ .Values.cluster.capo.resources_tag }}"
                  worker_security_group_name: "capo-{{ .Values.cluster.test_workload_cluster_name }}-security-group-workers-{{ .Values.cluster.capo.resources_tag }}"
                mgmt_cluster_external_domain: '{{ .Values.cluster.cluster_external_domain }}'
                mgmt_cluster_external_ip: '{{ .Values.cluster.cluster_external_ip }}'
                dns_resolver: true

          #multus:   ## can be overriden via environment values
          #  enabled: true

          #longhorn:   ## can be overriden via environment values
          #  enabled: true

          #sriov:   ## can be overriden via environment values
          #  enabled: true

          #sriov-resources:  ## can be overriden via environment values
          #  enabled: true

        proxies: '{{ .Values.proxies | include "preserve-type" }}'

        ntp: '{{ .Values.ntp | include "preserve-type" }}'

        registry_mirrors: '{{ .Values.registry_mirrors | include "preserve-type" }}'

    helm_secret_values:
      cluster:
        capo:
          clouds_yaml: '{{ .Values.cluster | dig "capo" "clouds_yaml" dict | include "preserve-type" }}'  # for CAPO and cinder-csi
        capv:
          username: '{{ .Values.cluster.capv.username }}'
          password: '{{ .Values.cluster.capv.password }}'


  os-image-server:
    # this units allows to deploy web server on management cluster
    # which serves OS image file for baremetal workload cluster
    enabled_conditions:
      - '{{ tuple . "metal3" | include "unit-enabled" }}'
    depends_on:
      ingress-nginx: '{{ tuple . "ingress-nginx" | include "unit-enabled" }}'
    repo: os-image-server
    annotations:
      sylvactl/readyMessage: |
        OS images are served at:
        {{- $osImageFqdn := (coalesce .Values.cluster.os_image_server.external_hostname .Values.cluster.display_external_ip) -}}
        {{- range $imageName, $imageParams := (coalesce .Values.cluster.os_images .Values._internal.default_os_images) }}
          * https://{{ $osImageFqdn }}/{{ $imageParams.filename }}(.sha256sum)
        {{- end }}
        {{- if not (eq $osImageFqdn .Values.cluster.display_external_ip)}}
        ({{ .Values.cluster.os_image_server.external_hostname }} must resolve to {{ .Values.cluster.display_external_ip }})
        {{- end }}
    helmrelease_spec:
      chart:
        spec:
          chart: ./charts/os-image-server
      targetNamespace: os-images
      install:
        createNamespace: true
      values:
        downloader:
          proxy: '{{ get .Values.proxies "https_proxy" }}'
          no_proxy: '{{ include "sylva-units.no_proxy" . }}'
        ingress:
          className: nginx
          hosts:
            - host: '{{ .Values.cluster.os_image_server.external_hostname }}'
        osImages: '{{ coalesce .Values.cluster.os_images .Values._internal.default_os_images | include "preserve-type" }}'
        osImagePersistenceDefaults:
          enabled: true
          size: 1Gi
          storageClass: '{{ .Values._internal.default_storage_class }}'

## stuff related to the 'cluster' unit

cluster:
  name: management-cluster

  # can be set to true to do an RKE2 deployment disconnected from the Internet:
  air_gapped: false

  # cis profile to be used. Curently supported only for rke2 clusters. "cis-1.6" for k8s prior to 1.25, "cis-1.23" for 1.25+
  cis_profile: cis-1.6

  # Admin password that will be configured by default on various units
  admin_password: '{{ .Values._internal.default_password }}'

  # image reference depends provider, used for control plane nodes
  image: registry.gitlab.com/sylva-projects/sylva-elements/container-images/rke2-in-docker:v1-24-12-rke2r1

  # worker_machine_image: "Ubuntu 20.04 Pack"         # Openstack image for worker nodes

  # for now, the choice below needs to be made
  # consistently with the choice of a matching kustomization path
  # for the 'cluster' unit
  # e.g. you can use ./management-cluster-def/rke2-capd
  capi_providers:
    infra_provider: capd      # capd, capo, capm3 or capv
    bootstrap_provider: cabpk # cabpr (RKE2) or cabpk (kubeadm)

  # cis benchmark is only for rke2 so far, e.g. rke2-cis-1.23-profile-hardened
  cis_benchmark_scan_profile: '{{ eq .Values.cluster.capi_providers.bootstrap_provider "cabpr" | ternary "rke2-cis-1.23-profile-hardened" "no-scan-profile-defined-for-kubeadm-cluster" }}'

  # kubernetes version to be used
  k8s_version: '{{ .Values.cluster.capi_providers.bootstrap_provider | eq "cabpr" | ternary "v1.24.12+rke2r1" "v1.24.12" }}'

  # Additional IP addresses to be used by metallb and L3 configuration if needed
  # metallb_additional_options:
  #   l3_options:
  #     bgp_peers:
  #       - name: peerName
  #         local_asn: 64511  # example only, must be updated
  #         peer_asn: 64510  # example only, must be updated
  #         peer_address: 55.55.55.55
  #         advertised_pools:
  #           - lbpool  # default IP pool used for kube-api and ingress exposure
  #           - my-custom-pool  # additional IP pools to be advertised to this peer
  #   address_pools:
  #     - name: my-custom-pool
  #       addresses:
  #         - 55.55.55.55/33
  #         - 10.10.10.10-255.10.10.12

  # Nodes number for controlplane and workers
  control_plane_replicas: 3
  worker_replicas: 0

  capo:
    flavor_name: m1.large # Openstack flavor name
    worker_flavor_name: m1.large
    # worker_az only required to be provided when .Values.cluster.worker_replicas > 0
    ssh_key_name: # OpenStack VM SSH key
    network_id:  # OpenStack network used for nodes and VIP port
    #external_network_id # Can be provided if a FIP is needed in order to reach the management cluster VIP
    floating_ip: ""
    additional_networks:
    # Leave these parameters empty or specify additional networks to be used. Only one network of each type can be added and supported types are virtio or direct
      virtio_network_id: ""
      direct_network_id: ""
    rootVolume: {} # Let this parameter empty if you don't intent to use root volume
    # otherwise, provide following values
    # diskSize: 20 # Size of the VMs root disk
    # volumeType: '__DEFAULT__' # Type of volume to be created
    control_plane_affinity_policy: soft-anti-affinity
    worker_affinity_policy: soft-anti-affinity
    #control_plane_az: # list of OpenStack availability zones to deploy control planes nodes to, otherwise all would be candidates
    clouds_yaml: # (this is a dict, not a YAML string)
      clouds:
        capo_cloud:
          auth:
            auth_url: # replace me
            user_domain_name: # replace me
            project_domain_name: # replace me
            project_name: # replace me
            username: # replace me
            password: # replace me
          region_name: # replace me
          verify: # e.g. false
    #cacert: # cert used to validate CA of OpenStack APIs
    storageClass:
      name: cinder-csi  # name of the storageClass to be created
      #type: xxx  # please provide the cinder volume type, e.g. 'ceph_sas' (must exist in OpenStack)
    resources_tag: '{{ eq .Values.cluster.capi_providers.infra_provider "capo" | ternary (printf "sylva-%s" .Values.cluster.capo.clouds_yaml.clouds.capo_cloud.auth.username) "" }}' # tag set for OpenStack resources in management and test workload cluster

  cluster_external_ip: 55.55.55.55
  cluster_external_domain: sylva
  display_external_ip: '{{ .Values.cluster.capo.floating_ip | eq "" | ternary .Values.cluster.cluster_external_ip .Values.cluster.capo.floating_ip }}'

  capd:
    docker_host: unix:///var/run/docker.sock

  capv:
    username: ""
    password: ""
    dataCenter: ""
    network: ""
    server: ""
    dataStore: ""
    tlsThumbprint: ""
    folder: ""
    resourcePool: ""
    storagePolicyName: ""

  flux_webui:
    admin_user: admin

  rancher:
    external_hostname: 'rancher.{{ .Values.cluster.cluster_external_domain }}'

  vault:
    external_hostname: 'vault.{{ .Values.cluster.cluster_external_domain }}'

  keycloak:
    external_hostname: 'keycloak.{{ .Values.cluster.cluster_external_domain }}'

  flux:
    external_hostname: 'flux.{{ .Values.cluster.cluster_external_domain }}'

  os_image_server:
    external_hostname: ''

  # osImages images that should be served and from where they should be downloaded
  # if empty default value are used (default_os_images)
  os_images: {}

  test_workload_cluster_name: first-workload-cluster

  # to configure the SR-IOV VFs on the supported NICs of cluster nodes
  sriov:
    node_policies: {}
#     mypolicy1:
#       nodeSelector: {}  # <<< lets you further limit the SR-IOV capable nodes on which the VFs have to be created in a certain config; if not set it applies to all SR-IOV nodes
#       resourceName: ""
#       numVfs: ""
#       deviceType: ""
#       nicSelector:
#         deviceID: ""
#         vendor: ""
#         pfNames: []
#         rootDevices: []

# add your proxy settings if required
proxies:
  https_proxy: ""
  http_proxy: ""
  no_proxy: ""

# disable default values for no_proxy (localhost,.svc,.cluster.local.,.cluster.local,.sylva)
# Ex: localhost: false
no_proxy_additional: {}

# configure containerd registry mirrors following https://github.com/containerd/containerd/blob/main/docs/hosts.md
registry_mirrors:
  default_settings:                          # <<< These settings will apply to all configured mirrors
    capabilities: ["pull", "resolve"]
#   skip_verify: true
#   override_path: true
# hosts_config:
#   docker.io:
#   - mirror_url: http://your.mirror/docker
#     registry_settings:                     # <<< Host settings can be used to override default_settings
#       skip_verify: false
#   registry.k8s.io:
#   - mirror_url: ...
#   _default:
#   - mirror_url: ...

# set the type of environment between 3 possible values: dev, ci and prod
env_type: prod

# set NTP servers by IP or FQDN and enable their usage for control plane nodes
ntp:
  enabled: false
  servers:
    - 1.2.3.4
    - europe.pool.ntp.org

# this value can be overriden to use an alternate OCI registry
# by default, for an OCI deployment we'll derive the base registry URL from the one of the HelmRepository of sylva-core
# and for an non-OCI deployment, "registry.gitlab.com/sylva-projects" (this is used for OS images, which are delivered as OCI artifact even on a non-OCI deployment)
sylva_base_oci_registry: >-2
  {{
  lookup "source.toolkit.fluxcd.io/v1beta2" "HelmRepository" "default" "sylva-core" | dig "spec" "url" "" | regexReplaceAll "/[^/]$" ""
  | default "oci://registry.gitlab.com/sylva-projects"
  }}

_internal:
  # unless the default password has already been generated by a previous run, we generate a new one randomly
  default_password: '{{ lookup "v1" "Secret" "default" "sylva-units-values" | dig "data" "values" "" | b64dec | fromYaml | dig "_internal" "default_password" (randAlphaNum 64) }}'

  default_storage_class_unit: >-
    {{- tuple . (tuple . "cinder-csi" | include "unit-enabled") "cinder-csi" "local-path-provisioner" | include "interpret-ternary" -}}
  default_storage_class: >-
    {{- tuple . (tuple . "cinder-csi" | include "unit-enabled") .Values.cluster.capo.storageClass.name "local-path" | include "interpret-ternary" -}}
  vault_replicas: >-
    {{ ge (int .Values.cluster.control_plane_replicas) 3 | ternary 3 1 }}

  # this is used in source_templates.sylva-core below
  # and in any unit using sylva-units Helm chart
  sylva_core_version: ''

  sylva_core_existing_source: '{{ .Values.unit_helmrelease_kustomization_spec_default.sourceRef }}'

  # These values are used only if `cluster.os_images` is empty
  default_os_images:
    ubuntu-2204-plain-qcow2:
      uri: "{{ .Values.sylva_base_oci_registry }}/sylva-elements/diskimage-builder/diskimage-builder-plain:0.0.7"
      filename: ubuntu-22.04-plain.qcow2
      checksum: 56a6363f25a5491aa955e4dda6fae17faa10c5e0850d01fe2a10c996c7830971
