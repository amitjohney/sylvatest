---

.capm3-virt-kubeconfig:
  - |
    if [[ ${ENV_NAME} == 'rke2-capm3-virt' ]]; then
      yq -i e '.clusters[].cluster.insecure-skip-tls-verify = true' $WORKLOAD_CLUSTER_NAME-rancher.yaml
      yq -i e 'del(.clusters[].cluster.certificate-authority-data)' $WORKLOAD_CLUSTER_NAME-rancher.yaml
    fi

.test_scripts:
  edit_hosts_file:
    - domain=$(kubectl --kubeconfig management-cluster-kubeconfig get secret sylva-units-values -oyaml | yq '.data.values | @base64d' | yq '.cluster_domain')
    - |
      echo $domain
      if [[ $domain == "sylva" ]]; then
        echo "-- Get ingress saved into /etc/hosts"
        access_ip=$(kubectl --kubeconfig management-cluster-kubeconfig -n sylva-system get secrets/sylva-units-values -o jsonpath='{.data.values}' | base64 -d | yq -r .display_external_ip)
        for i in $(kubectl --kubeconfig management-cluster-kubeconfig get ingress -A -o custom-columns=:.spec.tls[].hosts[] | grep -v "<none>"); do
          echo $access_ip $i >> /etc/hosts
        done
      fi
  test_no_sso:
    - unset https_proxy http_proxy
    - export KUBECONFIG=management-cluster-kubeconfig
    - ./tools/shell-lib/get-wc-kubeconfig-from-rancher.sh $WORKLOAD_CLUSTER_NAME > $WORKLOAD_CLUSTER_NAME-rancher.yaml
    # For capm3 job we need to modify the $WORKLOAD_CLUSTER_NAME-rancher.yaml to remove the certificate and use an insecure option
    - !reference [.capm3-virt-kubeconfig]

  test_sso:
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_test_loging[collapsed=true]\r\e[0KGet ingress saved into /etc/hosts and set variables\e[0m"
    - export flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }')
    - export keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export neuvector_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n neuvector neuvector-webui-ingress  -o jsonpath='{ .spec.tls[].hosts[] }')
    - export USER_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.username}'| base64 -d)
    - export PASSWORD_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.password}'| base64 -d)
    - echo "Test dependencies and initialize firefox/driver"
    - /usr/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager --browser firefox --output json --debug
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_test_loging\r\e[0K"
    - echo "-- Testing login using SSO"
    - unset https_proxy http_proxy
    - python3 ./tools/login-test/test-sso.py
    # For capm3 job we need to modify the $WORKLOAD_CLUSTER_NAME-rancher.yaml to remove the certificate and use an insecure option
    - !reference [.capm3-virt-kubeconfig]

test-sso+workload-kubeconfig:
  stage: deployment-test
  extends:
    - .test-tags
    - .rules:skip-if-only-deploy-mgmt
  retry:
    max: 2
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_sso]
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Selenim
    - |
      attempts=0; max_attempts=5
      until kubectl run test-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- All done"

test-sso:
  stage: deployment-test
  extends:
    - .test-tags
    - .rules:run-if-only-deploy-mgmt
  retry:
    max: 2
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_sso]

test-no-sso+workload-kubeconfig:
  stage: deployment-test
  extends:
    - .test-tags
    - .rules:skip-if-only-deploy-mgmt
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_no_sso]
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Rancher API
    - |
      attempts=0; max_attempts=5
      until kubectl run test-no-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-no-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-no-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- All done"

test-login:
  stage: deployment-test
  extends:
    - .test-tags
  variables:
    HURL_JUNIT_REPORT: login_junit_report.xml
  artifacts:
    expire_in: 48 hour
    when: always
    paths:
      - $HURL_JUNIT_REPORT
      - index.html
      - store/
    reports:
      junit:
      - $HURL_JUNIT_REPORT
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - export HURL_token=$(kubectl --kubeconfig management-cluster-kubeconfig -n vault get secrets/vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d)
    - export HURL_flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }'); export domain=$(echo $HURL_rancher_url | cut -f2 -d .)
    - export HURL_keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_neuvector_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n neuvector neuvector-webui-ingress  -o jsonpath='{ .spec.tls[].hosts[] }')
    - |
      if [ -n "${HURL_neuvector_url}" ]; then
          sleep 2;
      else
          rm -rf  ./tools/login-test/neuvector-admin.hurl
      fi
    - |
      if [ -n "${HURL_harbor_url}" ]; then
          sleep 2;
      else
          rm -rf  ./tools/login-test/harbor-admin.hurl
      fi
    - hurl -k ./tools/login-test/*.hurl --test --color --noproxy=".$domain"  --report-junit $HURL_JUNIT_REPORT --report-html .
