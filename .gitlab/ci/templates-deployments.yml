---
# =============================
# Templates for deployments
# =============================

include:
  - local: .gitlab/ci/common.yml

.common-deployment:
  setup_ci_values:
    - |
      function change_git_revision() {
        echo "GIT_REVISION: $GIT_REVISION"
          if git ls-remote --exit-code --tags https://gitlab.com/sylva-projects/sylva-core.git "${GIT_REVISION}" > /dev/null; then
            echo "Provided GIT_REVISION: $GIT_REVISION is a known upstream tag. Using it as it"
            if [ -n "${OCI_TAG}" ] ; then
              OCI_TAG=${GIT_REVISION}
            fi
          elif git ls-remote --exit-code --heads https://gitlab.com/sylva-projects/sylva-core.git "${GIT_REVISION}" > /dev/null; then
            echo "Provided GIT_REVISION: $GIT_REVISION is a known upstream branch. It will be used as reference for fetching sylva-core code"
            GIT_REVISION=$(git ls-remote --exit-code --heads https://gitlab.com/sylva-projects/sylva-core.git "${GIT_REVISION}" | cut -f 1)
            SHORT_ID=$(echo $GIT_REVISION | head -c 8)
            if [ -n "${OCI_TAG}" ] ; then
              OCI_TAG=0.0.0-git-$SHORT_ID
            fi
          else
            echo "Provided GIT_REVISION: $GIT_REVISION, will try to use it as a commit id"
            SHORT_ID=$(echo $GIT_REVISION | head -c 8)
            if [ -n "${OCI_TAG}" ] ; then
              OCI_TAG=0.0.0-git-$SHORT_ID
            fi
          fi
        if [ -n "${OCI_TAG}" ] ; then
          echo "Sylva-units chart will use: $OCI_TAG reference"
          echo "Checking if oci://registry.gitlab.com/sylva-projects/sylva-core/sylva-units:$OCI_TAG exist"
          if helm pull oci://registry.gitlab.com/sylva-projects/sylva-core/sylva-units --version $OCI_TAG -d /tmp > /dev/null; then
            echo "oci://registry.gitlab.com/sylva-projects/sylva-core/sylva-units:$OCI_TAG was sucessfully pulled"
          else
            echo "Failed to retrieve: oci://registry.gitlab.com/sylva-projects/sylva-core/sylva-units:$OCI_TAG"
            echo "Ensure you are targeting a branch with an open MR and check if that OCI artifact was not removed by cleanup job"
          fi
        fi
        git fetch origin $GIT_REVISION
        echo -e "Changing git revision to use \\e[1;94m$1\\e[0m"
        git checkout $GIT_REVISION
        git status
        echo -e "Current revision: $(git rev-parse HEAD)"
      }
    - |
      function update_values_file() {
        echo -e "Applying values from \\e[1;94m$@\\e[0m to \\e[1;92m$values_file\\e[0m"
        yq -i eval-all '. as $item ireduce ({}; . * $item )' $values_file $@
        echo "Content of $values_file:"
        cat $values_file
      }
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_common[collapsed=true]\r\e[0KRunning common deployment steps\e[0m"
    - |
      if [ -n "${GIT_REVISION}" ] ; then
        change_git_revision $GIT_REVISION
      else
        echo -e "Current revision: $(git rev-parse HEAD)"
      fi
    - |
      if [[ ${CLUSTER_TYPE} == 'workload' ]]; then
          ENV_SUB_PATH="environment-values/workload-clusters"
          export OCI_COMPONENT_PATH="../../components/workload-cluster-oci-artifacts"
          export CLUSTER_CI_NAME=${WORKLOAD_CLUSTER_NAME}
      else
          ENV_SUB_PATH="environment-values"
          export OCI_COMPONENT_PATH="../components/oci-artifacts"
          export CLUSTER_CI_NAME=${MANAGEMENT_CLUSTER_NAME}
      fi
    - export ENV_PATH=${ENV_SUB_PATH}/${ENV_NAME}
    - values_file=$ENV_PATH/values.yaml
    - |
      if [ -n "$METAL3_PROVIDER" ]; then
        yq -i ".metal3.provider = \"$METAL3_PROVIDER\"" $values_file
      fi
    - |
      if [ -n "$DOCKER_HOST" ]; then
        docker network create --attachable kind
        export DOCKER_IP=$(getent ahostsv4 docker | awk '{print $1}' | sort -u)
        KIND_PREFIX=$(docker network inspect kind -f '{{ (index .IPAM.Config 0).Subnet }}')
        ip route add $KIND_PREFIX via $DOCKER_IP
      fi
    - |
      if [ ${OCI_TAG} ]; then
        echo -e "Applying OCI configuration for tag: ${OCI_TAG}"
        if [ $(yq '.components | length' $ENV_PATH/kustomization.yaml) -eq "0" ]; then
          yq -i '.components = []' $ENV_PATH/kustomization.yaml
        fi
        yq -i '.components += strenv(OCI_COMPONENT_PATH)' $ENV_PATH/kustomization.yaml
      cat <<EOF>> $ENV_PATH/kustomization.yaml
      patches:
      - target:
          kind: HelmRelease
          name: sylva-units
        patch: |
          - op: replace
            path: /spec/chart/spec/version
            value: ${OCI_TAG}
      EOF
        cat $ENV_PATH/kustomization.yaml
      fi
    - yq -i '.cluster.name = strenv(CLUSTER_CI_NAME)' $values_file
    - yq -i '.env_type = "ci"' $values_file
    - |
      if [ -n "${MGMT_ADDITIONAL_VALUES}" ] && [ "${CLUSTER_TYPE}" = "management" ]; then
        update_values_file ${MGMT_ADDITIONAL_VALUES}
      fi
    - |
      if [ -n "${WC_ADDITIONAL_VALUES}" ] && [ "${CLUSTER_TYPE}" = "workload" ]; then
        update_values_file ${WC_ADDITIONAL_VALUES}
      fi
    - |
      if [ -n "${REMOTE_VALUES}" ]; then
        echo -e "This pipeline is a cross project pipeline."
        echo "${REMOTE_VALUES}" > /tmp/remote_values.yaml
        update_values_file /tmp/remote_values.yaml
      fi
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_common\r\e[0K"
    - |
      case "$ENV_NAME" in
        *capd*)
           echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_capd[collapsed=true]\r\e[0KApplying specific capd configuration\e[0m"
           export DOCKER_HOST=tcp://$DOCKER_IP:2375
           yq -i '.capd_docker_host = strenv(DOCKER_HOST)' $values_file
           export CLUSTER_IP=$(echo $KIND_PREFIX | awk -F"." '{print $1"."$2"."$3".100"}')
           yq -i '.cluster_virtual_ip = strenv(CLUSTER_IP)' $values_file
           echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_capd\r\e[0K"
        ;;
        *capo*)
          echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_capo[collapsed=true]\r\e[0KApplying specific capo configuration\e[0m"
          mkdir -p ~/.config/openstack
      cat <<EOF>> ~/.config/openstack/clouds.yml
      clouds:
        capo_cloud:
          auth:
            auth_url: '${OS_AUTH_URL}'
            user_domain_name: '${OS_USER_DOMAIN_ID}'
            project_domain_name: '${OS_PROJECT_DOMAIN_ID}'
            project_name: '${OS_TENANT_NAME}'
            username: '${OS_USERNAME}'
            password: '${OS_PASSWORD}'
          region_name: '${OS_REGION_NAME}'
          verify: false
      EOF
          if [[ ${CLUSTER_TYPE} == 'management' ]]; then
            yq -i eval-all 'select(fileIndex==0).cluster.capo.clouds_yaml = select(fileIndex==1) | select(fileIndex==0)' $ENV_PATH/secrets.yaml ~/.config/openstack/clouds.yml
          fi
          yq -i '.cluster.capo.resources_tag = strenv(CAPO_TAG)' $values_file

          # The PRIVATE variables below are set in the gitlab runner configuration
          # and allow to enable extra values needed in the context of the platform on which this test actually runs
          PRIVATE_RUNNER_CONTEXT=falcon   # Temporary: to be set at runner level
          echo "Applying private $PRIVATE_RUNNER_CONTEXT values"
          PRIVATE_RUNNER_VALUES_PATH=${CI_PROJECT_DIR}/environment-values/ci/private/${PRIVATE_RUNNER_CONTEXT}-base

          # decypher falcon secret and apply them with values into extra kustomize component
          sops -d ${PRIVATE_RUNNER_VALUES_PATH}/secrets.enc.yaml > ${PRIVATE_RUNNER_VALUES_PATH}/secrets.yaml
          export EXTRA_COMPONENT="$(realpath ${PRIVATE_RUNNER_VALUES_PATH} --relative-to $ENV_PATH)"
          echo EXTRA_COMPONENT=$EXTRA_COMPONENT
          if [ $(yq '.components | length' $ENV_PATH/kustomization.yaml) -eq "0" ]; then yq -i '.components = []' $ENV_PATH/kustomization.yaml; fi
          yq -i '.components += strenv(EXTRA_COMPONENT)' $ENV_PATH/kustomization.yaml

          if [[ ${CLUSTER_TYPE} == 'workload' ]]; then
            yq -i '.namespace = "WORKLOAD_CLUSTER_NAMESPACE"' $ENV_PATH/kustomization.yaml
          fi
          echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_capo\r\e[0K"
        ;;
      esac
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_update_values[collapsed=true]\r\e[0KApply specific values\e[0m"
    - |
      if [ -n "${SPECIFIC_VALUES_FILE}" ]; then
        update_values_file $SPECIFIC_VALUES_FILE
      else
        echo -e "No specific values to apply"
      fi

    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_update_values\r\e[0K"
  os_cloud:
    # the OS_* variables below are set in the gitlab runner configuration
    # and contain information to connect to the OpenStack instance used
    # by this test
    - mkdir -p ~/.config/openstack
    - |
      cat <<EOF>> ~/.config/openstack/clouds.yml
      clouds:
        capo_cloud:
          auth:
            auth_url: '${OS_AUTH_URL}'
            user_domain_name: '${OS_USER_DOMAIN_ID}'
            project_domain_name: '${OS_PROJECT_DOMAIN_ID}'
            project_name: '${OS_TENANT_NAME}'
            username: '${OS_USERNAME}'
            password: '${OS_PASSWORD}'
          region_name: '${OS_REGION_NAME}'
          verify: false
      EOF


.management-base:
  extends: .docker-service
  variables:
    CLUSTER_TYPE: management
  timeout: 60min
  artifacts:
    expire_in: 48 hour
    when: always
    paths:
      - debug-on-exit.log
      - bootstrap-cluster-dump/
      - management-cluster-dump/
      - bootstrap-cluster-units-report.xml
      - management-cluster-units-report.xml
      - "*-timeline.html"
      - management-cluster-kubeconfig
    reports:
      junit:
      - bootstrap-cluster-units-report.xml
      - management-cluster-units-report.xml
    # expose_as: $CI_JOB_NAME  # not allowed https://gitlab.com/gitlab-org/gitlab/-/issues/427149

.deploy-management:
  extends: .management-base
  stage: deploy
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./bootstrap.sh environment-values/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $MGMT_INITIAL_ADDITIONAL_VALUES
    GIT_REVISION: $MGMT_INITIAL_REVISION

.update-management:
  extends: .management-base
  stage: update
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply.sh environment-values/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $MGMT_UPDATE_ADDITIONAL_VALUES
    GIT_REVISION: $MGMT_UPDATE_REVISION

.workload-base:
  extends: .docker-service
  variables:
    CLUSTER_TYPE: workload
  timeout: 60min
  artifacts:
    expire_in: 48 hour
    when: always
    paths:
      - debug-on-exit.log
      - management-cluster-dump/
      - workload-cluster-dump/
      - management-cluster-units-report.xml
      - "*-timeline.html"
      - management-cluster-kubeconfig
    reports:
      junit:
      - management-cluster-units-report.xml

.deploy-workload:
  stage: deploy-wc
  extends: .workload-base
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply-workload-cluster.sh environment-values/workload-clusters/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $WC_INITIAL_ADDITIONAL_VALUES
    GIT_REVISION: $WC_INITIAL_REVISION

.update-workload:
  extends: .workload-base
  stage: update-wc
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply-workload-cluster.sh environment-values/workload-clusters/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $WC_UPDATE_ADDITIONAL_VALUES
    GIT_REVISION: $WC_UPDATE_REVISION

.cleanup-capo:
  stage: delete
  image:
    name: $OPENSTACK_CLIENT_IMAGE
  script:
    - !reference [.common-deployment, os_cloud]
    - ./tools/openstack-cleanup.sh capo_cloud "${CAPO_TAG}"
  tags:
    - $CAPO_PLATFORM_TAG


.capm3-virt-kubeconfig:
  - |
    if [[ ${ENV_NAME} == 'rke2-capm3-virt' ]]; then
      yq -i e '.clusters[].cluster.insecure-skip-tls-verify = true' $WORKLOAD_CLUSTER_NAME-rancher.yaml
      yq -i e 'del(.clusters[].cluster.certificate-authority-data)' $WORKLOAD_CLUSTER_NAME-rancher.yaml
    fi

.test_scripts:
  edit_hosts_file:
    - domain=$(kubectl --kubeconfig management-cluster-kubeconfig get secret sylva-units-values -oyaml | yq '.data.values | @base64d' | yq '.cluster_domain')
    - |
      echo $domain
      if [[ $domain == "sylva" ]]; then
        echo "-- Get ingress saved into /etc/hosts"
        access_ip=$(kubectl --kubeconfig management-cluster-kubeconfig -n sylva-system get secrets/sylva-units-values -o jsonpath='{.data.values}' | base64 -d | yq -r .display_external_ip)
        for i in $(kubectl --kubeconfig management-cluster-kubeconfig get ingress -A -o custom-columns=:.spec.tls[].hosts[] | grep -v "<none>"); do
          echo $access_ip $i >> /etc/hosts
        done
      fi
  test_no_sso:
    - unset https_proxy http_proxy
    - export KUBECONFIG=management-cluster-kubeconfig
    - ./tools/shell-lib/get-wc-kubeconfig-from-rancher.sh $WORKLOAD_CLUSTER_NAME > $WORKLOAD_CLUSTER_NAME-rancher.yaml
    # For capm3 job we need to modify the $WORKLOAD_CLUSTER_NAME-rancher.yaml to remove the certificate and use an insecure option
    - !reference [.capm3-virt-kubeconfig]

  test_sso:
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_test_loging[collapsed=true]\r\e[0KGet ingress saved into /etc/hosts and set variables\e[0m"
    - export flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }')
    - export keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export neuvector_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n neuvector neuvector-webui-ingress  -o jsonpath='{ .spec.tls[].hosts[] }')
    - export USER_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.username}'| base64 -d)
    - export PASSWORD_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.password}'| base64 -d)
    - echo "Test dependencies and initialize firefox/driver"
    - /usr/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager --browser firefox --output json --debug
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_test_loging\r\e[0K"
    - echo "-- Testing login using SSO"
    - unset https_proxy http_proxy
    - python3 ./tools/login-test/test-sso.py
    # For capm3 job we need to modify the $WORKLOAD_CLUSTER_NAME-rancher.yaml to remove the certificate and use an insecure option
    - !reference [.capm3-virt-kubeconfig]

.test-sso+workload-kubeconfig:
  stage: deployment-test
  retry:
    max: 2
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_sso]
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Selenim
    - |
      attempts=0; max_attempts=5
      until kubectl run test-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- All done"
  tags:
    - $CAPO_PLATFORM_TAG

.test-sso:
  stage: deployment-test
  retry:
    max: 2
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_sso]

.test-no-sso+workload-kubeconfig:
  stage: deployment-test
  before_script:
    - apk add jq
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_no_sso]
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Rancher API
    - |
      attempts=0; max_attempts=5
      until kubectl run test-no-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-no-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-no-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- All done"

.test-login:
  stage: deployment-test
  variables:
    HURL_JUNIT_REPORT: login_junit_report.xml
  artifacts:
    expire_in: 48 hour
    when: always
    paths:
      - $HURL_JUNIT_REPORT
      - index.html
      - store/
    reports:
      junit:
      - $HURL_JUNIT_REPORT
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - export HURL_token=$(kubectl --kubeconfig management-cluster-kubeconfig -n vault get secrets/vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d)
    - export HURL_flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }'); export domain=$(echo $HURL_rancher_url | cut -f2 -d .)
    - export HURL_keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_neuvector_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n neuvector neuvector-webui-ingress  -o jsonpath='{ .spec.tls[].hosts[] }')
    - |
      if [ -n "${HURL_neuvector_url}" ]; then
          sleep 2;
      else
          rm -rf  ./tools/login-test/neuvector-admin.hurl
      fi
    - |
      if [ -n "${HURL_harbor_url}" ]; then
          sleep 2;
      else
          rm -rf  ./tools/login-test/harbor-admin.hurl
      fi
    - hurl -k ./tools/login-test/*.hurl --test --color --noproxy=".$domain"  --report-junit $HURL_JUNIT_REPORT --report-html .
