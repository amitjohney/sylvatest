---
# =============================
# Templates for deployments
# =============================

include:
  - local: .gitlab/ci/common.yml

.common-deployment:
  setup_ci_values:
    - |
      function update_values_file() {
        echo -e "Applying values from \\e[1;94m$1\\e[0m to \\e[1;92m$values_file\\e[0m"
        yq -i eval-all '. as $item ireduce ({}; . * $item )' $values_file $1
        echo "Content of $values_file:"
        cat $values_file
      }
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_common[collapsed=true]\r\e[0KRunning common deployment steps\e[0m"
    - |
      if [[ ${CLUSTER_TYPE} == 'workload' ]]; then
          ENV_SUB_PATH="environment-values/workload-clusters"
          export OCI_COMPONENT_PATH="../../components/workload-cluster-oci-artifacts"
          export CLUSTER_CI_NAME=${WORKLOAD_CLUSTER_NAME}
      else
          ENV_SUB_PATH="environment-values"
          export OCI_COMPONENT_PATH="../components/oci-artifacts"
          export CLUSTER_CI_NAME=${MANAGEMENT_CLUSTER_NAME}
      fi
    - export ENV_PATH=${ENV_SUB_PATH}/${ENV_NAME}
    - values_file=$ENV_PATH/values.yaml
    - |
      if [ -n "$METAL3_PROVIDER" ]; then
        yq -i ".metal3.provider = \"$METAL3_PROVIDER\"" $values_file
      fi
    - |
      if [ -n "$DOCKER_HOST" ]; then
        docker network create --attachable kind
        export DOCKER_IP=$(getent ahostsv4 docker | awk '{print $1}' | sort -u)
        KIND_PREFIX=$(docker network inspect kind -f '{{ (index .IPAM.Config 0).Subnet }}')
        ip route add $KIND_PREFIX via $DOCKER_IP
      fi
    - |
      if [ ${OCI_TAG} ]; then
        echo -e "Applying OCI configuration for tag: ${OCI_TAG}"
        if [ $(yq '.components | length' $ENV_PATH/kustomization.yaml) -eq "0" ]; then
          yq -i '.components = []' $ENV_PATH/kustomization.yaml
        fi
        yq -i '.components += strenv(OCI_COMPONENT_PATH)' $ENV_PATH/kustomization.yaml
      cat <<EOF>> $ENV_PATH/kustomization.yaml
      patches:
      - target:
          kind: HelmRelease
          name: sylva-units
        patch: |
          - op: replace
            path: /spec/chart/spec/version
            value: ${OCI_TAG}
      EOF
        cat $ENV_PATH/kustomization.yaml
      fi
    - yq -i '.cluster.name = strenv(CLUSTER_CI_NAME)' $values_file
    - yq -i '.env_type = "ci"' $values_file
    - |
      if [ -n "${CUSTOM_MGMT_VALUES_FILE}" ] && [ "${CLUSTER_TYPE}" = "management" ]; then
        update_values_file ${CUSTOM_MGMT_VALUES_FILE}
      fi
    - |
      if [ -n "${CUSTOM_WC_VALUES_FILE}" ] && [ "${CLUSTER_TYPE}" = "workload" ]; then
        update_values_file ${CUSTOM_WC_VALUES_FILE}
      fi
    - |
      if [ -n "${REMOTE_VALUES}" ]; then
        echo -e "This pipeline is a cross project pipeline."
        echo "${REMOTE_VALUES}" > /tmp/remote_values.yaml
        update_values_file /tmp/remote_values.yaml
      fi
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_common\r\e[0K"
    - |
      case "$ENV_NAME" in
        *capd*)
           echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_capd[collapsed=true]\r\e[0KApplying specific capd configuration\e[0m"
           export DOCKER_HOST=tcp://$DOCKER_IP:2375
           yq -i '.capd_docker_host = strenv(DOCKER_HOST)' $values_file
           export CLUSTER_EXTERNAL_IP=$(echo $KIND_PREFIX | awk -F"." '{print $1"."$2"."$3".100"}')
           yq -i '.cluster_external_ip = strenv(CLUSTER_EXTERNAL_IP)' $values_file
           echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_capd\r\e[0K"
        ;;
        *capo*)
          echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_capo[collapsed=true]\r\e[0KApplying specific capo configuration\e[0m"
          mkdir -p ~/.config/openstack
      cat <<EOF>> ~/.config/openstack/clouds.yml
      clouds:
        capo_cloud:
          auth:
            auth_url: '${OS_AUTH_URL}'
            user_domain_name: '${OS_USER_DOMAIN_ID}'
            project_domain_name: '${OS_PROJECT_DOMAIN_ID}'
            project_name: '${OS_TENANT_NAME}'
            username: '${OS_USERNAME}'
            password: '${OS_PASSWORD}'
          region_name: '${OS_REGION_NAME}'
          verify: false
      EOF
          if [[ ${CLUSTER_TYPE} == 'management' ]]; then
            yq -i eval-all 'select(fileIndex==0).cluster.capo.clouds_yaml = select(fileIndex==1) | select(fileIndex==0)' $ENV_PATH/secrets.yaml ~/.config/openstack/clouds.yml
          fi
          yq -i '.cluster.capo.resources_tag = strenv(CAPO_TAG)' $values_file

          # The PRIVATE variables below are set in the gitlab runner configuration
          # and allow to enable extra values needed in the context of the platform on which this test actually runs
          PRIVATE_RUNNER_CONTEXT=falcon   # Temporary: to be set at runner level
          echo "Applying private $PRIVATE_RUNNER_CONTEXT values"
          PRIVATE_RUNNER_VALUES_PATH=${CI_PROJECT_DIR}/environment-values/ci/private/${PRIVATE_RUNNER_CONTEXT}-base
          PRIVATE_RUNNER_VALUES=${PRIVATE_RUNNER_VALUES_PATH}/values.yaml
          echo ">> PRIVATE_RUNNER_VALUES=$PRIVATE_RUNNER_VALUES"
          update_values_file ${PRIVATE_RUNNER_VALUES}

          # decypher falcon secret and apply them into extra kustomize component
          sops -d ${PRIVATE_RUNNER_VALUES_PATH}/secrets.enc.yaml > ${PRIVATE_RUNNER_VALUES_PATH}/secrets.yaml
          export EXTRA_COMPONENT="$(realpath ${PRIVATE_RUNNER_VALUES_PATH} --relative-to $ENV_PATH)"
          echo EXTRA_COMPONENT=$EXTRA_COMPONENT
          if [ $(yq '.components | length' $ENV_PATH/kustomization.yaml) -eq "0" ]; then yq -i '.components = []' $ENV_PATH/kustomization.yaml; fi
          yq -i '.components += strenv(EXTRA_COMPONENT)' $ENV_PATH/kustomization.yaml

          # bootstrap.sh needs registry_mirrors in values.yaml to take them in account
          yq -e '.registry_mirrors' ${PRIVATE_RUNNER_VALUES_PATH}/secrets.yaml > registry_mirrors.yml
          yq -i eval-all 'select(fileIndex==0).registry_mirrors = select(fileIndex==1) | select(fileIndex==0)' $values_file registry_mirrors.yml

          if [[ ${CLUSTER_TYPE} == 'workload' ]]; then
            yq -i '.namespace = "WORKLOAD_CLUSTER_NAMESPACE"' $ENV_PATH/kustomization.yaml
          fi
          echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_capo\r\e[0K"
        ;;
      esac
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_update_values[collapsed=true]\r\e[0KApply specific values\e[0m"
    - |
      if [ -n "${SPECIFIC_VALUES_FILE}" ]; then
        update_values_file $SPECIFIC_VALUES_FILE
      else
        echo -e "No specific values to apply"
      fi

    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_update_values\r\e[0K"
  os_cloud:
    # the OS_* variables below are set in the gitlab runner configuration
    # and contain information to connect to the OpenStack instance used
    # by this test
    - mkdir -p ~/.config/openstack
    - |
      cat <<EOF>> ~/.config/openstack/clouds.yml
      clouds:
        capo_cloud:
          auth:
            auth_url: '${OS_AUTH_URL}'
            user_domain_name: '${OS_USER_DOMAIN_ID}'
            project_domain_name: '${OS_PROJECT_DOMAIN_ID}'
            project_name: '${OS_TENANT_NAME}'
            username: '${OS_USERNAME}'
            password: '${OS_PASSWORD}'
          region_name: '${OS_REGION_NAME}'
          verify: false
      EOF


.management-base:
  extends: .docker-service
  variables:
    CLUSTER_TYPE: management
  timeout: 60min
  artifacts:
    expire_in: 12 hour
    when: always
    paths:
      - debug-on-exit.log
      - bootstrap-cluster-dump/
      - management-cluster-dump/
      - bootstrap-cluster-units-report.xml
      - management-cluster-units-report.xml
      - "*-timeline.html"
      - management-cluster-kubeconfig
    reports:
      junit:
      - bootstrap-cluster-units-report.xml
      - management-cluster-units-report.xml
    # expose_as: $CI_JOB_NAME  # not allowed https://gitlab.com/gitlab-org/gitlab/-/issues/427149

.deploy-management:
  extends: .management-base
  stage: deploy
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./bootstrap.sh environment-values/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $MGMT_DEPLOYMENT_CUSTOM

.update-management:
  extends: .management-base
  stage: update
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply.sh environment-values/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $MGMT_UPDATE_CUSTOM

.workload-base:
  extends: .docker-service
  variables:
    CLUSTER_TYPE: workload
  timeout: 60min
  artifacts:
    expire_in: 12 hour
    when: always
    paths:
      - debug-on-exit.log
      - management-cluster-dump/
      - workload-cluster-dump/
      - management-cluster-units-report.xml
      - "*-timeline.html"
      - management-cluster-kubeconfig
    reports:
      junit:
      - management-cluster-units-report.xml

.deploy-workload:
  stage: deploy-wc
  extends: .workload-base
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply-workload-cluster.sh environment-values/workload-clusters/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $WC_DEPLOYMENT_CUSTOM

.update-workload:
  extends: .workload-base
  stage: update-wc
  script:
    - !reference [.common-deployment, setup_ci_values]
    - ./apply-workload-cluster.sh environment-values/workload-clusters/${ENV_NAME}
  variables:
    SPECIFIC_VALUES_FILE: $WC_UPDATE_CUSTOM

.cleanup-capo:
  stage: delete
  image:
    name: $OPENSTACK_CLIENT_IMAGE
  script:
    - !reference [.common-deployment, os_cloud]
    - ./tools/openstack-cleanup.sh capo_cloud "${CAPO_TAG}"
  tags:
    - $CAPO_PLATFORM_TAG

.test_scripts:
  edit_hosts_file:
    - domain=$(kubectl --kubeconfig management-cluster-kubeconfig get secret sylva-units-values -oyaml | yq '.data.values | @base64d' | yq '.cluster_external_domain')
    - |
      echo $domain
      if [[ $domain == "sylva" ]]; then
        echo "-- Get ingress saved into /etc/hosts"
        kubectl --kubeconfig management-cluster-kubeconfig get ingress -A -o custom-columns=:.status.loadBalancer.ingress[].ip,:.spec.tls[].hosts[] | grep -v '<none>' >> /etc/hosts
      fi
  test_no_sso:
    - |
      echo "-- Getting kubeconfig workload cluster"
      unset https_proxy http_proxy
      export rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }')
      BOOTSTRAP_PASSWORD=$(kubectl --kubeconfig management-cluster-kubeconfig -n cattle-system get secret bootstrap-secret -o jsonpath='{.data.bootstrapPassword}' | base64 -d)
      TOKEN=$(curl --insecure -s https://$rancher_url/v3-public/localProviders/local?action=login -H 'content-type: application/json' --data-binary '{"username":"admin","password":"'$BOOTSTRAP_PASSWORD'","ttl":60000}' | jq -r .token)
      KUBECONFIG=$(curl --insecure -s https://$rancher_url/v3/clusters/  -H "Authorization: Bearer $TOKEN" | jq '.data[] | select (.name=="'$WORKLOAD_CLUSTER_NAME-capi'") | .actions.generateKubeconfig'  | tr -d '"')
      curl --insecure -s -X POST $KUBECONFIG -H "Authorization: Bearer $TOKEN" | jq -r .config > $WORKLOAD_CLUSTER_NAME-rancher.yaml

.test-workload-cluster:
  stage: deployment-test
  retry:
    max: 2
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_test_loging[collapsed=true]\r\e[0KGet ingress saved into /etc/hosts and set variables\e[0m"
    - kubectl --kubeconfig management-cluster-kubeconfig get ingress -A -o custom-columns=:.status.loadBalancer.ingress[].ip,:.spec.tls[].hosts[] | grep -v '<none>' >> /etc/hosts
    - export flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }')
    - export keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export USER_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.username}'| base64 -d)
    - export PASSWORD_SSO=$(kubectl --kubeconfig ./management-cluster-kubeconfig -n keycloak get secrets/credential-sylva-sylva-admin-keycloak -o jsonpath='{.data.password}'| base64 -d)
    - echo "Test dependencies and initialize firefox/driver"
    - /usr/lib/python3.11/site-packages/selenium/webdriver/common/linux/selenium-manager --browser firefox --output json --debug
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_test_loging\r\e[0K"
    - echo "-- Testing login using SSO"
    - unset https_proxy http_proxy
    - python3 ./tools/login-test/test-sso.py
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Selenim
    - |
      attempts=0; max_attempts=5
      until kubectl run test-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- Get the sample workload cluster nodes annotations for keys node.longhorn.io/default-node-tags & node.longhorn.io/default-disks-config"
    - kubectl --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml get nodes -o yaml | yq '.items[] | (.metadata.name,.metadata.annotations."node.longhorn.io/default-node-tags",.metadata.annotations."node.longhorn.io/default-disks-config")'
    - echo "-- All done"
  tags:
    - $CAPO_PLATFORM_TAG

.test-workload-cluster-no-sso:
  stage: deployment-test
  before_script:
    - apk add jq
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_no_sso]
    - echo "-- Testing workload cluster. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Rancher API
    - |
      attempts=0; max_attempts=5
      until kubectl run test-no-sso --image=registry.k8s.io/pause:3.9 --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --overrides='{"apiVersion": "v1","spec": {"containers": [{"name": "test","image": "registry.k8s.io/pause:3.9","securityContext": {"allowPrivilegeEscalation": false,"capabilities": {"drop": ["ALL"]},"runAsNonRoot": true,"runAsGroup": 1000,"runAsUser": 1000,"seccompProfile": {"type": "RuntimeDefault"}}}]}}'; do
        sleep 3
        ((attempts++)) && ((attempts==max_attempts)) && exit -1
      done
    - echo "-- Wait for test-no-sso pod to be created"
    - kubectl wait --for=condition=Ready pod/test-no-sso --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml --timeout=60s
    - echo "-- Get the sample workload cluster nodes annotations for keys node.longhorn.io/default-node-tags & node.longhorn.io/default-disks-config"
    - kubectl --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml get nodes -o yaml | yq '.items[] | (.metadata.name,.metadata.annotations."node.longhorn.io/default-node-tags",.metadata.annotations."node.longhorn.io/default-disks-config")'
    - echo "-- All done"

.test-login:
  stage: deployment-test
  variables:
    HURL_JUNIT_REPORT: login_junit_report.xml
  artifacts:
    expire_in: 6 hour
    when: always
    paths:
      - $HURL_JUNIT_REPORT
      - index.html
      - store/
    reports:
      junit:
      - $HURL_JUNIT_REPORT
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - export HURL_token=$(kubectl --kubeconfig management-cluster-kubeconfig -n vault get secrets/vault-unseal-keys -o jsonpath='{.data.vault-root}' | base64 -d)
    - export HURL_flux_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n flux-system flux-webui-weave-gitops -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_vault_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n vault vault -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_rancher_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n cattle-system rancher -o jsonpath='{ .spec.tls[].hosts[] }'); export domain=$(echo $HURL_rancher_url | cut -f2 -d .)
    - export HURL_keycloak_url=$(kubectl --kubeconfig management-cluster-kubeconfig  get ingress -n keycloak keycloak-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - export HURL_harbor_url=$(kubectl --kubeconfig management-cluster-kubeconfig get ingress -n harbor harbor-ingress -o jsonpath='{ .spec.tls[].hosts[] }')
    - hurl -k ./tools/login-test/*.hurl --test --color --noproxy=".$domain"  --report-junit $HURL_JUNIT_REPORT --report-html .

.test-rke2-node-annotations:
  stage: deployment-test
  variables:
    CLUSTER_TYPE: workload
  script:
    - !reference [.test_scripts, edit_hosts_file]
    - !reference [.test_scripts, test_no_sso]
    - echo "-- Testing RKE2 workload cluster nodes annotation. Job started at '$CI_JOB_STARTED_AT'."
    # the $WORKLOAD_CLUSTER_NAME-rancher.yaml file below is the kubeconfig previously downloaded from Rancher server through Rancher API
    - echo -e "\e[1m\e[0Ksection_start:`date +%s`:gitlab_ci_test_rke2_node_annotation[collapsed=true]\r\e[0KTesting RKE2 node annotations\e[0m"
    - |
      for cp_node in $(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes -o jsonpath='{ $.items[?(@.metadata.annotations.cluster\.x-k8s\.io\/owner-kind == "RKE2ControlPlane")].metadata.name}'); do
        echo -e "Testing specific annotation presence for RKE2 node $cp_node"
        test_annotation_value=$(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes $cp_node -o yaml | yq '.metadata.annotations."sylva.org/test-annotation-scope"')
        if [[ "${test_annotation_value}" == "ci" ]]; then
          echo -e "RKE2 node $cp_node has the expected annotation"
        else
          echo -e "CP RKE2 node $cp_node does not have the expected annotation, it has:\n$(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes $cp_node -o yaml | yq '.metadata.annotations')"
          exit_code=1
        fi
      done
    - |
      for md_node in $(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes -o jsonpath='{ $.items[?(@.metadata.annotations.cluster\.x-k8s\.io\/owner-kind == "MachineSet")].metadata.name}'); do
        echo -e "Testing specific annotation presence for RKE2 node $md_node"
        test_annotation_value=$(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes $md_node -o yaml | yq '.metadata.annotations."sylva.org/test-annotation-scope"')
        if [[ "${test_annotation_value}" == "ci" ]]; then
          echo -e "RKE2 node $md_node has the expected annotation"
        else
          echo -e "MD RKE2 node $md_node does not have the expected annotation, it has:\n$(kubectl get --kubeconfig $WORKLOAD_CLUSTER_NAME-rancher.yaml nodes $md_node -o yaml | yq '.metadata.annotations')"
          exit_code=1
        fi
      done
    - exit ${exit_code:-0}
    - echo "-- All done"
    - echo -e "\e[0Ksection_end:`date +%s`:gitlab_ci_test_rke2_node_annotation\r\e[0K"
